{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NIH Dataset (ZIP Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-01 19:05:23--  ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
      "           => ‘cell_images.zip’\n",
      "Resolving lhcftp.nlm.nih.gov (lhcftp.nlm.nih.gov)... 130.14.55.35, 2607:f220:41e:7055::35\n",
      "Connecting to lhcftp.nlm.nih.gov (lhcftp.nlm.nih.gov)|130.14.55.35|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /Open-Access-Datasets/Malaria ... done.\n",
      "==> SIZE cell_images.zip ... 353452851\n",
      "==> PASV ... done.    ==> RETR cell_images.zip ... done.\n",
      "Length: 353452851 (337M) (unauthoritative)\n",
      "\n",
      "cell_images.zip     100%[===================>] 337.08M  74.5MB/s    in 4.7s    \n",
      "\n",
      "2020-03-01 19:05:28 (71.2 MB/s) - ‘cell_images.zip’ saved [353452851]\n",
      "\n",
      "Extracting images...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Download NIH dataset zip file\n",
    "!wget -nc ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
    "\n",
    "# Extract images if not already extracted\n",
    "ROOT_DIR = os.path.join(\"/\", \"content\")\n",
    "if not os.path.isdir(\"cell_images\"):\n",
    "    print(\"Extracting images...\")\n",
    "    with ZipFile(os.path.join(\"cell_images.zip\"), \"r\") as zipObj:\n",
    "        zipObj.extractall()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Images, Resize, and Store in NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.2.0.32-cp36-cp36m-manylinux1_x86_64.whl (28.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.2 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.2.0.32\n",
      "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease          \u001b[0m\n",
      "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease                  \n",
      "Hit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Reading package lists... Done\u001b[0m                \u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "28 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libbsd0 libice6 libx11-6 libx11-data libxau6 libxcb1 libxdmcp6 x11-common\n",
      "The following NEW packages will be installed:\n",
      "  libbsd0 libice6 libsm6 libx11-6 libx11-data libxau6 libxcb1 libxdmcp6\n",
      "  libxext6 libxrender1 x11-common\n",
      "0 upgraded, 11 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 915 kB of archives.\n",
      "After this operation, 4091 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxau6 amd64 1:1.0.8-1 [8376 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbsd0 amd64 0.8.7-1ubuntu0.1 [41.6 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxdmcp6 amd64 1:1.1.2-3 [10.7 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb1 amd64 1.13-2~ubuntu18.04 [45.5 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-data all 2:1.6.4-3ubuntu0.2 [113 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-6 amd64 2:1.6.4-3ubuntu0.2 [569 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxext6 amd64 2:1.3.3-1 [29.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 x11-common all 1:7.7+19ubuntu7.1 [22.5 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libice6 amd64 2:1.0.9-2 [40.2 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsm6 amd64 2:1.2.2-1 [15.8 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrender1 amd64 1:0.9.10-1 [18.7 kB]\n",
      "Fetched 915 kB in 1s (1038 kB/s)       \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libxau6:amd64.\n",
      "(Reading database ... 16107 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libxau6_1%3a1.0.8-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libxau6:amd64 (1:1.0.8-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Selecting previously unselected package libbsd0:amd64.\n",
      "Preparing to unpack .../01-libbsd0_0.8.7-1ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libbsd0:amd64 (0.8.7-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libxdmcp6:amd64.\n",
      "Preparing to unpack .../02-libxdmcp6_1%3a1.1.2-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libxdmcp6:amd64 (1:1.1.2-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libxcb1:amd64.\n",
      "Preparing to unpack .../03-libxcb1_1.13-2~ubuntu18.04_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libxcb1:amd64 (1.13-2~ubuntu18.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libx11-data.\n",
      "Preparing to unpack .../04-libx11-data_2%3a1.6.4-3ubuntu0.2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libx11-data (2:1.6.4-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libx11-6:amd64.\n",
      "Preparing to unpack .../05-libx11-6_2%3a1.6.4-3ubuntu0.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libx11-6:amd64 (2:1.6.4-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libxext6:amd64.\n",
      "Preparing to unpack .../06-libxext6_2%3a1.3.3-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libxext6:amd64 (2:1.3.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package x11-common.\n",
      "Preparing to unpack .../07-x11-common_1%3a7.7+19ubuntu7.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8\u001b[1mdpkg-query:\u001b[0m no packages found matching nux-tools\n",
      "Unpacking x11-common (1:7.7+19ubuntu7.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libice6:amd64.\n",
      "Preparing to unpack .../08-libice6_2%3a1.0.9-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libice6:amd64 (2:1.0.9-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package libsm6:amd64.\n",
      "Preparing to unpack .../09-libsm6_2%3a1.2.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Unpacking libsm6:amd64 (2:1.2.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Selecting previously unselected package libxrender1:amd64.\n",
      "Preparing to unpack .../10-libxrender1_1%3a0.9.10-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [################################..........................] \u001b8Unpacking libxrender1:amd64 (1:0.9.10-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libbsd0:amd64 (0.8.7-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libxdmcp6:amd64 (1:1.1.2-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up x11-common (1:7.7+19ubuntu7.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libx11-data (2:1.6.4-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libxau6:amd64 (1:1.0.8-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libice6:amd64 (2:1.0.9-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libxcb1:amd64 (1.13-2~ubuntu18.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libsm6:amd64 (2:1.2.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libx11-6:amd64 (2:1.6.4-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libxrender1:amd64 (1:0.9.10-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libxext6:amd64 (2:1.3.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JUninfected Dataset size is: (13779, 128, 128, 3)\n",
      "Parasitized Dataset size is: (13779, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# Install and import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "!pip install opencv-python\n",
    "!apt update && apt install -y libsm6 libxext6 libxrender1\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Create new folders to save rescaled images\n",
    "if not os.path.isdir(\"RescaledSet\"):\n",
    "    os.mkdir(\"RescaledSet\")\n",
    "if not os.path.isdir(\"RescaledSet/Parasitized\"):\n",
    "    os.mkdir(\"RescaledSet/Parasitized\")\n",
    "if not os.path.isdir(\"RescaledSet/Uninfected\"):\n",
    "    os.mkdir(\"RescaledSet/Uninfected\")\n",
    "\n",
    "# Generate list of parasitized file names\n",
    "ParasitizedFiles = os.listdir(\"cell_images/Parasitized/\")\n",
    "UninfectedFiles = os.listdir(\"cell_images/Uninfected/\")\n",
    "\n",
    "# Remove Thumb.db files\n",
    "while 'Thumbs.db' in ParasitizedFiles: ParasitizedFiles.remove('Thumbs.db')   \n",
    "while 'Thumbs.db' in UninfectedFiles: UninfectedFiles.remove('Thumbs.db')  \n",
    "\n",
    "# Pre-allocate memory space for images\n",
    "Parasitized = np.empty([13779,128,128,3])\n",
    "Uninfected = np.empty([13779,128,128,3])\n",
    "\n",
    "# Resize and load parasitized images\n",
    "for i in range(13779):\n",
    "    TempImage = cv2.imread('cell_images/Parasitized/'+ParasitizedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Parasitized[i,:,:,:] = ResizedImage\n",
    "\n",
    "# Resize and load uninfected images\n",
    "for i in range(13779):\n",
    "    TempImage = cv2.imread('cell_images/Uninfected/'+UninfectedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Uninfected[i,:,:,:] = ResizedImage\n",
    "    \n",
    "print('Uninfected Dataset size is:',np.shape(Uninfected))\n",
    "print('Parasitized Dataset size is:',np.shape(Parasitized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cross-Validation Indices for Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset labels\n",
    "ParasitizedLabels = np.repeat([[0,1]], 13779, axis=0)\n",
    "UninfectedLabels = np.repeat([[1,0]], 13779, axis=0)\n",
    "Labels = np.concatenate((ParasitizedLabels,UninfectedLabels), axis=0)\n",
    "\n",
    "# Generate image dataset\n",
    "Dataset = np.concatenate((Parasitized, Uninfected), axis=0)\n",
    "\n",
    "# Generate 5-fold cross-validation groups\n",
    "CVIndices = np.random.permutation(Dataset.shape[0])\n",
    "Index1, Index2, Index3, Index4, Index5 = CVIndices[:5512], CVIndices[5512:11024], CVIndices[11024:16536], CVIndices[16536:22048], CVIndices[22048:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create List of Classification Layer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant neural network architecture packages \n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Save Results as CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:123: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now training cross-validation set # 1\n",
      "Train on 22048 samples, validate on 5510 samples\n",
      "Epoch 1/75\n",
      "22048/22048 [==============================] - 55s 2ms/step - loss: 0.7193 - accuracy: 0.7221 - val_loss: 0.5677 - val_accuracy: 0.7074\n",
      "Epoch 2/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.4274 - accuracy: 0.8382 - val_loss: 0.2298 - val_accuracy: 0.9042\n",
      "Epoch 3/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.3346 - accuracy: 0.8795 - val_loss: 0.1804 - val_accuracy: 0.9318\n",
      "Epoch 4/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.2775 - accuracy: 0.9012 - val_loss: 0.1638 - val_accuracy: 0.9410\n",
      "Epoch 5/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.2417 - accuracy: 0.9141 - val_loss: 0.1545 - val_accuracy: 0.9445\n",
      "Epoch 6/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.2272 - accuracy: 0.9217 - val_loss: 0.1541 - val_accuracy: 0.9465\n",
      "Epoch 7/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.2061 - accuracy: 0.9301 - val_loss: 0.1454 - val_accuracy: 0.9506\n",
      "Epoch 8/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1953 - accuracy: 0.9322 - val_loss: 0.1391 - val_accuracy: 0.9535\n",
      "Epoch 9/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1936 - accuracy: 0.9329 - val_loss: 0.1389 - val_accuracy: 0.9534\n",
      "Epoch 10/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1782 - accuracy: 0.9399 - val_loss: 0.1368 - val_accuracy: 0.9546\n",
      "Epoch 11/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1765 - accuracy: 0.9433 - val_loss: 0.1289 - val_accuracy: 0.9570\n",
      "Epoch 12/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1713 - accuracy: 0.9423 - val_loss: 0.1270 - val_accuracy: 0.9574\n",
      "Epoch 13/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1652 - accuracy: 0.9438 - val_loss: 0.1264 - val_accuracy: 0.9584\n",
      "Epoch 14/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1607 - accuracy: 0.9464 - val_loss: 0.1261 - val_accuracy: 0.9581\n",
      "Epoch 15/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1563 - accuracy: 0.9478 - val_loss: 0.1272 - val_accuracy: 0.9583\n",
      "Epoch 16/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1471 - accuracy: 0.9496 - val_loss: 0.1231 - val_accuracy: 0.9593\n",
      "Epoch 17/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1478 - accuracy: 0.9498 - val_loss: 0.1235 - val_accuracy: 0.9595\n",
      "Epoch 18/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1502 - accuracy: 0.9498 - val_loss: 0.1196 - val_accuracy: 0.9610\n",
      "Epoch 19/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1426 - accuracy: 0.9525 - val_loss: 0.1185 - val_accuracy: 0.9613\n",
      "Epoch 20/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1414 - accuracy: 0.9540 - val_loss: 0.1216 - val_accuracy: 0.9604\n",
      "Epoch 21/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1357 - accuracy: 0.9541 - val_loss: 0.1180 - val_accuracy: 0.9617\n",
      "Epoch 22/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1361 - accuracy: 0.9546 - val_loss: 0.1156 - val_accuracy: 0.9632\n",
      "Epoch 23/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1362 - accuracy: 0.9545 - val_loss: 0.1161 - val_accuracy: 0.9619\n",
      "Epoch 24/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1307 - accuracy: 0.9572 - val_loss: 0.1160 - val_accuracy: 0.9632\n",
      "Epoch 25/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1286 - accuracy: 0.9551 - val_loss: 0.1134 - val_accuracy: 0.9633\n",
      "Epoch 26/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1247 - accuracy: 0.9576 - val_loss: 0.1189 - val_accuracy: 0.9621\n",
      "Epoch 27/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1212 - accuracy: 0.9581 - val_loss: 0.1128 - val_accuracy: 0.9633\n",
      "Epoch 28/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1285 - accuracy: 0.9551 - val_loss: 0.1167 - val_accuracy: 0.9628\n",
      "Epoch 29/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1237 - accuracy: 0.9575 - val_loss: 0.1096 - val_accuracy: 0.9644\n",
      "Epoch 30/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1177 - accuracy: 0.9591 - val_loss: 0.1115 - val_accuracy: 0.9646\n",
      "Epoch 31/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1156 - accuracy: 0.9600 - val_loss: 0.1114 - val_accuracy: 0.9646\n",
      "Epoch 32/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1173 - accuracy: 0.9596 - val_loss: 0.1141 - val_accuracy: 0.9633\n",
      "Epoch 33/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1152 - accuracy: 0.9611 - val_loss: 0.1119 - val_accuracy: 0.9639\n",
      "Epoch 34/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1117 - accuracy: 0.9624 - val_loss: 0.1091 - val_accuracy: 0.9648\n",
      "Epoch 35/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1099 - accuracy: 0.9617 - val_loss: 0.1088 - val_accuracy: 0.9650\n",
      "Epoch 36/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1102 - accuracy: 0.9639 - val_loss: 0.1120 - val_accuracy: 0.9637\n",
      "Epoch 37/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1083 - accuracy: 0.9626 - val_loss: 0.1082 - val_accuracy: 0.9648\n",
      "Epoch 38/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1044 - accuracy: 0.9633 - val_loss: 0.1090 - val_accuracy: 0.9644\n",
      "Epoch 39/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1038 - accuracy: 0.9634 - val_loss: 0.1071 - val_accuracy: 0.9648\n",
      "Epoch 40/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0987 - accuracy: 0.9655 - val_loss: 0.1066 - val_accuracy: 0.9666\n",
      "Epoch 41/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1054 - accuracy: 0.9617 - val_loss: 0.1114 - val_accuracy: 0.9642\n",
      "Epoch 42/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0976 - accuracy: 0.9657 - val_loss: 0.1081 - val_accuracy: 0.9646\n",
      "Epoch 43/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.1004 - accuracy: 0.9641 - val_loss: 0.1062 - val_accuracy: 0.9661\n",
      "Epoch 44/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0969 - accuracy: 0.9666 - val_loss: 0.1059 - val_accuracy: 0.9662\n",
      "Epoch 45/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0943 - accuracy: 0.9670 - val_loss: 0.1049 - val_accuracy: 0.9657\n",
      "Epoch 46/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0935 - accuracy: 0.9659 - val_loss: 0.1090 - val_accuracy: 0.9642\n",
      "Epoch 47/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0977 - accuracy: 0.9644 - val_loss: 0.1092 - val_accuracy: 0.9644\n",
      "Epoch 48/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0927 - accuracy: 0.9674 - val_loss: 0.1068 - val_accuracy: 0.9644\n",
      "Epoch 49/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0913 - accuracy: 0.9671 - val_loss: 0.1065 - val_accuracy: 0.9655\n",
      "Epoch 50/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0921 - accuracy: 0.9672 - val_loss: 0.1079 - val_accuracy: 0.9648\n",
      "Epoch 51/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0856 - accuracy: 0.9685 - val_loss: 0.1051 - val_accuracy: 0.9661\n",
      "Epoch 52/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0922 - accuracy: 0.9675 - val_loss: 0.1070 - val_accuracy: 0.9655\n",
      "Epoch 53/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0863 - accuracy: 0.9692 - val_loss: 0.1077 - val_accuracy: 0.9650\n",
      "Epoch 54/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0864 - accuracy: 0.9696 - val_loss: 0.1055 - val_accuracy: 0.9655\n",
      "Epoch 55/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0882 - accuracy: 0.9695 - val_loss: 0.1047 - val_accuracy: 0.9664\n",
      "Epoch 56/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0859 - accuracy: 0.9698 - val_loss: 0.1045 - val_accuracy: 0.9664\n",
      "Epoch 57/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0856 - accuracy: 0.9706 - val_loss: 0.1053 - val_accuracy: 0.9668\n",
      "Epoch 58/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0818 - accuracy: 0.9698 - val_loss: 0.1073 - val_accuracy: 0.9650\n",
      "Epoch 59/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0818 - accuracy: 0.9703 - val_loss: 0.1075 - val_accuracy: 0.9646\n",
      "Epoch 60/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0756 - accuracy: 0.9723 - val_loss: 0.1013 - val_accuracy: 0.9677\n",
      "Epoch 61/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0786 - accuracy: 0.9711 - val_loss: 0.1036 - val_accuracy: 0.9659\n",
      "Epoch 62/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0784 - accuracy: 0.9711 - val_loss: 0.1036 - val_accuracy: 0.9662\n",
      "Epoch 63/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0819 - accuracy: 0.9700 - val_loss: 0.1047 - val_accuracy: 0.9666\n",
      "Epoch 64/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0764 - accuracy: 0.9722 - val_loss: 0.1046 - val_accuracy: 0.9666\n",
      "Epoch 65/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0794 - accuracy: 0.9707 - val_loss: 0.1042 - val_accuracy: 0.9670\n",
      "Epoch 66/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0778 - accuracy: 0.9721 - val_loss: 0.1038 - val_accuracy: 0.9670\n",
      "Epoch 67/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0722 - accuracy: 0.9744 - val_loss: 0.1035 - val_accuracy: 0.9681\n",
      "Epoch 68/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0711 - accuracy: 0.9744 - val_loss: 0.1029 - val_accuracy: 0.9670\n",
      "Epoch 69/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0740 - accuracy: 0.9726 - val_loss: 0.1048 - val_accuracy: 0.9666\n",
      "Epoch 70/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0720 - accuracy: 0.9731 - val_loss: 0.1031 - val_accuracy: 0.9670\n",
      "Epoch 71/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0717 - accuracy: 0.9749 - val_loss: 0.1047 - val_accuracy: 0.9673\n",
      "Epoch 72/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0685 - accuracy: 0.9756 - val_loss: 0.1036 - val_accuracy: 0.9672\n",
      "Epoch 73/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0676 - accuracy: 0.9752 - val_loss: 0.1068 - val_accuracy: 0.9666\n",
      "Epoch 74/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0676 - accuracy: 0.9752 - val_loss: 0.1021 - val_accuracy: 0.9679\n",
      "Epoch 75/75\n",
      "22048/22048 [==============================] - 53s 2ms/step - loss: 0.0682 - accuracy: 0.9742 - val_loss: 0.1061 - val_accuracy: 0.9672\n",
      "Training Loss: [0.7193, 0.4274, 0.3346, 0.2775, 0.2417, 0.2272, 0.2061, 0.1953, 0.1936, 0.1782, 0.1765, 0.1713, 0.1652, 0.1607, 0.1563, 0.1471, 0.1478, 0.1502, 0.1426, 0.1414, 0.1357, 0.1361, 0.1362, 0.1307, 0.1286, 0.1247, 0.1212, 0.1285, 0.1237, 0.1177, 0.1156, 0.1173, 0.1152, 0.1117, 0.1099, 0.1102, 0.1083, 0.1044, 0.1038, 0.0987, 0.1054, 0.0976, 0.1004, 0.0969, 0.0943, 0.0935, 0.0977, 0.0927, 0.0913, 0.0921, 0.0856, 0.0922, 0.0863, 0.0864, 0.0882, 0.0859, 0.0856, 0.0818, 0.0818, 0.0756, 0.0786, 0.0784, 0.0819, 0.0764, 0.0794, 0.0778, 0.0722, 0.0711, 0.074, 0.072, 0.0717, 0.0685, 0.0676, 0.0676, 0.0682]\n",
      "Training Accuracy: [0.7221, 0.8382, 0.8795, 0.9012, 0.9141, 0.9217, 0.9301, 0.9322, 0.9329, 0.9399, 0.9433, 0.9423, 0.9438, 0.9464, 0.9478, 0.9496, 0.9498, 0.9498, 0.9525, 0.954, 0.9541, 0.9546, 0.9545, 0.9572, 0.9551, 0.9576, 0.9581, 0.9551, 0.9575, 0.9591, 0.96, 0.9596, 0.9611, 0.9624, 0.9617, 0.9639, 0.9626, 0.9633, 0.9634, 0.9655, 0.9617, 0.9657, 0.9641, 0.9666, 0.967, 0.9659, 0.9644, 0.9674, 0.9671, 0.9672, 0.9685, 0.9675, 0.9692, 0.9696, 0.9695, 0.9698, 0.9706, 0.9698, 0.9703, 0.9723, 0.9711, 0.9711, 0.97, 0.9722, 0.9707, 0.9721, 0.9744, 0.9744, 0.9726, 0.9731, 0.9749, 0.9756, 0.9752, 0.9752, 0.9742]\n",
      "Validation Loss: [0.5677, 0.2298, 0.1804, 0.1638, 0.1545, 0.1541, 0.1454, 0.1391, 0.1389, 0.1368, 0.1289, 0.127, 0.1264, 0.1261, 0.1272, 0.1231, 0.1235, 0.1196, 0.1185, 0.1216, 0.118, 0.1156, 0.1161, 0.116, 0.1134, 0.1189, 0.1128, 0.1167, 0.1096, 0.1115, 0.1114, 0.1141, 0.1119, 0.1091, 0.1088, 0.112, 0.1082, 0.109, 0.1071, 0.1066, 0.1114, 0.1081, 0.1062, 0.1059, 0.1049, 0.109, 0.1092, 0.1068, 0.1065, 0.1079, 0.1051, 0.107, 0.1077, 0.1055, 0.1047, 0.1045, 0.1053, 0.1073, 0.1075, 0.1013, 0.1036, 0.1036, 0.1047, 0.1046, 0.1042, 0.1038, 0.1035, 0.1029, 0.1048, 0.1031, 0.1047, 0.1036, 0.1068, 0.1021, 0.1061]\n",
      "Validation Accuracy: [0.7074, 0.9042, 0.9318, 0.941, 0.9445, 0.9465, 0.9506, 0.9535, 0.9534, 0.9546, 0.957, 0.9574, 0.9584, 0.9581, 0.9583, 0.9593, 0.9595, 0.961, 0.9613, 0.9604, 0.9617, 0.9632, 0.9619, 0.9632, 0.9633, 0.9621, 0.9633, 0.9628, 0.9644, 0.9646, 0.9646, 0.9633, 0.9639, 0.9648, 0.965, 0.9637, 0.9648, 0.9644, 0.9648, 0.9666, 0.9642, 0.9646, 0.9661, 0.9662, 0.9657, 0.9642, 0.9644, 0.9644, 0.9655, 0.9648, 0.9661, 0.9655, 0.965, 0.9655, 0.9664, 0.9664, 0.9668, 0.965, 0.9646, 0.9677, 0.9659, 0.9662, 0.9666, 0.9666, 0.967, 0.967, 0.9681, 0.967, 0.9666, 0.967, 0.9673, 0.9672, 0.9666, 0.9679, 0.9672]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:123: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now training cross-validation set # 2\n",
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 54s 2ms/step - loss: 0.7851 - accuracy: 0.6915 - val_loss: 0.3276 - val_accuracy: 0.8639\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.4494 - accuracy: 0.8288 - val_loss: 0.2073 - val_accuracy: 0.9131\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.3439 - accuracy: 0.8721 - val_loss: 0.1762 - val_accuracy: 0.9305\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2928 - accuracy: 0.8957 - val_loss: 0.1612 - val_accuracy: 0.9399\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2552 - accuracy: 0.9108 - val_loss: 0.1468 - val_accuracy: 0.9472\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2379 - accuracy: 0.9182 - val_loss: 0.1395 - val_accuracy: 0.9510\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2155 - accuracy: 0.9265 - val_loss: 0.1337 - val_accuracy: 0.9532\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2033 - accuracy: 0.9315 - val_loss: 0.1322 - val_accuracy: 0.9548\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1935 - accuracy: 0.9339 - val_loss: 0.1299 - val_accuracy: 0.9546\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1845 - accuracy: 0.9373 - val_loss: 0.1271 - val_accuracy: 0.9561\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1806 - accuracy: 0.9408 - val_loss: 0.1224 - val_accuracy: 0.9565\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1789 - accuracy: 0.9397 - val_loss: 0.1254 - val_accuracy: 0.9577\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1644 - accuracy: 0.9447 - val_loss: 0.1224 - val_accuracy: 0.9581\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1637 - accuracy: 0.9461 - val_loss: 0.1221 - val_accuracy: 0.9583\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1615 - accuracy: 0.9458 - val_loss: 0.1190 - val_accuracy: 0.9597\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1637 - accuracy: 0.9465 - val_loss: 0.1159 - val_accuracy: 0.9606\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1518 - accuracy: 0.9495 - val_loss: 0.1163 - val_accuracy: 0.9606\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1492 - accuracy: 0.9488 - val_loss: 0.1148 - val_accuracy: 0.9604\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1473 - accuracy: 0.9500 - val_loss: 0.1155 - val_accuracy: 0.9606\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1389 - accuracy: 0.9533 - val_loss: 0.1140 - val_accuracy: 0.9612\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1391 - accuracy: 0.9528 - val_loss: 0.1181 - val_accuracy: 0.9604\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1351 - accuracy: 0.9540 - val_loss: 0.1120 - val_accuracy: 0.9610\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1355 - accuracy: 0.9550 - val_loss: 0.1136 - val_accuracy: 0.9612\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1309 - accuracy: 0.9546 - val_loss: 0.1099 - val_accuracy: 0.9619\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1319 - accuracy: 0.9549 - val_loss: 0.1102 - val_accuracy: 0.9630\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1255 - accuracy: 0.9560 - val_loss: 0.1127 - val_accuracy: 0.9626\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1236 - accuracy: 0.9563 - val_loss: 0.1074 - val_accuracy: 0.9637\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1212 - accuracy: 0.9598 - val_loss: 0.1095 - val_accuracy: 0.9626\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1256 - accuracy: 0.9567 - val_loss: 0.1123 - val_accuracy: 0.9624\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1192 - accuracy: 0.9581 - val_loss: 0.1094 - val_accuracy: 0.9623\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1166 - accuracy: 0.9588 - val_loss: 0.1061 - val_accuracy: 0.9628\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1134 - accuracy: 0.9605 - val_loss: 0.1069 - val_accuracy: 0.9634\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1182 - accuracy: 0.9604 - val_loss: 0.1094 - val_accuracy: 0.9626\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1144 - accuracy: 0.9597 - val_loss: 0.1071 - val_accuracy: 0.9632\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1113 - accuracy: 0.9608 - val_loss: 0.1058 - val_accuracy: 0.9632\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1106 - accuracy: 0.9619 - val_loss: 0.1065 - val_accuracy: 0.9628\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1045 - accuracy: 0.9633 - val_loss: 0.1066 - val_accuracy: 0.9628\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1065 - accuracy: 0.9623 - val_loss: 0.1051 - val_accuracy: 0.9637\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1015 - accuracy: 0.9638 - val_loss: 0.1061 - val_accuracy: 0.9634\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1029 - accuracy: 0.9628 - val_loss: 0.1044 - val_accuracy: 0.9646\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1034 - accuracy: 0.9632 - val_loss: 0.1041 - val_accuracy: 0.9646\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1020 - accuracy: 0.9636 - val_loss: 0.1054 - val_accuracy: 0.9639\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0977 - accuracy: 0.9655 - val_loss: 0.1049 - val_accuracy: 0.9644\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0978 - accuracy: 0.9651 - val_loss: 0.1069 - val_accuracy: 0.9643\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0900 - accuracy: 0.9687 - val_loss: 0.1038 - val_accuracy: 0.9644\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1000 - accuracy: 0.9648 - val_loss: 0.1032 - val_accuracy: 0.9648\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0929 - accuracy: 0.9670 - val_loss: 0.1036 - val_accuracy: 0.9644\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0949 - accuracy: 0.9661 - val_loss: 0.1026 - val_accuracy: 0.9644\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0948 - accuracy: 0.9659 - val_loss: 0.1056 - val_accuracy: 0.9641\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0927 - accuracy: 0.9667 - val_loss: 0.1035 - val_accuracy: 0.9639\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0898 - accuracy: 0.9684 - val_loss: 0.1038 - val_accuracy: 0.9644\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0900 - accuracy: 0.9686 - val_loss: 0.1021 - val_accuracy: 0.9646\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0892 - accuracy: 0.9675 - val_loss: 0.1028 - val_accuracy: 0.9641\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0866 - accuracy: 0.9694 - val_loss: 0.1046 - val_accuracy: 0.9648\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0861 - accuracy: 0.9688 - val_loss: 0.1031 - val_accuracy: 0.9652\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0818 - accuracy: 0.9705 - val_loss: 0.1028 - val_accuracy: 0.9650\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0857 - accuracy: 0.9686 - val_loss: 0.1008 - val_accuracy: 0.9659\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0848 - accuracy: 0.9702 - val_loss: 0.1070 - val_accuracy: 0.9644\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0827 - accuracy: 0.9701 - val_loss: 0.1018 - val_accuracy: 0.9648\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0811 - accuracy: 0.9709 - val_loss: 0.1017 - val_accuracy: 0.9655\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0806 - accuracy: 0.9708 - val_loss: 0.1050 - val_accuracy: 0.9643\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0804 - accuracy: 0.9710 - val_loss: 0.1024 - val_accuracy: 0.9650\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0776 - accuracy: 0.9721 - val_loss: 0.1039 - val_accuracy: 0.9653\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0750 - accuracy: 0.9719 - val_loss: 0.1031 - val_accuracy: 0.9652\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0766 - accuracy: 0.9727 - val_loss: 0.1020 - val_accuracy: 0.9663\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0778 - accuracy: 0.9718 - val_loss: 0.1028 - val_accuracy: 0.9646\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0726 - accuracy: 0.9737 - val_loss: 0.1046 - val_accuracy: 0.9650\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0726 - accuracy: 0.9741 - val_loss: 0.1023 - val_accuracy: 0.9650\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0707 - accuracy: 0.9740 - val_loss: 0.1044 - val_accuracy: 0.9644\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0685 - accuracy: 0.9744 - val_loss: 0.1040 - val_accuracy: 0.9652\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0682 - accuracy: 0.9752 - val_loss: 0.1016 - val_accuracy: 0.9663\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0731 - accuracy: 0.9720 - val_loss: 0.1021 - val_accuracy: 0.9659\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0684 - accuracy: 0.9750 - val_loss: 0.1027 - val_accuracy: 0.9657\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0659 - accuracy: 0.9761 - val_loss: 0.1006 - val_accuracy: 0.9664\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0674 - accuracy: 0.9760 - val_loss: 0.1023 - val_accuracy: 0.9659\n",
      "Training Loss: [0.7851, 0.4494, 0.3439, 0.2928, 0.2552, 0.2379, 0.2155, 0.2033, 0.1935, 0.1845, 0.1806, 0.1789, 0.1644, 0.1637, 0.1615, 0.1637, 0.1518, 0.1492, 0.1473, 0.1389, 0.1391, 0.1351, 0.1355, 0.1309, 0.1319, 0.1255, 0.1236, 0.1212, 0.1256, 0.1192, 0.1166, 0.1134, 0.1182, 0.1144, 0.1113, 0.1106, 0.1045, 0.1065, 0.1015, 0.1029, 0.1034, 0.102, 0.0977, 0.0978, 0.09, 0.1, 0.0929, 0.0949, 0.0948, 0.0927, 0.0898, 0.09, 0.0892, 0.0866, 0.0861, 0.0818, 0.0857, 0.0848, 0.0827, 0.0811, 0.0806, 0.0804, 0.0776, 0.075, 0.0766, 0.0778, 0.0726, 0.0726, 0.0707, 0.0685, 0.0682, 0.0731, 0.0684, 0.0659, 0.0674]\n",
      "Training Accuracy: [0.6915, 0.8288, 0.8721, 0.8957, 0.9108, 0.9182, 0.9265, 0.9315, 0.9339, 0.9373, 0.9408, 0.9397, 0.9447, 0.9461, 0.9458, 0.9465, 0.9495, 0.9488, 0.95, 0.9533, 0.9528, 0.954, 0.955, 0.9546, 0.9549, 0.956, 0.9563, 0.9598, 0.9567, 0.9581, 0.9588, 0.9605, 0.9604, 0.9597, 0.9608, 0.9619, 0.9633, 0.9623, 0.9638, 0.9628, 0.9632, 0.9636, 0.9655, 0.9651, 0.9687, 0.9648, 0.967, 0.9661, 0.9659, 0.9667, 0.9684, 0.9686, 0.9675, 0.9694, 0.9688, 0.9705, 0.9686, 0.9702, 0.9701, 0.9709, 0.9708, 0.971, 0.9721, 0.9719, 0.9727, 0.9718, 0.9737, 0.9741, 0.974, 0.9744, 0.9752, 0.972, 0.975, 0.9761, 0.976]\n",
      "Validation Loss: [0.3276, 0.2073, 0.1762, 0.1612, 0.1468, 0.1395, 0.1337, 0.1322, 0.1299, 0.1271, 0.1224, 0.1254, 0.1224, 0.1221, 0.119, 0.1159, 0.1163, 0.1148, 0.1155, 0.114, 0.1181, 0.112, 0.1136, 0.1099, 0.1102, 0.1127, 0.1074, 0.1095, 0.1123, 0.1094, 0.1061, 0.1069, 0.1094, 0.1071, 0.1058, 0.1065, 0.1066, 0.1051, 0.1061, 0.1044, 0.1041, 0.1054, 0.1049, 0.1069, 0.1038, 0.1032, 0.1036, 0.1026, 0.1056, 0.1035, 0.1038, 0.1021, 0.1028, 0.1046, 0.1031, 0.1028, 0.1008, 0.107, 0.1018, 0.1017, 0.105, 0.1024, 0.1039, 0.1031, 0.102, 0.1028, 0.1046, 0.1023, 0.1044, 0.104, 0.1016, 0.1021, 0.1027, 0.1006, 0.1023]\n",
      "Validation Accuracy: [0.8639, 0.9131, 0.9305, 0.9399, 0.9472, 0.951, 0.9532, 0.9548, 0.9546, 0.9561, 0.9565, 0.9577, 0.9581, 0.9583, 0.9597, 0.9606, 0.9606, 0.9604, 0.9606, 0.9612, 0.9604, 0.961, 0.9612, 0.9619, 0.963, 0.9626, 0.9637, 0.9626, 0.9624, 0.9623, 0.9628, 0.9634, 0.9626, 0.9632, 0.9632, 0.9628, 0.9628, 0.9637, 0.9634, 0.9646, 0.9646, 0.9639, 0.9644, 0.9643, 0.9644, 0.9648, 0.9644, 0.9644, 0.9641, 0.9639, 0.9644, 0.9646, 0.9641, 0.9648, 0.9652, 0.965, 0.9659, 0.9644, 0.9648, 0.9655, 0.9643, 0.965, 0.9653, 0.9652, 0.9663, 0.9646, 0.965, 0.965, 0.9644, 0.9652, 0.9663, 0.9659, 0.9657, 0.9664, 0.9659]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:123: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now training cross-validation set # 3\n",
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 54s 2ms/step - loss: 0.7612 - accuracy: 0.7068 - val_loss: 0.3721 - val_accuracy: 0.8380\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.4484 - accuracy: 0.8320 - val_loss: 0.2241 - val_accuracy: 0.9100\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.3502 - accuracy: 0.8697 - val_loss: 0.1856 - val_accuracy: 0.9292\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2864 - accuracy: 0.8952 - val_loss: 0.1686 - val_accuracy: 0.9376\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2545 - accuracy: 0.9107 - val_loss: 0.1558 - val_accuracy: 0.9427\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2279 - accuracy: 0.9199 - val_loss: 0.1489 - val_accuracy: 0.9461\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2245 - accuracy: 0.9257 - val_loss: 0.1459 - val_accuracy: 0.9481\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.2014 - accuracy: 0.9339 - val_loss: 0.1421 - val_accuracy: 0.9501\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1937 - accuracy: 0.9346 - val_loss: 0.1394 - val_accuracy: 0.9517\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1805 - accuracy: 0.9400 - val_loss: 0.1376 - val_accuracy: 0.9534\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1777 - accuracy: 0.9412 - val_loss: 0.1371 - val_accuracy: 0.9541\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1681 - accuracy: 0.9427 - val_loss: 0.1342 - val_accuracy: 0.9565\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1635 - accuracy: 0.9455 - val_loss: 0.1322 - val_accuracy: 0.9566\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1625 - accuracy: 0.9464 - val_loss: 0.1324 - val_accuracy: 0.9570\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1607 - accuracy: 0.9481 - val_loss: 0.1321 - val_accuracy: 0.9583\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1442 - accuracy: 0.9516 - val_loss: 0.1316 - val_accuracy: 0.9595\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1484 - accuracy: 0.9516 - val_loss: 0.1298 - val_accuracy: 0.9603\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1492 - accuracy: 0.9508 - val_loss: 0.1283 - val_accuracy: 0.9610\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1392 - accuracy: 0.9533 - val_loss: 0.1286 - val_accuracy: 0.9621\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1373 - accuracy: 0.9544 - val_loss: 0.1274 - val_accuracy: 0.9615\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1416 - accuracy: 0.9526 - val_loss: 0.1290 - val_accuracy: 0.9612\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1391 - accuracy: 0.9531 - val_loss: 0.1247 - val_accuracy: 0.9624\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1315 - accuracy: 0.9555 - val_loss: 0.1255 - val_accuracy: 0.9623\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1331 - accuracy: 0.9555 - val_loss: 0.1264 - val_accuracy: 0.9615\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1284 - accuracy: 0.9567 - val_loss: 0.1253 - val_accuracy: 0.9623\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1290 - accuracy: 0.9573 - val_loss: 0.1246 - val_accuracy: 0.9632\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1253 - accuracy: 0.9573 - val_loss: 0.1235 - val_accuracy: 0.9626\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1226 - accuracy: 0.9587 - val_loss: 0.1238 - val_accuracy: 0.9621\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1249 - accuracy: 0.9586 - val_loss: 0.1229 - val_accuracy: 0.9632\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1199 - accuracy: 0.9589 - val_loss: 0.1238 - val_accuracy: 0.9637\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1118 - accuracy: 0.9615 - val_loss: 0.1233 - val_accuracy: 0.9630\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1154 - accuracy: 0.9607 - val_loss: 0.1230 - val_accuracy: 0.9639\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1140 - accuracy: 0.9611 - val_loss: 0.1229 - val_accuracy: 0.9634\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1118 - accuracy: 0.9604 - val_loss: 0.1227 - val_accuracy: 0.9637\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1122 - accuracy: 0.9612 - val_loss: 0.1218 - val_accuracy: 0.9644\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1069 - accuracy: 0.9638 - val_loss: 0.1210 - val_accuracy: 0.9641\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1131 - accuracy: 0.9609 - val_loss: 0.1213 - val_accuracy: 0.9632\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1061 - accuracy: 0.9622 - val_loss: 0.1210 - val_accuracy: 0.9635\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1039 - accuracy: 0.9637 - val_loss: 0.1220 - val_accuracy: 0.9635\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1077 - accuracy: 0.9637 - val_loss: 0.1218 - val_accuracy: 0.9641\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1042 - accuracy: 0.9633 - val_loss: 0.1213 - val_accuracy: 0.9641\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.1001 - accuracy: 0.9637 - val_loss: 0.1207 - val_accuracy: 0.9644\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0954 - accuracy: 0.9673 - val_loss: 0.1201 - val_accuracy: 0.9646\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0981 - accuracy: 0.9654 - val_loss: 0.1199 - val_accuracy: 0.9655\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0953 - accuracy: 0.9669 - val_loss: 0.1195 - val_accuracy: 0.9648\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0962 - accuracy: 0.9646 - val_loss: 0.1194 - val_accuracy: 0.9653\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0940 - accuracy: 0.9666 - val_loss: 0.1192 - val_accuracy: 0.9641\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0942 - accuracy: 0.9664 - val_loss: 0.1197 - val_accuracy: 0.9641\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0896 - accuracy: 0.9691 - val_loss: 0.1190 - val_accuracy: 0.9643\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0890 - accuracy: 0.9692 - val_loss: 0.1210 - val_accuracy: 0.9652\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0938 - accuracy: 0.9678 - val_loss: 0.1197 - val_accuracy: 0.9653\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0841 - accuracy: 0.9696 - val_loss: 0.1192 - val_accuracy: 0.9650\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0898 - accuracy: 0.9694 - val_loss: 0.1194 - val_accuracy: 0.9655\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0865 - accuracy: 0.9693 - val_loss: 0.1196 - val_accuracy: 0.9657\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0825 - accuracy: 0.9702 - val_loss: 0.1189 - val_accuracy: 0.9648\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0837 - accuracy: 0.9706 - val_loss: 0.1192 - val_accuracy: 0.9648\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0854 - accuracy: 0.9686 - val_loss: 0.1185 - val_accuracy: 0.9657\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0818 - accuracy: 0.9702 - val_loss: 0.1199 - val_accuracy: 0.9639\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0782 - accuracy: 0.9725 - val_loss: 0.1197 - val_accuracy: 0.9652\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0794 - accuracy: 0.9706 - val_loss: 0.1200 - val_accuracy: 0.9650\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0772 - accuracy: 0.9719 - val_loss: 0.1209 - val_accuracy: 0.9644\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0801 - accuracy: 0.9709 - val_loss: 0.1198 - val_accuracy: 0.9653\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.0798 - accuracy: 0.9711 - val_loss: 0.1206 - val_accuracy: 0.9648\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0751 - accuracy: 0.9733 - val_loss: 0.1197 - val_accuracy: 0.9650\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0750 - accuracy: 0.9727 - val_loss: 0.1209 - val_accuracy: 0.9661\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0731 - accuracy: 0.9733 - val_loss: 0.1207 - val_accuracy: 0.9652\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0748 - accuracy: 0.9724 - val_loss: 0.1205 - val_accuracy: 0.9664\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0747 - accuracy: 0.9724 - val_loss: 0.1209 - val_accuracy: 0.9652\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0735 - accuracy: 0.9731 - val_loss: 0.1198 - val_accuracy: 0.9657\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0723 - accuracy: 0.9726 - val_loss: 0.1210 - val_accuracy: 0.9652\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0694 - accuracy: 0.9752 - val_loss: 0.1206 - val_accuracy: 0.9661\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0689 - accuracy: 0.9755 - val_loss: 0.1203 - val_accuracy: 0.9664\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0713 - accuracy: 0.9748 - val_loss: 0.1208 - val_accuracy: 0.9661\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0647 - accuracy: 0.9760 - val_loss: 0.1210 - val_accuracy: 0.9666\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 52s 2ms/step - loss: 0.0666 - accuracy: 0.9755 - val_loss: 0.1214 - val_accuracy: 0.9650\n",
      "Training Loss: [0.7612, 0.4484, 0.3502, 0.2864, 0.2545, 0.2279, 0.2245, 0.2014, 0.1937, 0.1805, 0.1777, 0.1681, 0.1635, 0.1625, 0.1607, 0.1442, 0.1484, 0.1492, 0.1392, 0.1373, 0.1416, 0.1391, 0.1315, 0.1331, 0.1284, 0.129, 0.1253, 0.1226, 0.1249, 0.1199, 0.1118, 0.1154, 0.114, 0.1118, 0.1122, 0.1069, 0.1131, 0.1061, 0.1039, 0.1077, 0.1042, 0.1001, 0.0954, 0.0981, 0.0953, 0.0962, 0.094, 0.0942, 0.0896, 0.089, 0.0938, 0.0841, 0.0898, 0.0865, 0.0825, 0.0837, 0.0854, 0.0818, 0.0782, 0.0794, 0.0772, 0.0801, 0.0798, 0.0751, 0.075, 0.0731, 0.0748, 0.0747, 0.0735, 0.0723, 0.0694, 0.0689, 0.0713, 0.0647, 0.0666]\n",
      "Training Accuracy: [0.7068, 0.832, 0.8697, 0.8952, 0.9107, 0.9199, 0.9257, 0.9339, 0.9346, 0.94, 0.9412, 0.9427, 0.9455, 0.9464, 0.9481, 0.9516, 0.9516, 0.9508, 0.9533, 0.9544, 0.9526, 0.9531, 0.9555, 0.9555, 0.9567, 0.9573, 0.9573, 0.9587, 0.9586, 0.9589, 0.9615, 0.9607, 0.9611, 0.9604, 0.9612, 0.9638, 0.9609, 0.9622, 0.9637, 0.9637, 0.9633, 0.9637, 0.9673, 0.9654, 0.9669, 0.9646, 0.9666, 0.9664, 0.9691, 0.9692, 0.9678, 0.9696, 0.9694, 0.9693, 0.9702, 0.9706, 0.9686, 0.9702, 0.9725, 0.9706, 0.9719, 0.9709, 0.9711, 0.9733, 0.9727, 0.9733, 0.9724, 0.9724, 0.9731, 0.9726, 0.9752, 0.9755, 0.9748, 0.976, 0.9755]\n",
      "Validation Loss: [0.3721, 0.2241, 0.1856, 0.1686, 0.1558, 0.1489, 0.1459, 0.1421, 0.1394, 0.1376, 0.1371, 0.1342, 0.1322, 0.1324, 0.1321, 0.1316, 0.1298, 0.1283, 0.1286, 0.1274, 0.129, 0.1247, 0.1255, 0.1264, 0.1253, 0.1246, 0.1235, 0.1238, 0.1229, 0.1238, 0.1233, 0.123, 0.1229, 0.1227, 0.1218, 0.121, 0.1213, 0.121, 0.122, 0.1218, 0.1213, 0.1207, 0.1201, 0.1199, 0.1195, 0.1194, 0.1192, 0.1197, 0.119, 0.121, 0.1197, 0.1192, 0.1194, 0.1196, 0.1189, 0.1192, 0.1185, 0.1199, 0.1197, 0.12, 0.1209, 0.1198, 0.1206, 0.1197, 0.1209, 0.1207, 0.1205, 0.1209, 0.1198, 0.121, 0.1206, 0.1203, 0.1208, 0.121, 0.1214]\n",
      "Validation Accuracy: [0.838, 0.91, 0.9292, 0.9376, 0.9427, 0.9461, 0.9481, 0.9501, 0.9517, 0.9534, 0.9541, 0.9565, 0.9566, 0.957, 0.9583, 0.9595, 0.9603, 0.961, 0.9621, 0.9615, 0.9612, 0.9624, 0.9623, 0.9615, 0.9623, 0.9632, 0.9626, 0.9621, 0.9632, 0.9637, 0.963, 0.9639, 0.9634, 0.9637, 0.9644, 0.9641, 0.9632, 0.9635, 0.9635, 0.9641, 0.9641, 0.9644, 0.9646, 0.9655, 0.9648, 0.9653, 0.9641, 0.9641, 0.9643, 0.9652, 0.9653, 0.965, 0.9655, 0.9657, 0.9648, 0.9648, 0.9657, 0.9639, 0.9652, 0.965, 0.9644, 0.9653, 0.9648, 0.965, 0.9661, 0.9652, 0.9664, 0.9652, 0.9657, 0.9652, 0.9661, 0.9664, 0.9661, 0.9666, 0.965]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:123: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now training cross-validation set # 4\n",
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 55s 2ms/step - loss: 0.7521 - accuracy: 0.7147 - val_loss: 0.4931 - val_accuracy: 0.7533\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.4425 - accuracy: 0.8291 - val_loss: 0.2389 - val_accuracy: 0.9022\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.3395 - accuracy: 0.8750 - val_loss: 0.2017 - val_accuracy: 0.9233\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.2773 - accuracy: 0.8993 - val_loss: 0.1874 - val_accuracy: 0.9309\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.2455 - accuracy: 0.9144 - val_loss: 0.1750 - val_accuracy: 0.9398\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.2263 - accuracy: 0.9214 - val_loss: 0.1638 - val_accuracy: 0.9434\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.2065 - accuracy: 0.9266 - val_loss: 0.1602 - val_accuracy: 0.9459\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.2022 - accuracy: 0.9307 - val_loss: 0.1571 - val_accuracy: 0.9478\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1857 - accuracy: 0.9354 - val_loss: 0.1541 - val_accuracy: 0.9497\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1750 - accuracy: 0.9389 - val_loss: 0.1501 - val_accuracy: 0.9497\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1742 - accuracy: 0.9423 - val_loss: 0.1471 - val_accuracy: 0.9519\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1660 - accuracy: 0.9442 - val_loss: 0.1451 - val_accuracy: 0.9516\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 53s 2ms/step - loss: 0.1650 - accuracy: 0.9458 - val_loss: 0.1352 - val_accuracy: 0.9554\n",
      "Epoch 14/75\n",
      "15424/22046 [===================>..........] - ETA: 14s - loss: 0.1569 - accuracy: 0.9466"
     ]
    }
   ],
   "source": [
    "# Import relevant packages for neural network training\n",
    "import sys\n",
    "import csv\n",
    "if 'tensorflow' in sys.modules == False:\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "# Create empty lists to store results\n",
    "TrainLoss = []\n",
    "TrainAcc = []\n",
    "TestLoss = []\n",
    "TestAcc = []\n",
    "All_FPR = []\n",
    "All_TPR = []\n",
    "All_thresholds = []\n",
    "All_AUC = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    # Create the appropriate training and testing sets\n",
    "    if i == 0:\n",
    "        TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index4,:]), axis=0)\n",
    "        TestImages = Dataset[Index5,:]\n",
    "        TestLabels = Labels[Index5,:]\n",
    "    elif i == 1:\n",
    "        TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index5,:]), axis=0)\n",
    "        TestImages = Dataset[Index4,:]\n",
    "        TestLabels = Labels[Index4,:]\n",
    "    elif i == 2:\n",
    "        TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "        TestImages = Dataset[Index3,:]\n",
    "        TestLabels = Labels[Index3,:]\n",
    "    elif i == 3:\n",
    "        TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "        TestImages = Dataset[Index2,:]\n",
    "        TestLabels = Labels[Index2,:]\n",
    "    else:\n",
    "        TrainImages = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index2,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "        TestImages = Dataset[Index1,:]\n",
    "        TestLabels = Labels[Index1,:]\n",
    "\n",
    "    base_model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (128, 128, 3))\n",
    "\n",
    "    # Store the fully connected layers\n",
    "    Input = base_model.layers[0]\n",
    "    ConvBlock1_1 = base_model.layers[1]\n",
    "    ConvBlock1_2 = base_model.layers[2]\n",
    "    ConvBlock1_Pool = base_model.layers[3]\n",
    "\n",
    "    ConvBlock2_1 = base_model.layers[4]\n",
    "    ConvBlock2_2 = base_model.layers[5]\n",
    "    ConvBlock2_Pool = base_model.layers[6]\n",
    "    ConvBlock3_1 = base_model.layers[7]\n",
    "    ConvBlock3_2 = base_model.layers[8]\n",
    "    ConvBlock3_3 = base_model.layers[9]\n",
    "    ConvBlock3_Pool = base_model.layers[10]\n",
    "    ConvBlock4_1 = base_model.layers[11]\n",
    "    ConvBlock4_2 = base_model.layers[12]\n",
    "    ConvBlock4_3 = base_model.layers[13]\n",
    "    ConvBlock4_Pool = base_model.layers[14]\n",
    "    ConvBlock5_1 = base_model.layers[15]\n",
    "    ConvBlock5_2 = base_model.layers[16]\n",
    "    ConvBlock5_3 = base_model.layers[17]\n",
    "    ConvBlock5_Pool = base_model.layers[18]\n",
    "\n",
    "    #NewLayer = ConvBlock5_3(ConvBlock5_2)\n",
    "\n",
    "\n",
    "    # Reconstructing neural network architecture w/ batch normalization\n",
    "    x = Input.output\n",
    "    x = ConvBlock1_1(x)\n",
    "    x = ConvBlock1_2(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ConvBlock1_Pool(x)\n",
    "    x = ConvBlock2_1(x)\n",
    "    x = ConvBlock2_2(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ConvBlock2_Pool(x)\n",
    "    x = ConvBlock3_1(x)\n",
    "    x = ConvBlock3_2(x)\n",
    "    x = ConvBlock3_3(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ConvBlock3_Pool(x)\n",
    "    x = ConvBlock4_1(x)\n",
    "    x = ConvBlock4_2(x)\n",
    "    x = ConvBlock4_3(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ConvBlock4_Pool(x)\n",
    "    x = ConvBlock5_1(x)\n",
    "    x = ConvBlock5_2(x)\n",
    "    x = ConvBlock5_3(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ConvBlock5_Pool(x)\n",
    "\n",
    "    ### INSERT REST OF LAYERS HERE ###\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x) # INSERT BATCH NORMALIZATION LAYER HERE\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x) # INSERT BATCH NORMALIZATION LAYER HERE\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(2, activation=\"softmax\")(x)\n",
    "    model = Model(input = base_model.input, output = predictions)\n",
    "    sgd = optimizers.SGD(learning_rate=1e-5, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train model and evaluate performance\n",
    "    print('We are now training cross-validation set #',i+1)\n",
    "    Results = model.fit(TrainImages, TrainLabels, epochs=75, batch_size=64, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
    "\n",
    "    # Display and store performance results\n",
    "    Results.history['loss'] = [round(l, 4) for l in Results.history['loss']]\n",
    "    Results.history['accuracy'] = [round(l, 4) for l in Results.history['accuracy']]\n",
    "    Results.history['val_loss'] = [round(l, 4) for l in Results.history['val_loss']]\n",
    "    Results.history['val_accuracy'] = [round(l, 4) for l in Results.history['val_accuracy']]\n",
    "\n",
    "    print('Training Loss:',Results.history['loss'])\n",
    "    print('Training Accuracy:',Results.history['accuracy'])\n",
    "    print('Validation Loss:',Results.history['val_loss'])\n",
    "    print('Validation Accuracy:',Results.history['val_accuracy'])\n",
    "\n",
    "    TrainLoss.append(Results.history['loss'])\n",
    "    TrainAcc.append(Results.history['accuracy'])\n",
    "    TestLoss.append(Results.history['val_loss'])\n",
    "    TestAcc.append(Results.history['val_accuracy'])\n",
    "    print('')\n",
    "\n",
    "\n",
    "    # Predict values for test set\n",
    "    Probabilities = model.predict(TestImages)\n",
    "\n",
    "    # Calculate data for ROC curve\n",
    "    FPR, TPR, thresholds = roc_curve(TestLabels[:,1], Probabilities[:,1])\n",
    "    All_FPR.append(FPR)\n",
    "    All_TPR.append(TPR)\n",
    "    All_thresholds.append(thresholds)\n",
    "\n",
    "# Save and export as CSV files\n",
    "with open(\"BatchNorm_TrainLoss.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(TrainLoss)\n",
    "with open(\"BatchNorm_TrainAcc.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(TrainAcc)\n",
    "with open(\"BatchNorm_TestLoss.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(TestLoss)\n",
    "with open(\"BatchNorm_TestAcc.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(TestAcc)\n",
    "with open(\"BatchNorm_FPR.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(All_FPR)\n",
    "with open(\"BatchNorm_TPR.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(All_TPR)\n",
    "with open(\"BatchNorm_Thresholds.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(All_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages for neural network training\n",
    "import sys\n",
    "import csv\n",
    "if 'tensorflow' in sys.modules == False:\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "# Create empty lists to store results\n",
    "TrainLoss = []\n",
    "TrainAcc = []\n",
    "TestLoss = []\n",
    "TestAcc = []\n",
    "All_FPR = []\n",
    "All_TPR = []\n",
    "All_thresholds = []\n",
    "All_AUC = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    # Create the appropriate training and testing sets\n",
    "    if i == 0:\n",
    "        TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index4,:]), axis=0)\n",
    "        TestImages = Dataset[Index5,:]\n",
    "        TestLabels = Labels[Index5,:]\n",
    "    elif i == 1:\n",
    "        TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index5,:]), axis=0)\n",
    "        TestImages = Dataset[Index4,:]\n",
    "        TestLabels = Labels[Index4,:]\n",
    "    elif i == 2:\n",
    "        TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "        TestImages = Dataset[Index3,:]\n",
    "        TestLabels = Labels[Index3,:]\n",
    "    elif i == 3:\n",
    "        TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "        TestImages = Dataset[Index2,:]\n",
    "        TestLabels = Labels[Index2,:]\n",
    "    else:\n",
    "        TrainImages = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "        TrainLabels = np.concatenate((Labels[Index2,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "        TestImages = Dataset[Index1,:]\n",
    "        TestLabels = Labels[Index1,:]\n",
    "\n",
    "    base_model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (128, 128, 3))\n",
    "\n",
    "    # Reconstructing neural network architecture w/ batch normalization\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x) # INSERT BATCH NORMALIZATION LAYER HERE\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x) # INSERT BATCH NORMALIZATION LAYER HERE\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(2, activation=\"softmax\")(x)\n",
    "    model = Model(input = base_model.input, output = predictions)\n",
    "    sgd = optimizers.SGD(learning_rate=1e-5, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train model and evaluate performance\n",
    "    print('We are now training cross-validation set #',i+1)\n",
    "    Results = model.fit(TrainImages, TrainLabels, epochs=75, batch_size=64, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
    "\n",
    "    # Display and store performance results\n",
    "    Results.history['loss'] = [round(l, 4) for l in Results.history['loss']]\n",
    "    Results.history['accuracy'] = [round(l, 4) for l in Results.history['accuracy']]\n",
    "    Results.history['val_loss'] = [round(l, 4) for l in Results.history['val_loss']]\n",
    "    Results.history['val_accuracy'] = [round(l, 4) for l in Results.history['val_accuracy']]\n",
    "\n",
    "    print('Training Loss:',Results.history['loss'])\n",
    "    print('Training Accuracy:',Results.history['accuracy'])\n",
    "    print('Validation Loss:',Results.history['val_loss'])\n",
    "    print('Validation Accuracy:',Results.history['val_accuracy'])\n",
    "\n",
    "    TrainLoss.append(Results.history['loss'])\n",
    "    TrainAcc.append(Results.history['accuracy'])\n",
    "    TestLoss.append(Results.history['val_loss'])\n",
    "    TestAcc.append(Results.history['val_accuracy'])\n",
    "    print('')\n",
    "\n",
    "\n",
    "    # Predict values for test set\n",
    "    Probabilities = model.predict(TestImages)\n",
    "\n",
    "    # Calculate data for ROC curve\n",
    "    FPR, TPR, thresholds = roc_curve(TestLabels[:,1], Probabilities[:,1])\n",
    "    All_FPR.append(FPR)\n",
    "    All_TPR.append(TPR)\n",
    "    All_thresholds.append(thresholds)\n",
    "\n",
    "# Save and export as CSV files\n",
    "with open(\"NoBatchNorm_TrainLoss.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(TrainLoss)\n",
    "with open(\"NoBatchNorm_TrainAcc.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(TrainAcc)\n",
    "with open(\"NoBatchNorm_TestLoss.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(TestLoss)\n",
    "with open(\"NoBatchNorm_TestAcc.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(TestAcc)\n",
    "with open(\"NoBatchNorm_FPR.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(All_FPR)\n",
    "with open(\"NoBatchNorm_TPR.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(All_TPR)\n",
    "with open(\"NoBatchNorm_Thresholds.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(All_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine RAM Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine RAM Usage\n",
    "import sys\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
