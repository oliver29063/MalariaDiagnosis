{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Architecture Testing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBUjoX1q6kfm",
        "colab_type": "text"
      },
      "source": [
        "# INFORMATION TO USER\n",
        "The notebook is used to generate the results at the following GitHub folders via the **Google Cloud Computing Platform**:\n",
        "\n",
        "- https://github.com/oliver29063/MalariaDiagnosis/tree/master/DenseNodes\n",
        "- https://github.com/oliver29063/MalariaDiagnosis/tree/master/ActivationFunctions\n",
        "- https://github.com/oliver29063/MalariaDiagnosis/tree/master/Dropout\n",
        "\n",
        "These results are also referred to in **Figure 2** of the manuscript in the **Results** subsection titled **Evaluating Pre-Trained Neural Network Architectures**. In short, this notebook is used to determine the optimal pre-trained CNN.\n",
        "\n",
        "Please note that this notebook requires about 24 GB of RAM to run properly, which exceeds Google Colab capacity and most personal laptops. Consequently, please adjust the number of images used to reduce RAM is necessary when testing the functionality of this notebook. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U-9hf2B7-02",
        "colab_type": "text"
      },
      "source": [
        "# Package Versions\n",
        "Standard Libraries for Python 3.6.9\n",
        "- os\n",
        "- shutil\n",
        "- zipfile\n",
        "- sys\n",
        "- csv\n",
        "\n",
        "Imported Libraries\n",
        "- numpy: 1.18.5\n",
        "- tensorflow: 2.2.0\n",
        "- keras: 2.3.1\n",
        "- sklearn: 0.22.2.post1\n",
        "- cv2: 4.1.2\n",
        "- PIL: 7.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px0nG1HU4dKu",
        "colab_type": "text"
      },
      "source": [
        "### Download NIH Malaria Dataset\n",
        "For more information about the dataset, visit https://lhncbc.nlm.nih.gov/publication/pub9932"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IwWg3TPzlVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "9926c5f0-48de-4413-f5bd-8d484b1a433c"
      },
      "source": [
        "# Import relevant packages\n",
        "import numpy as np\n",
        "import os\n",
        "from shutil import copyfile\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Download NIH dataset zip file\n",
        "!wget -nc ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
        "\n",
        "# Extract images if not already extracted\n",
        "ROOT_DIR = os.path.join(\"/\", \"content\")\n",
        "if not os.path.isdir(\"cell_images\"):\n",
        "    print(\"Extracting images...\")\n",
        "    with ZipFile(os.path.join(\"cell_images.zip\"), \"r\") as zipObj:\n",
        "        zipObj.extractall()\n",
        "    print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-06 22:26:12--  ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
            "           => ‘cell_images.zip’\n",
            "Resolving lhcftp.nlm.nih.gov (lhcftp.nlm.nih.gov)... 130.14.55.35, 2607:f220:41e:7055::35\n",
            "Connecting to lhcftp.nlm.nih.gov (lhcftp.nlm.nih.gov)|130.14.55.35|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /Open-Access-Datasets/Malaria ... done.\n",
            "==> SIZE cell_images.zip ... 353452851\n",
            "==> PASV ... done.    ==> RETR cell_images.zip ... done.\n",
            "Length: 353452851 (337M) (unauthoritative)\n",
            "\n",
            "cell_images.zip     100%[===================>] 337.08M  19.5MB/s    in 16s     \n",
            "\n",
            "2020-07-06 22:26:28 (20.9 MB/s) - ‘cell_images.zip’ saved [353452851]\n",
            "\n",
            "Extracting images...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SQ_fb1B4pnp",
        "colab_type": "text"
      },
      "source": [
        "### Specify Number of Images to Use\n",
        "In the manuscript we used 10000 images instead of the full set of approximately 22000 images to reduce computational burden. To check if the code runs properly on your device, we first recommend setting ```numImg``` equal to some small amount, such as 1000 to prevent RAM overload. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1o_Yy3K2ae_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numImg = 1000\n",
        "numClass = numImg//2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3OEdQR65EPc",
        "colab_type": "text"
      },
      "source": [
        "### Basic Image Preprocessing\n",
        "Here we just load our image data into two NumPy arrays ```Parasitized``` and ```Uninfected```, which each contain our set of 128x128 RGB images in a 4D NumPy array for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QtH1pvmzpi4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "62c25f5b-8b2a-4a50-ffa3-7dd395d33921"
      },
      "source": [
        "# Install and import relevant packages\n",
        "import numpy as np\n",
        "import os\n",
        "!pip install opencv-python\n",
        "!apt update && apt install -y libsm6 libxext6 libxrender1\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Create new folders to save rescaled images\n",
        "if not os.path.isdir(\"RescaledSet\"):\n",
        "    os.mkdir(\"RescaledSet\")\n",
        "if not os.path.isdir(\"RescaledSet/Parasitized\"):\n",
        "    os.mkdir(\"RescaledSet/Parasitized\")\n",
        "if not os.path.isdir(\"RescaledSet/Uninfected\"):\n",
        "    os.mkdir(\"RescaledSet/Uninfected\")\n",
        "\n",
        "# Generate list of parasitized file names\n",
        "ParasitizedFiles = os.listdir(\"cell_images/Parasitized/\")\n",
        "UninfectedFiles = os.listdir(\"cell_images/Uninfected/\")\n",
        "\n",
        "# Remove Thumb.db files\n",
        "while 'Thumbs.db' in ParasitizedFiles: ParasitizedFiles.remove('Thumbs.db')   \n",
        "while 'Thumbs.db' in UninfectedFiles: UninfectedFiles.remove('Thumbs.db')  \n",
        "\n",
        "# Pre-allocate memory space for images\n",
        "Parasitized = np.empty([numClass,128,128,3])\n",
        "Uninfected = np.empty([numClass,128,128,3])\n",
        "\n",
        "# Resize and load parasitized images\n",
        "for i in range(numClass):\n",
        "    TempImage = cv2.imread('cell_images/Parasitized/'+ParasitizedFiles[i])\n",
        "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
        "    Parasitized[i,:,:,:] = ResizedImage\n",
        "\n",
        "# Resize and load uninfected images\n",
        "for i in range(numClass):\n",
        "    TempImage = cv2.imread('cell_images/Uninfected/'+UninfectedFiles[i])\n",
        "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
        "    Uninfected[i,:,:,:] = ResizedImage\n",
        "    \n",
        "print('Uninfected Dataset size is:',np.shape(Uninfected))\n",
        "print('Parasitized Dataset size is:',np.shape(Parasitized))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [93.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [41.2 kB]\n",
            "Get:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,845 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [867 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,405 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [87.8 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,282 B]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [996 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [13.6 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [102 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,301 kB]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [890 kB]\n",
            "Fetched 7,925 kB in 3s (2,897 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.2-1).\n",
            "libxext6 is already the newest version (2:1.3.3-1).\n",
            "libxrender1 is already the newest version (1:0.9.10-1).\n",
            "libxrender1 set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n",
            "Uninfected Dataset size is: (500, 128, 128, 3)\n",
            "Parasitized Dataset size is: (500, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk7kmBIl5Qay",
        "colab_type": "text"
      },
      "source": [
        "### Create Model Inputs and Cross-Validation Groups\n",
        "Now we create our ```Labels``` as our ground truth and merge our two image classes into a single NumPy array ```Dataset```. We then randomize the order of the images and create our five cross-validation set indices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR7JDUPO14lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate dataset labels\n",
        "ParasitizedLabels = np.repeat([[0,1]], numClass, axis=0)\n",
        "UninfectedLabels = np.repeat([[1,0]], numClass, axis=0)\n",
        "Labels = np.concatenate((ParasitizedLabels,UninfectedLabels), axis=0)\n",
        "\n",
        "# Generate image dataset\n",
        "Dataset = np.concatenate((Parasitized, Uninfected), axis=0)\n",
        "\n",
        "# Generate 5-fold cross-validation groups\n",
        "Spaces = np.linspace(0,numImg,6).astype('int')\n",
        "CVIndices = np.random.permutation(Dataset.shape[0])\n",
        "Index1, Index2, Index3, Index4, Index5 = CVIndices[:Spaces[1]], CVIndices[Spaces[1]:Spaces[2]], CVIndices[Spaces[2]:Spaces[3]], CVIndices[Spaces[3]:Spaces[4]], CVIndices[Spaces[4]:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtNfsWCk5jro",
        "colab_type": "text"
      },
      "source": [
        "### Import CNN Models\n",
        "Here we import our pretrained CNN models from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAX-ZEtb2AWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant neural network architecture packages \n",
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "\n",
        "# List of model names and import functions\n",
        "ModelNames = ['ResNet50V2','VGG16','VGG19','InceptionV3','Xception','InceptionResNetV2','DenseNet121','MobileNetV2']\n",
        "ModelImport = [InceptionV3(weights = \"imagenet\", include_top=False, input_shape = (128,128,3)),\n",
        "              VGG16(weights = \"imagenet\", include_top=False, input_shape = (128,128,3)),\n",
        "              VGG19(weights = \"imagenet\", include_top=False, input_shape = (128,128,3)),\n",
        "              InceptionV3(weights = \"imagenet\", include_top=False, input_shape = (128,128,3)),\n",
        "              Xception(weights = \"imagenet\", include_top=False, input_shape = (128,128,3)),\n",
        "              InceptionResNetV2(weights = \"imagenet\", include_top=False, input_shape = (128,128,3)),\n",
        "              DenseNet121(weights = \"imagenet\", include_top=False, input_shape = (128,128,3)),\n",
        "              MobileNetV2(weights = \"imagenet\", include_top=False, input_shape = (128,128,3))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbUAQvoX8D6c",
        "colab_type": "text"
      },
      "source": [
        "### Import Relevant Packages for Neural Network Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_9yvNfU8FtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "91b1d45f-dafa-4b2d-d70c-2e52c541c5be"
      },
      "source": [
        "# Import relevant packages for neural network training\n",
        "import sys\n",
        "import csv\n",
        "if 'tensorflow' in sys.modules == False:\n",
        "    %tensorflow_version 2.x\n",
        "    import tensorflow as tf\n",
        "import keras\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "\n",
        "!pip install scikit-learn\n",
        "import sklearn\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMfOVu2K8HiS",
        "colab_type": "text"
      },
      "source": [
        "### Test Different Pre-Trained CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsNG7ySd2HIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for j in ModelImport:\n",
        "\n",
        "    # Create empty lists to store results\n",
        "    TrainLoss = []\n",
        "    TrainAcc = []\n",
        "    TestLoss = []\n",
        "    TestAcc = []\n",
        "    All_FPR = []\n",
        "    All_TPR = []\n",
        "    All_thresholds = []\n",
        "    All_AUC = []\n",
        "\n",
        "    for i in range(5):\n",
        "\n",
        "        # Create the appropriate training and testing sets\n",
        "        if i == 0:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index4,:]), axis=0)\n",
        "            TestImages = Dataset[Index5,:]\n",
        "            TestLabels = Labels[Index5,:]\n",
        "        elif i == 1:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index4,:]\n",
        "            TestLabels = Labels[Index4,:]\n",
        "        elif i == 2:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index3,:]\n",
        "            TestLabels = Labels[Index3,:]\n",
        "        elif i == 3:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index2,:]\n",
        "            TestLabels = Labels[Index2,:]\n",
        "        else:\n",
        "            TrainImages = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index2,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index1,:]\n",
        "            TestLabels = Labels[Index1,:]\n",
        "\n",
        "        base_model = j\n",
        "\n",
        "        x = base_model.output\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(512, activation=\"relu\")(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(512, activation=\"relu\")(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        predictions = Dense(2, activation=\"softmax\")(x)\n",
        "        model = Model(input = base_model.input, output = predictions)\n",
        "        adam = optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "        model.compile(loss = \"categorical_crossentropy\", optimizer = adam, metrics=[\"accuracy\"])\n",
        "\n",
        "        # Train model and evaluate performance\n",
        "        print('We are now training cross-validation set #',i+1)\n",
        "        Results = model.fit(TrainImages, TrainLabels, epochs=75, batch_size=64, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
        "\n",
        "        # Display and store performance results\n",
        "        Results.history['loss'] = [round(k, 4) for k in Results.history['loss']]\n",
        "        Results.history['accuracy'] = [round(k, 4) for k in Results.history['accuracy']]\n",
        "        Results.history['val_loss'] = [round(k, 4) for k in Results.history['val_loss']]\n",
        "        Results.history['val_accuracy'] = [round(k, 4) for k in Results.history['val_accuracy']]\n",
        "        \n",
        "        print('Training Loss:',Results.history['loss'])\n",
        "        print('Training Accuracy:',Results.history['accuracy'])\n",
        "        print('Validation Loss:',Results.history['val_loss'])\n",
        "        print('Validation Accuracy:',Results.history['val_accuracy'])\n",
        "        \n",
        "        TrainLoss.append(Results.history['loss'])\n",
        "        TrainAcc.append(Results.history['accuracy'])\n",
        "        TestLoss.append(Results.history['val_loss'])\n",
        "        TestAcc.append(Results.history['val_accuracy'])\n",
        "        print('')\n",
        "\n",
        "\n",
        "        # Predict values for test set\n",
        "        Probabilities = model.predict(TestImages)\n",
        "\n",
        "        # Calculate data for ROC curve\n",
        "        FPR, TPR, thresholds = roc_curve(TestLabels[:,1], Probabilities[:,1])\n",
        "        All_FPR.append(FPR)\n",
        "        All_TPR.append(TPR)\n",
        "        All_thresholds.append(thresholds)\n",
        "        \n",
        "        # Save and export as CSV files\n",
        "        with open(ModelNames[j]+\"_TrainLoss.csv\", \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(TrainLoss)\n",
        "        with open(ModelNames[j]+\"_TrainAcc.csv\", \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(TrainAcc)\n",
        "        with open(ModelNames[j]+\"_TestLoss.csv\", \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(TestLoss)\n",
        "        with open(ModelNames[j]+\"_TestAcc.csv\", \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(TestAcc)\n",
        "        with open(ModelNames[j]+\"_FPR.csv\", \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(All_FPR)\n",
        "        with open(ModelNames[j]+\"_TPR.csv\", \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(All_TPR)\n",
        "        with open(ModelNames[j]+\"_Thresholds.csv\", \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(All_thresholds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}