{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NIH Dataset (ZIP Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-18 06:31:39--  ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
      "           => ‘cell_images.zip’\n",
      "Resolving lhcftp.nlm.nih.gov (lhcftp.nlm.nih.gov)... 130.14.55.35, 2607:f220:41e:7055::35\n",
      "Connecting to lhcftp.nlm.nih.gov (lhcftp.nlm.nih.gov)|130.14.55.35|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /Open-Access-Datasets/Malaria ... done.\n",
      "==> SIZE cell_images.zip ... 353452851\n",
      "==> PASV ... done.    ==> RETR cell_images.zip ... done.\n",
      "Length: 353452851 (337M) (unauthoritative)\n",
      "\n",
      "cell_images.zip     100%[===================>] 337.08M  14.9MB/s    in 28s     \n",
      "\n",
      "2020-02-18 06:32:09 (12.0 MB/s) - ‘cell_images.zip’ saved [353452851]\n",
      "\n",
      "Extracting images...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Download NIH dataset zip file\n",
    "!wget -nc ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
    "\n",
    "# Extract images if not already extracted\n",
    "ROOT_DIR = os.path.join(\"/\", \"content\")\n",
    "if not os.path.isdir(\"cell_images\"):\n",
    "    print(\"Extracting images...\")\n",
    "    with ZipFile(os.path.join(\"cell_images.zip\"), \"r\") as zipObj:\n",
    "        zipObj.extractall()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Images, Resize, and Store in NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.2.0.32-cp36-cp36m-manylinux1_x86_64.whl (28.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.2 MB 32 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.2.0.32\n",
      "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease      \u001b[0m    \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease                \n",
      "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
      "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Reading package lists... Done \u001b[0m33m\u001b[33m\u001b[33m\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "28 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libbsd0 libice6 libx11-6 libx11-data libxau6 libxcb1 libxdmcp6 x11-common\n",
      "The following NEW packages will be installed:\n",
      "  libbsd0 libice6 libsm6 libx11-6 libx11-data libxau6 libxcb1 libxdmcp6\n",
      "  libxext6 libxrender1 x11-common\n",
      "0 upgraded, 11 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 915 kB of archives.\n",
      "After this operation, 4091 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxau6 amd64 1:1.0.8-1 [8376 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbsd0 amd64 0.8.7-1ubuntu0.1 [41.6 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxdmcp6 amd64 1:1.1.2-3 [10.7 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb1 amd64 1.13-2~ubuntu18.04 [45.5 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-data all 2:1.6.4-3ubuntu0.2 [113 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-6 amd64 2:1.6.4-3ubuntu0.2 [569 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxext6 amd64 2:1.3.3-1 [29.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 x11-common all 1:7.7+19ubuntu7.1 [22.5 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libice6 amd64 2:1.0.9-2 [40.2 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsm6 amd64 2:1.2.2-1 [15.8 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrender1 amd64 1:0.9.10-1 [18.7 kB]\n",
      "Fetched 915 kB in 1s (1098 kB/s)       \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libxau6:amd64.\n",
      "(Reading database ... 16107 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libxau6_1%3a1.0.8-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libxau6:amd64 (1:1.0.8-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Selecting previously unselected package libbsd0:amd64.\n",
      "Preparing to unpack .../01-libbsd0_0.8.7-1ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libbsd0:amd64 (0.8.7-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libxdmcp6:amd64.\n",
      "Preparing to unpack .../02-libxdmcp6_1%3a1.1.2-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libxdmcp6:amd64 (1:1.1.2-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libxcb1:amd64.\n",
      "Preparing to unpack .../03-libxcb1_1.13-2~ubuntu18.04_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libxcb1:amd64 (1.13-2~ubuntu18.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libx11-data.\n",
      "Preparing to unpack .../04-libx11-data_2%3a1.6.4-3ubuntu0.2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libx11-data (2:1.6.4-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libx11-6:amd64.\n",
      "Preparing to unpack .../05-libx11-6_2%3a1.6.4-3ubuntu0.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libx11-6:amd64 (2:1.6.4-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libxext6:amd64.\n",
      "Preparing to unpack .../06-libxext6_2%3a1.3.3-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libxext6:amd64 (2:1.3.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package x11-common.\n",
      "Preparing to unpack .../07-x11-common_1%3a7.7+19ubuntu7.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8\u001b[1mdpkg-query:\u001b[0m no packages found matching nux-tools\n",
      "Unpacking x11-common (1:7.7+19ubuntu7.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libice6:amd64.\n",
      "Preparing to unpack .../08-libice6_2%3a1.0.9-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libice6:amd64 (2:1.0.9-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package libsm6:amd64.\n",
      "Preparing to unpack .../09-libsm6_2%3a1.2.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Unpacking libsm6:amd64 (2:1.2.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Selecting previously unselected package libxrender1:amd64.\n",
      "Preparing to unpack .../10-libxrender1_1%3a0.9.10-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [################################..........................] \u001b8Unpacking libxrender1:amd64 (1:0.9.10-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libbsd0:amd64 (0.8.7-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libxdmcp6:amd64 (1:1.1.2-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up x11-common (1:7.7+19ubuntu7.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libx11-data (2:1.6.4-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libxau6:amd64 (1:1.0.8-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libice6:amd64 (2:1.0.9-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libxcb1:amd64 (1.13-2~ubuntu18.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libsm6:amd64 (2:1.2.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libx11-6:amd64 (2:1.6.4-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libxrender1:amd64 (1:0.9.10-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libxext6:amd64 (2:1.3.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JUninfected Dataset size is: (13779, 128, 128, 3)\n",
      "Parasitized Dataset size is: (13779, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# Install and import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "!pip install opencv-python\n",
    "!apt update && apt install -y libsm6 libxext6 libxrender1\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Create new folders to save rescaled images\n",
    "if not os.path.isdir(\"RescaledSet\"):\n",
    "    os.mkdir(\"RescaledSet\")\n",
    "if not os.path.isdir(\"RescaledSet/Parasitized\"):\n",
    "    os.mkdir(\"RescaledSet/Parasitized\")\n",
    "if not os.path.isdir(\"RescaledSet/Uninfected\"):\n",
    "    os.mkdir(\"RescaledSet/Uninfected\")\n",
    "\n",
    "# Generate list of parasitized file names\n",
    "ParasitizedFiles = os.listdir(\"cell_images/Parasitized/\")\n",
    "UninfectedFiles = os.listdir(\"cell_images/Uninfected/\")\n",
    "\n",
    "# Remove Thumb.db files\n",
    "while 'Thumbs.db' in ParasitizedFiles: ParasitizedFiles.remove('Thumbs.db')   \n",
    "while 'Thumbs.db' in UninfectedFiles: UninfectedFiles.remove('Thumbs.db')  \n",
    "\n",
    "# Pre-allocate memory space for images\n",
    "Parasitized = np.empty([13779,128,128,3])\n",
    "Uninfected = np.empty([13779,128,128,3])\n",
    "\n",
    "# Resize and load parasitized images\n",
    "for i in range(13779):\n",
    "    TempImage = cv2.imread('cell_images/Parasitized/'+ParasitizedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Parasitized[i,:,:,:] = ResizedImage\n",
    "\n",
    "# Resize and load uninfected images\n",
    "for i in range(13779):\n",
    "    TempImage = cv2.imread('cell_images/Uninfected/'+UninfectedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Uninfected[i,:,:,:] = ResizedImage\n",
    "    \n",
    "print('Uninfected Dataset size is:',np.shape(Uninfected))\n",
    "print('Parasitized Dataset size is:',np.shape(Parasitized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cross-Validation Indices for Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset labels\n",
    "ParasitizedLabels = np.repeat([[0,1]], 13779, axis=0)\n",
    "UninfectedLabels = np.repeat([[1,0]], 13779, axis=0)\n",
    "Labels = np.concatenate((ParasitizedLabels,UninfectedLabels), axis=0)\n",
    "\n",
    "# Generate image dataset\n",
    "Dataset = np.concatenate((Parasitized, Uninfected), axis=0)\n",
    "\n",
    "# Generate 5-fold cross-validation groups\n",
    "CVIndices = np.random.permutation(Dataset.shape[0])\n",
    "Index1, Index2, Index3, Index4, Index5 = CVIndices[:5512], CVIndices[5512:11024], CVIndices[11024:16536], CVIndices[16536:22048], CVIndices[22048:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Save Results as CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.1)\n",
      "0 input_2 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22048 samples, validate on 5510 samples\n",
      "Epoch 1/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 2.9410 - accuracy: 0.5930 - val_loss: 0.3870 - val_accuracy: 0.8240\n",
      "Epoch 2/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.7514 - accuracy: 0.7268 - val_loss: 0.2923 - val_accuracy: 0.8868\n",
      "Epoch 3/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.4771 - accuracy: 0.8081 - val_loss: 0.2482 - val_accuracy: 0.9118\n",
      "Epoch 4/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.3651 - accuracy: 0.8583 - val_loss: 0.2141 - val_accuracy: 0.9290\n",
      "Epoch 5/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.2939 - accuracy: 0.8874 - val_loss: 0.1809 - val_accuracy: 0.9385\n",
      "Epoch 6/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.2476 - accuracy: 0.9068 - val_loss: 0.1603 - val_accuracy: 0.9456\n",
      "Epoch 7/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.2154 - accuracy: 0.9231 - val_loss: 0.1463 - val_accuracy: 0.9515\n",
      "Epoch 8/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1897 - accuracy: 0.9339 - val_loss: 0.1359 - val_accuracy: 0.9563\n",
      "Epoch 9/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1682 - accuracy: 0.9425 - val_loss: 0.1322 - val_accuracy: 0.9579\n",
      "Epoch 10/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1595 - accuracy: 0.9472 - val_loss: 0.1304 - val_accuracy: 0.9586\n",
      "Epoch 11/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1524 - accuracy: 0.9497 - val_loss: 0.1260 - val_accuracy: 0.9593\n",
      "Epoch 12/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1401 - accuracy: 0.9512 - val_loss: 0.1237 - val_accuracy: 0.9590\n",
      "Epoch 13/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1385 - accuracy: 0.9541 - val_loss: 0.1208 - val_accuracy: 0.9588\n",
      "Epoch 14/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1276 - accuracy: 0.9559 - val_loss: 0.1198 - val_accuracy: 0.9588\n",
      "Epoch 15/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1230 - accuracy: 0.9575 - val_loss: 0.1214 - val_accuracy: 0.9617\n",
      "Epoch 16/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1193 - accuracy: 0.9597 - val_loss: 0.1159 - val_accuracy: 0.9613\n",
      "Epoch 17/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1127 - accuracy: 0.9609 - val_loss: 0.1143 - val_accuracy: 0.9606\n",
      "Epoch 18/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1097 - accuracy: 0.9607 - val_loss: 0.1160 - val_accuracy: 0.9626\n",
      "Epoch 19/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.1052 - accuracy: 0.9632 - val_loss: 0.1147 - val_accuracy: 0.9630\n",
      "Epoch 20/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0999 - accuracy: 0.9645 - val_loss: 0.1117 - val_accuracy: 0.9630\n",
      "Epoch 21/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0972 - accuracy: 0.9655 - val_loss: 0.1108 - val_accuracy: 0.9610\n",
      "Epoch 22/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0931 - accuracy: 0.9671 - val_loss: 0.1085 - val_accuracy: 0.9639\n",
      "Epoch 23/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0895 - accuracy: 0.9692 - val_loss: 0.1080 - val_accuracy: 0.9624\n",
      "Epoch 24/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0872 - accuracy: 0.9693 - val_loss: 0.1071 - val_accuracy: 0.9635\n",
      "Epoch 25/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0823 - accuracy: 0.9709 - val_loss: 0.1082 - val_accuracy: 0.9644\n",
      "Epoch 26/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0804 - accuracy: 0.9711 - val_loss: 0.1055 - val_accuracy: 0.9637\n",
      "Epoch 27/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0746 - accuracy: 0.9724 - val_loss: 0.1096 - val_accuracy: 0.9648\n",
      "Epoch 28/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0716 - accuracy: 0.9734 - val_loss: 0.1078 - val_accuracy: 0.9653\n",
      "Epoch 29/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0694 - accuracy: 0.9741 - val_loss: 0.1080 - val_accuracy: 0.9661\n",
      "Epoch 30/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0657 - accuracy: 0.9758 - val_loss: 0.1094 - val_accuracy: 0.9657\n",
      "Epoch 31/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0639 - accuracy: 0.9771 - val_loss: 0.1067 - val_accuracy: 0.9646\n",
      "Epoch 32/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0587 - accuracy: 0.9789 - val_loss: 0.1097 - val_accuracy: 0.9652\n",
      "Epoch 33/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0561 - accuracy: 0.9798 - val_loss: 0.1165 - val_accuracy: 0.9652\n",
      "Epoch 34/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0532 - accuracy: 0.9813 - val_loss: 0.1125 - val_accuracy: 0.9653\n",
      "Epoch 35/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0497 - accuracy: 0.9824 - val_loss: 0.1133 - val_accuracy: 0.9662\n",
      "Epoch 36/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0463 - accuracy: 0.9836 - val_loss: 0.1132 - val_accuracy: 0.9653\n",
      "Epoch 37/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0439 - accuracy: 0.9835 - val_loss: 0.1194 - val_accuracy: 0.9670\n",
      "Epoch 38/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0427 - accuracy: 0.9845 - val_loss: 0.1146 - val_accuracy: 0.9661\n",
      "Epoch 39/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0409 - accuracy: 0.9854 - val_loss: 0.1185 - val_accuracy: 0.9659\n",
      "Epoch 40/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0371 - accuracy: 0.9873 - val_loss: 0.1197 - val_accuracy: 0.9664\n",
      "Epoch 41/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0343 - accuracy: 0.9878 - val_loss: 0.1291 - val_accuracy: 0.9650\n",
      "Epoch 42/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0325 - accuracy: 0.9890 - val_loss: 0.1249 - val_accuracy: 0.9655\n",
      "Epoch 43/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0289 - accuracy: 0.9903 - val_loss: 0.1316 - val_accuracy: 0.9652\n",
      "Epoch 44/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0278 - accuracy: 0.9912 - val_loss: 0.1336 - val_accuracy: 0.9646\n",
      "Epoch 45/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0258 - accuracy: 0.9910 - val_loss: 0.1413 - val_accuracy: 0.9641\n",
      "Epoch 46/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.1411 - val_accuracy: 0.9641\n",
      "Epoch 47/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0220 - accuracy: 0.9926 - val_loss: 0.1456 - val_accuracy: 0.9646\n",
      "Epoch 48/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0201 - accuracy: 0.9928 - val_loss: 0.1557 - val_accuracy: 0.9641\n",
      "Epoch 49/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.1468 - val_accuracy: 0.9639\n",
      "Epoch 50/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.1562 - val_accuracy: 0.9646\n",
      "Epoch 51/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.1590 - val_accuracy: 0.9642\n",
      "Epoch 52/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.1609 - val_accuracy: 0.9653\n",
      "Epoch 53/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.1665 - val_accuracy: 0.9644\n",
      "Epoch 54/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.1667 - val_accuracy: 0.9648\n",
      "Epoch 55/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 0.1804 - val_accuracy: 0.9644\n",
      "Epoch 56/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.1795 - val_accuracy: 0.9637\n",
      "Epoch 57/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.1861 - val_accuracy: 0.9635\n",
      "Epoch 58/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.1877 - val_accuracy: 0.9646\n",
      "Epoch 59/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.1959 - val_accuracy: 0.9630\n",
      "Epoch 60/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.2040 - val_accuracy: 0.9637\n",
      "Epoch 61/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.2097 - val_accuracy: 0.9633\n",
      "Epoch 62/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.2044 - val_accuracy: 0.9650\n",
      "Epoch 63/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.2143 - val_accuracy: 0.9644\n",
      "Epoch 64/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.2150 - val_accuracy: 0.9642\n",
      "Epoch 65/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2153 - val_accuracy: 0.9635\n",
      "Epoch 66/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2258 - val_accuracy: 0.9653\n",
      "Epoch 67/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.2336 - val_accuracy: 0.9642\n",
      "Epoch 68/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2409 - val_accuracy: 0.9644\n",
      "Epoch 69/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2521 - val_accuracy: 0.9641\n",
      "Epoch 70/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2613 - val_accuracy: 0.9632\n",
      "Epoch 71/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.2463 - val_accuracy: 0.9657\n",
      "Epoch 72/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2429 - val_accuracy: 0.9637\n",
      "Epoch 73/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2568 - val_accuracy: 0.9641\n",
      "Epoch 74/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2598 - val_accuracy: 0.9646\n",
      "Epoch 75/75\n",
      "22048/22048 [==============================] - 28s 1ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2764 - val_accuracy: 0.9648\n",
      "Training Loss: [2.941, 0.7514, 0.4771, 0.3651, 0.2939, 0.2476, 0.2154, 0.1897, 0.1682, 0.1595, 0.1524, 0.1401, 0.1385, 0.1276, 0.123, 0.1193, 0.1127, 0.1097, 0.1052, 0.0999, 0.0972, 0.0931, 0.0895, 0.0872, 0.0823, 0.0804, 0.0746, 0.0716, 0.0694, 0.0657, 0.0639, 0.0587, 0.0561, 0.0532, 0.0497, 0.0463, 0.0439, 0.0427, 0.0409, 0.0371, 0.0343, 0.0325, 0.0289, 0.0278, 0.0258, 0.0237, 0.022, 0.0201, 0.0176, 0.0164, 0.0152, 0.0126, 0.0122, 0.0104, 0.0094, 0.0091, 0.0086, 0.0069, 0.0055, 0.0054, 0.0048, 0.0047, 0.004, 0.0034, 0.0033, 0.0025, 0.0026, 0.0023, 0.0017, 0.0016, 0.0028, 0.0015, 0.0013, 0.001, 0.0013]\n",
      "Training Accuracy: [0.593, 0.7268, 0.8081, 0.8583, 0.8874, 0.9068, 0.9231, 0.9339, 0.9425, 0.9472, 0.9497, 0.9512, 0.9541, 0.9559, 0.9575, 0.9597, 0.9609, 0.9607, 0.9632, 0.9645, 0.9655, 0.9671, 0.9692, 0.9693, 0.9709, 0.9711, 0.9724, 0.9734, 0.9741, 0.9758, 0.9771, 0.9789, 0.9798, 0.9813, 0.9824, 0.9836, 0.9835, 0.9845, 0.9854, 0.9873, 0.9878, 0.989, 0.9903, 0.9912, 0.991, 0.9917, 0.9926, 0.9928, 0.9944, 0.995, 0.9952, 0.9962, 0.9962, 0.997, 0.9968, 0.9976, 0.9976, 0.9981, 0.9986, 0.9987, 0.999, 0.9988, 0.999, 0.9993, 0.9994, 0.9996, 0.9997, 0.9996, 0.9998, 0.9997, 0.9991, 0.9998, 0.9998, 0.9999, 0.9999]\n",
      "Validation Loss: [0.387, 0.2923, 0.2482, 0.2141, 0.1809, 0.1603, 0.1463, 0.1359, 0.1322, 0.1304, 0.126, 0.1237, 0.1208, 0.1198, 0.1214, 0.1159, 0.1143, 0.116, 0.1147, 0.1117, 0.1108, 0.1085, 0.108, 0.1071, 0.1082, 0.1055, 0.1096, 0.1078, 0.108, 0.1094, 0.1067, 0.1097, 0.1165, 0.1125, 0.1133, 0.1132, 0.1194, 0.1146, 0.1185, 0.1197, 0.1291, 0.1249, 0.1316, 0.1336, 0.1413, 0.1411, 0.1456, 0.1557, 0.1468, 0.1562, 0.159, 0.1609, 0.1665, 0.1667, 0.1804, 0.1795, 0.1861, 0.1877, 0.1959, 0.204, 0.2097, 0.2044, 0.2143, 0.215, 0.2153, 0.2258, 0.2336, 0.2409, 0.2521, 0.2613, 0.2463, 0.2429, 0.2568, 0.2598, 0.2764]\n",
      "Validation Accuracy: [0.824, 0.8868, 0.9118, 0.929, 0.9385, 0.9456, 0.9515, 0.9563, 0.9579, 0.9586, 0.9593, 0.959, 0.9588, 0.9588, 0.9617, 0.9613, 0.9606, 0.9626, 0.963, 0.963, 0.961, 0.9639, 0.9624, 0.9635, 0.9644, 0.9637, 0.9648, 0.9653, 0.9661, 0.9657, 0.9646, 0.9652, 0.9652, 0.9653, 0.9662, 0.9653, 0.967, 0.9661, 0.9659, 0.9664, 0.965, 0.9655, 0.9652, 0.9646, 0.9641, 0.9641, 0.9646, 0.9641, 0.9639, 0.9646, 0.9642, 0.9653, 0.9644, 0.9648, 0.9644, 0.9637, 0.9635, 0.9646, 0.963, 0.9637, 0.9633, 0.965, 0.9644, 0.9642, 0.9635, 0.9653, 0.9642, 0.9644, 0.9641, 0.9632, 0.9657, 0.9637, 0.9641, 0.9646, 0.9648]\n",
      "\n",
      "0 input_3 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 29s 1ms/step - loss: 2.6489 - accuracy: 0.6163 - val_loss: 0.4173 - val_accuracy: 0.8113\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.7606 - accuracy: 0.7188 - val_loss: 0.3188 - val_accuracy: 0.8623\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.4761 - accuracy: 0.7984 - val_loss: 0.2637 - val_accuracy: 0.8980\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.3593 - accuracy: 0.8477 - val_loss: 0.2173 - val_accuracy: 0.9211\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.2853 - accuracy: 0.8869 - val_loss: 0.1871 - val_accuracy: 0.9369\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.2392 - accuracy: 0.9082 - val_loss: 0.1631 - val_accuracy: 0.9465\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.2062 - accuracy: 0.9249 - val_loss: 0.1507 - val_accuracy: 0.9512\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1850 - accuracy: 0.9349 - val_loss: 0.1407 - val_accuracy: 0.9536\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1689 - accuracy: 0.9420 - val_loss: 0.1345 - val_accuracy: 0.9563\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1590 - accuracy: 0.9447 - val_loss: 0.1303 - val_accuracy: 0.9579\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1494 - accuracy: 0.9487 - val_loss: 0.1279 - val_accuracy: 0.9588\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1422 - accuracy: 0.9518 - val_loss: 0.1240 - val_accuracy: 0.9588\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1351 - accuracy: 0.9530 - val_loss: 0.1220 - val_accuracy: 0.9601\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1297 - accuracy: 0.9554 - val_loss: 0.1189 - val_accuracy: 0.9599\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1220 - accuracy: 0.9582 - val_loss: 0.1176 - val_accuracy: 0.9604\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1158 - accuracy: 0.9596 - val_loss: 0.1156 - val_accuracy: 0.9608\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1129 - accuracy: 0.9602 - val_loss: 0.1167 - val_accuracy: 0.9619\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1076 - accuracy: 0.9624 - val_loss: 0.1129 - val_accuracy: 0.9608\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1070 - accuracy: 0.9620 - val_loss: 0.1111 - val_accuracy: 0.9621\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1003 - accuracy: 0.9643 - val_loss: 0.1103 - val_accuracy: 0.9615\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0943 - accuracy: 0.9663 - val_loss: 0.1101 - val_accuracy: 0.9624\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0896 - accuracy: 0.9679 - val_loss: 0.1092 - val_accuracy: 0.9621\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0873 - accuracy: 0.9686 - val_loss: 0.1091 - val_accuracy: 0.9610\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0839 - accuracy: 0.9691 - val_loss: 0.1161 - val_accuracy: 0.9632\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0804 - accuracy: 0.9716 - val_loss: 0.1057 - val_accuracy: 0.9637\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0782 - accuracy: 0.9720 - val_loss: 0.1108 - val_accuracy: 0.9644\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0734 - accuracy: 0.9738 - val_loss: 0.1097 - val_accuracy: 0.9630\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0696 - accuracy: 0.9749 - val_loss: 0.1106 - val_accuracy: 0.9644\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0659 - accuracy: 0.9769 - val_loss: 0.1083 - val_accuracy: 0.9657\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0637 - accuracy: 0.9770 - val_loss: 0.1073 - val_accuracy: 0.9653\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0589 - accuracy: 0.9785 - val_loss: 0.1068 - val_accuracy: 0.9650\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0573 - accuracy: 0.9798 - val_loss: 0.1095 - val_accuracy: 0.9641\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0536 - accuracy: 0.9808 - val_loss: 0.1095 - val_accuracy: 0.9653\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0511 - accuracy: 0.9819 - val_loss: 0.1101 - val_accuracy: 0.9646\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0472 - accuracy: 0.9833 - val_loss: 0.1117 - val_accuracy: 0.9659\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0452 - accuracy: 0.9836 - val_loss: 0.1159 - val_accuracy: 0.9659\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0410 - accuracy: 0.9858 - val_loss: 0.1214 - val_accuracy: 0.9652\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0391 - accuracy: 0.9866 - val_loss: 0.1202 - val_accuracy: 0.9655\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0369 - accuracy: 0.9869 - val_loss: 0.1207 - val_accuracy: 0.9663\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0336 - accuracy: 0.9887 - val_loss: 0.1268 - val_accuracy: 0.9652\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0309 - accuracy: 0.9898 - val_loss: 0.1255 - val_accuracy: 0.9659\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0296 - accuracy: 0.9898 - val_loss: 0.1323 - val_accuracy: 0.9664\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.1323 - val_accuracy: 0.9659\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.1338 - val_accuracy: 0.9652\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.1380 - val_accuracy: 0.9652\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0197 - accuracy: 0.9938 - val_loss: 0.1411 - val_accuracy: 0.9653\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0183 - accuracy: 0.9942 - val_loss: 0.1403 - val_accuracy: 0.9655\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0163 - accuracy: 0.9953 - val_loss: 0.1467 - val_accuracy: 0.9657\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0149 - accuracy: 0.9960 - val_loss: 0.1465 - val_accuracy: 0.9659\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0136 - accuracy: 0.9959 - val_loss: 0.1632 - val_accuracy: 0.9655\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.1619 - val_accuracy: 0.9652\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.1637 - val_accuracy: 0.9657\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 0.1680 - val_accuracy: 0.9655\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.1696 - val_accuracy: 0.9673\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 0.1827 - val_accuracy: 0.9663\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.1860 - val_accuracy: 0.9653\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.1897 - val_accuracy: 0.9663\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.1941 - val_accuracy: 0.9659\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.1926 - val_accuracy: 0.9655\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.1980 - val_accuracy: 0.9653\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.2063 - val_accuracy: 0.9668\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2069 - val_accuracy: 0.9663\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.2210 - val_accuracy: 0.9661\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2190 - val_accuracy: 0.9664\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.2266 - val_accuracy: 0.9655\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.2241 - val_accuracy: 0.9655\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.2371 - val_accuracy: 0.9655\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2563 - val_accuracy: 0.9666\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2464 - val_accuracy: 0.9657\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2442 - val_accuracy: 0.9652\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.2578 - val_accuracy: 0.9652\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2578 - val_accuracy: 0.9646\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 9.8444e-04 - accuracy: 1.0000 - val_loss: 0.2686 - val_accuracy: 0.9652\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.2721 - val_accuracy: 0.9655\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2679 - val_accuracy: 0.9659\n",
      "Training Loss: [2.6489, 0.7606, 0.4761, 0.3593, 0.2853, 0.2392, 0.2062, 0.185, 0.1689, 0.159, 0.1494, 0.1422, 0.1351, 0.1297, 0.122, 0.1158, 0.1129, 0.1076, 0.107, 0.1003, 0.0943, 0.0896, 0.0873, 0.0839, 0.0804, 0.0782, 0.0734, 0.0696, 0.0659, 0.0637, 0.0589, 0.0573, 0.0536, 0.0511, 0.0472, 0.0452, 0.041, 0.0391, 0.0369, 0.0336, 0.0309, 0.0296, 0.0264, 0.0243, 0.0221, 0.0197, 0.0183, 0.0163, 0.0149, 0.0136, 0.0126, 0.0111, 0.0093, 0.0087, 0.0083, 0.0065, 0.0061, 0.005, 0.004, 0.0037, 0.0042, 0.0039, 0.0027, 0.0026, 0.002, 0.0032, 0.0018, 0.0019, 0.0013, 0.0015, 0.0018, 0.0011, 0.001, 0.0015, 0.0013]\n",
      "Training Accuracy: [0.6163, 0.7188, 0.7984, 0.8477, 0.8869, 0.9082, 0.9249, 0.9349, 0.942, 0.9447, 0.9487, 0.9518, 0.953, 0.9554, 0.9582, 0.9596, 0.9602, 0.9624, 0.962, 0.9643, 0.9663, 0.9679, 0.9686, 0.9691, 0.9716, 0.972, 0.9738, 0.9749, 0.9769, 0.977, 0.9785, 0.9798, 0.9808, 0.9819, 0.9833, 0.9836, 0.9858, 0.9866, 0.9869, 0.9887, 0.9898, 0.9898, 0.9912, 0.992, 0.993, 0.9938, 0.9942, 0.9953, 0.996, 0.9959, 0.9965, 0.9973, 0.9976, 0.9976, 0.9976, 0.9985, 0.9985, 0.9988, 0.9994, 0.9993, 0.9992, 0.9993, 0.9997, 0.9996, 0.9997, 0.9993, 0.9998, 0.9996, 0.9999, 0.9998, 0.9998, 0.9999, 1.0, 0.9995, 0.9998]\n",
      "Validation Loss: [0.4173, 0.3188, 0.2637, 0.2173, 0.1871, 0.1631, 0.1507, 0.1407, 0.1345, 0.1303, 0.1279, 0.124, 0.122, 0.1189, 0.1176, 0.1156, 0.1167, 0.1129, 0.1111, 0.1103, 0.1101, 0.1092, 0.1091, 0.1161, 0.1057, 0.1108, 0.1097, 0.1106, 0.1083, 0.1073, 0.1068, 0.1095, 0.1095, 0.1101, 0.1117, 0.1159, 0.1214, 0.1202, 0.1207, 0.1268, 0.1255, 0.1323, 0.1323, 0.1338, 0.138, 0.1411, 0.1403, 0.1467, 0.1465, 0.1632, 0.1619, 0.1637, 0.168, 0.1696, 0.1827, 0.186, 0.1897, 0.1941, 0.1926, 0.198, 0.2063, 0.2069, 0.221, 0.219, 0.2266, 0.2241, 0.2371, 0.2563, 0.2464, 0.2442, 0.2578, 0.2578, 0.2686, 0.2721, 0.2679]\n",
      "Validation Accuracy: [0.8113, 0.8623, 0.898, 0.9211, 0.9369, 0.9465, 0.9512, 0.9536, 0.9563, 0.9579, 0.9588, 0.9588, 0.9601, 0.9599, 0.9604, 0.9608, 0.9619, 0.9608, 0.9621, 0.9615, 0.9624, 0.9621, 0.961, 0.9632, 0.9637, 0.9644, 0.963, 0.9644, 0.9657, 0.9653, 0.965, 0.9641, 0.9653, 0.9646, 0.9659, 0.9659, 0.9652, 0.9655, 0.9663, 0.9652, 0.9659, 0.9664, 0.9659, 0.9652, 0.9652, 0.9653, 0.9655, 0.9657, 0.9659, 0.9655, 0.9652, 0.9657, 0.9655, 0.9673, 0.9663, 0.9653, 0.9663, 0.9659, 0.9655, 0.9653, 0.9668, 0.9663, 0.9661, 0.9664, 0.9655, 0.9655, 0.9655, 0.9666, 0.9657, 0.9652, 0.9652, 0.9646, 0.9652, 0.9655, 0.9659]\n",
      "\n",
      "0 input_4 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 2.4830 - accuracy: 0.6522 - val_loss: 0.2901 - val_accuracy: 0.8844\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.6485 - accuracy: 0.7797 - val_loss: 0.2092 - val_accuracy: 0.9300\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.3874 - accuracy: 0.8627 - val_loss: 0.1794 - val_accuracy: 0.9418\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.2903 - accuracy: 0.9013 - val_loss: 0.1694 - val_accuracy: 0.9478\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.2453 - accuracy: 0.9213 - val_loss: 0.1630 - val_accuracy: 0.9508\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.2144 - accuracy: 0.9317 - val_loss: 0.1541 - val_accuracy: 0.9519\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1953 - accuracy: 0.9386 - val_loss: 0.1509 - val_accuracy: 0.9532\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1807 - accuracy: 0.9426 - val_loss: 0.1488 - val_accuracy: 0.9525\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1677 - accuracy: 0.9470 - val_loss: 0.1459 - val_accuracy: 0.9543\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1617 - accuracy: 0.9491 - val_loss: 0.1401 - val_accuracy: 0.9543\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1468 - accuracy: 0.9514 - val_loss: 0.1416 - val_accuracy: 0.9554\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1435 - accuracy: 0.9530 - val_loss: 0.1379 - val_accuracy: 0.9552\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1350 - accuracy: 0.9560 - val_loss: 0.1337 - val_accuracy: 0.9546\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1289 - accuracy: 0.9574 - val_loss: 0.1324 - val_accuracy: 0.9563\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1224 - accuracy: 0.9599 - val_loss: 0.1322 - val_accuracy: 0.9568\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1200 - accuracy: 0.9591 - val_loss: 0.1307 - val_accuracy: 0.9565\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1126 - accuracy: 0.9622 - val_loss: 0.1323 - val_accuracy: 0.9574\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1106 - accuracy: 0.9624 - val_loss: 0.1335 - val_accuracy: 0.9585\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1071 - accuracy: 0.9649 - val_loss: 0.1284 - val_accuracy: 0.9577\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1027 - accuracy: 0.9645 - val_loss: 0.1270 - val_accuracy: 0.9550\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0975 - accuracy: 0.9666 - val_loss: 0.1303 - val_accuracy: 0.9586\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0939 - accuracy: 0.9686 - val_loss: 0.1256 - val_accuracy: 0.9577\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0918 - accuracy: 0.9698 - val_loss: 0.1225 - val_accuracy: 0.9599\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0880 - accuracy: 0.9700 - val_loss: 0.1243 - val_accuracy: 0.9592\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0815 - accuracy: 0.9716 - val_loss: 0.1236 - val_accuracy: 0.9595\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0813 - accuracy: 0.9718 - val_loss: 0.1230 - val_accuracy: 0.9592\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0783 - accuracy: 0.9732 - val_loss: 0.1247 - val_accuracy: 0.9601\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0764 - accuracy: 0.9729 - val_loss: 0.1277 - val_accuracy: 0.9574\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0724 - accuracy: 0.9745 - val_loss: 0.1257 - val_accuracy: 0.9603\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0678 - accuracy: 0.9765 - val_loss: 0.1251 - val_accuracy: 0.9597\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0654 - accuracy: 0.9764 - val_loss: 0.1291 - val_accuracy: 0.9603\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0621 - accuracy: 0.9785 - val_loss: 0.1288 - val_accuracy: 0.9603\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0602 - accuracy: 0.9791 - val_loss: 0.1310 - val_accuracy: 0.9597\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0565 - accuracy: 0.9802 - val_loss: 0.1376 - val_accuracy: 0.9603\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0551 - accuracy: 0.9818 - val_loss: 0.1324 - val_accuracy: 0.9601\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0525 - accuracy: 0.9819 - val_loss: 0.1331 - val_accuracy: 0.9601\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0500 - accuracy: 0.9819 - val_loss: 0.1341 - val_accuracy: 0.9595\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0470 - accuracy: 0.9839 - val_loss: 0.1379 - val_accuracy: 0.9615\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0428 - accuracy: 0.9855 - val_loss: 0.1455 - val_accuracy: 0.9617\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0418 - accuracy: 0.9851 - val_loss: 0.1410 - val_accuracy: 0.9610\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0394 - accuracy: 0.9864 - val_loss: 0.1437 - val_accuracy: 0.9621\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0375 - accuracy: 0.9869 - val_loss: 0.1424 - val_accuracy: 0.9603\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0347 - accuracy: 0.9875 - val_loss: 0.1490 - val_accuracy: 0.9610\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0342 - accuracy: 0.9879 - val_loss: 0.1517 - val_accuracy: 0.9608\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0320 - accuracy: 0.9895 - val_loss: 0.1538 - val_accuracy: 0.9604\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0295 - accuracy: 0.9902 - val_loss: 0.1595 - val_accuracy: 0.9610\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0272 - accuracy: 0.9913 - val_loss: 0.1714 - val_accuracy: 0.9608\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0263 - accuracy: 0.9914 - val_loss: 0.1673 - val_accuracy: 0.9615\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0239 - accuracy: 0.9921 - val_loss: 0.1840 - val_accuracy: 0.9614\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0229 - accuracy: 0.9925 - val_loss: 0.1700 - val_accuracy: 0.9610\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0207 - accuracy: 0.9930 - val_loss: 0.1840 - val_accuracy: 0.9610\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.1804 - val_accuracy: 0.9604\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0174 - accuracy: 0.9947 - val_loss: 0.1877 - val_accuracy: 0.9612\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0164 - accuracy: 0.9942 - val_loss: 0.1953 - val_accuracy: 0.9599\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 0.1888 - val_accuracy: 0.9595\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.1969 - val_accuracy: 0.9606\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0125 - accuracy: 0.9965 - val_loss: 0.2058 - val_accuracy: 0.9606\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.2122 - val_accuracy: 0.9599\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0103 - accuracy: 0.9972 - val_loss: 0.2271 - val_accuracy: 0.9604\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.2249 - val_accuracy: 0.9597\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.2305 - val_accuracy: 0.9617\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.2275 - val_accuracy: 0.9612\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.2349 - val_accuracy: 0.9599\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.2517 - val_accuracy: 0.9621\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.2633 - val_accuracy: 0.9614\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.2747 - val_accuracy: 0.9624\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.2722 - val_accuracy: 0.9619\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.2847 - val_accuracy: 0.9617\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.2914 - val_accuracy: 0.9588\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.3150 - val_accuracy: 0.9615\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.3125 - val_accuracy: 0.9603\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.2912 - val_accuracy: 0.9604\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.3019 - val_accuracy: 0.9606\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.2879 - val_accuracy: 0.9606\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.3150 - val_accuracy: 0.9599\n",
      "Training Loss: [2.483, 0.6485, 0.3874, 0.2903, 0.2453, 0.2144, 0.1953, 0.1807, 0.1677, 0.1617, 0.1468, 0.1435, 0.135, 0.1289, 0.1224, 0.12, 0.1126, 0.1106, 0.1071, 0.1027, 0.0975, 0.0939, 0.0918, 0.088, 0.0815, 0.0813, 0.0783, 0.0764, 0.0724, 0.0678, 0.0654, 0.0621, 0.0602, 0.0565, 0.0551, 0.0525, 0.05, 0.047, 0.0428, 0.0418, 0.0394, 0.0375, 0.0347, 0.0342, 0.032, 0.0295, 0.0272, 0.0263, 0.0239, 0.0229, 0.0207, 0.0204, 0.0174, 0.0164, 0.0148, 0.0145, 0.0125, 0.0111, 0.0103, 0.0094, 0.0075, 0.007, 0.0061, 0.0065, 0.0048, 0.0044, 0.0044, 0.0036, 0.004, 0.0033, 0.0027, 0.0031, 0.0025, 0.0019, 0.0021]\n",
      "Training Accuracy: [0.6522, 0.7797, 0.8627, 0.9013, 0.9213, 0.9317, 0.9386, 0.9426, 0.947, 0.9491, 0.9514, 0.953, 0.956, 0.9574, 0.9599, 0.9591, 0.9622, 0.9624, 0.9649, 0.9645, 0.9666, 0.9686, 0.9698, 0.97, 0.9716, 0.9718, 0.9732, 0.9729, 0.9745, 0.9765, 0.9764, 0.9785, 0.9791, 0.9802, 0.9818, 0.9819, 0.9819, 0.9839, 0.9855, 0.9851, 0.9864, 0.9869, 0.9875, 0.9879, 0.9895, 0.9902, 0.9913, 0.9914, 0.9921, 0.9925, 0.993, 0.9932, 0.9947, 0.9942, 0.9953, 0.9955, 0.9965, 0.9966, 0.9972, 0.9975, 0.998, 0.9979, 0.9985, 0.9981, 0.9989, 0.9988, 0.9989, 0.9994, 0.999, 0.9991, 0.9995, 0.9993, 0.9993, 0.9998, 0.9995]\n",
      "Validation Loss: [0.2901, 0.2092, 0.1794, 0.1694, 0.163, 0.1541, 0.1509, 0.1488, 0.1459, 0.1401, 0.1416, 0.1379, 0.1337, 0.1324, 0.1322, 0.1307, 0.1323, 0.1335, 0.1284, 0.127, 0.1303, 0.1256, 0.1225, 0.1243, 0.1236, 0.123, 0.1247, 0.1277, 0.1257, 0.1251, 0.1291, 0.1288, 0.131, 0.1376, 0.1324, 0.1331, 0.1341, 0.1379, 0.1455, 0.141, 0.1437, 0.1424, 0.149, 0.1517, 0.1538, 0.1595, 0.1714, 0.1673, 0.184, 0.17, 0.184, 0.1804, 0.1877, 0.1953, 0.1888, 0.1969, 0.2058, 0.2122, 0.2271, 0.2249, 0.2305, 0.2275, 0.2349, 0.2517, 0.2633, 0.2747, 0.2722, 0.2847, 0.2914, 0.315, 0.3125, 0.2912, 0.3019, 0.2879, 0.315]\n",
      "Validation Accuracy: [0.8844, 0.93, 0.9418, 0.9478, 0.9508, 0.9519, 0.9532, 0.9525, 0.9543, 0.9543, 0.9554, 0.9552, 0.9546, 0.9563, 0.9568, 0.9565, 0.9574, 0.9585, 0.9577, 0.955, 0.9586, 0.9577, 0.9599, 0.9592, 0.9595, 0.9592, 0.9601, 0.9574, 0.9603, 0.9597, 0.9603, 0.9603, 0.9597, 0.9603, 0.9601, 0.9601, 0.9595, 0.9615, 0.9617, 0.961, 0.9621, 0.9603, 0.961, 0.9608, 0.9604, 0.961, 0.9608, 0.9615, 0.9614, 0.961, 0.961, 0.9604, 0.9612, 0.9599, 0.9595, 0.9606, 0.9606, 0.9599, 0.9604, 0.9597, 0.9617, 0.9612, 0.9599, 0.9621, 0.9614, 0.9624, 0.9619, 0.9617, 0.9588, 0.9615, 0.9603, 0.9604, 0.9606, 0.9606, 0.9599]\n",
      "\n",
      "0 input_5 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 2.6343 - accuracy: 0.6112 - val_loss: 0.4104 - val_accuracy: 0.8128\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.7528 - accuracy: 0.7102 - val_loss: 0.3049 - val_accuracy: 0.8741\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.4989 - accuracy: 0.7891 - val_loss: 0.2285 - val_accuracy: 0.9080\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.3739 - accuracy: 0.8460 - val_loss: 0.1854 - val_accuracy: 0.9269\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.2934 - accuracy: 0.8845 - val_loss: 0.1561 - val_accuracy: 0.9448\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.2467 - accuracy: 0.9043 - val_loss: 0.1396 - val_accuracy: 0.9523\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.2085 - accuracy: 0.9258 - val_loss: 0.1295 - val_accuracy: 0.9561\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1821 - accuracy: 0.9362 - val_loss: 0.1258 - val_accuracy: 0.9579\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1657 - accuracy: 0.9439 - val_loss: 0.1209 - val_accuracy: 0.9604\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1608 - accuracy: 0.9465 - val_loss: 0.1189 - val_accuracy: 0.9594\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1488 - accuracy: 0.9496 - val_loss: 0.1171 - val_accuracy: 0.9608\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1461 - accuracy: 0.9520 - val_loss: 0.1172 - val_accuracy: 0.9603\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1373 - accuracy: 0.9548 - val_loss: 0.1144 - val_accuracy: 0.9604\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1275 - accuracy: 0.9564 - val_loss: 0.1140 - val_accuracy: 0.9621\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1212 - accuracy: 0.9573 - val_loss: 0.1115 - val_accuracy: 0.9623\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1193 - accuracy: 0.9581 - val_loss: 0.1104 - val_accuracy: 0.9617\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1122 - accuracy: 0.9607 - val_loss: 0.1114 - val_accuracy: 0.9626\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1098 - accuracy: 0.9613 - val_loss: 0.1104 - val_accuracy: 0.9630\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1065 - accuracy: 0.9624 - val_loss: 0.1092 - val_accuracy: 0.9630\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.1022 - accuracy: 0.9634 - val_loss: 0.1104 - val_accuracy: 0.9634\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0970 - accuracy: 0.9654 - val_loss: 0.1095 - val_accuracy: 0.9646\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0921 - accuracy: 0.9671 - val_loss: 0.1092 - val_accuracy: 0.9648\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0874 - accuracy: 0.9679 - val_loss: 0.1074 - val_accuracy: 0.9643\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0855 - accuracy: 0.9685 - val_loss: 0.1099 - val_accuracy: 0.9650\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0795 - accuracy: 0.9712 - val_loss: 0.1197 - val_accuracy: 0.9641\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0782 - accuracy: 0.9709 - val_loss: 0.1113 - val_accuracy: 0.9646\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0753 - accuracy: 0.9719 - val_loss: 0.1075 - val_accuracy: 0.9646\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0709 - accuracy: 0.9731 - val_loss: 0.1164 - val_accuracy: 0.9634\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0643 - accuracy: 0.9752 - val_loss: 0.1140 - val_accuracy: 0.9641\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0626 - accuracy: 0.9760 - val_loss: 0.1155 - val_accuracy: 0.9637\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0591 - accuracy: 0.9765 - val_loss: 0.1112 - val_accuracy: 0.9637\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0595 - accuracy: 0.9765 - val_loss: 0.1174 - val_accuracy: 0.9637\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0537 - accuracy: 0.9799 - val_loss: 0.1209 - val_accuracy: 0.9646\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0521 - accuracy: 0.9794 - val_loss: 0.1193 - val_accuracy: 0.9648\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0487 - accuracy: 0.9809 - val_loss: 0.1227 - val_accuracy: 0.9644\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0465 - accuracy: 0.9816 - val_loss: 0.1231 - val_accuracy: 0.9650\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0423 - accuracy: 0.9832 - val_loss: 0.1297 - val_accuracy: 0.9650\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0398 - accuracy: 0.9840 - val_loss: 0.1265 - val_accuracy: 0.9643\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0377 - accuracy: 0.9848 - val_loss: 0.1409 - val_accuracy: 0.9637\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0348 - accuracy: 0.9866 - val_loss: 0.1343 - val_accuracy: 0.9643\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0328 - accuracy: 0.9870 - val_loss: 0.1397 - val_accuracy: 0.9650\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0315 - accuracy: 0.9882 - val_loss: 0.1361 - val_accuracy: 0.9643\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0292 - accuracy: 0.9886 - val_loss: 0.1428 - val_accuracy: 0.9646\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0274 - accuracy: 0.9902 - val_loss: 0.1565 - val_accuracy: 0.9643\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0248 - accuracy: 0.9897 - val_loss: 0.1522 - val_accuracy: 0.9648\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0229 - accuracy: 0.9912 - val_loss: 0.1571 - val_accuracy: 0.9652\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0200 - accuracy: 0.9927 - val_loss: 0.1653 - val_accuracy: 0.9641\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1672 - val_accuracy: 0.9650\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0176 - accuracy: 0.9936 - val_loss: 0.1601 - val_accuracy: 0.9643\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0165 - accuracy: 0.9943 - val_loss: 0.1733 - val_accuracy: 0.9634\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0143 - accuracy: 0.9948 - val_loss: 0.1836 - val_accuracy: 0.9635\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 0.1999 - val_accuracy: 0.9632\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0111 - accuracy: 0.9961 - val_loss: 0.1933 - val_accuracy: 0.9637\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0110 - accuracy: 0.9965 - val_loss: 0.2006 - val_accuracy: 0.9644\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.2088 - val_accuracy: 0.9641\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.2061 - val_accuracy: 0.9632\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.2193 - val_accuracy: 0.9635\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.2359 - val_accuracy: 0.9641\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.2212 - val_accuracy: 0.9637\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.2315 - val_accuracy: 0.9637\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.2382 - val_accuracy: 0.9639\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 0.2609 - val_accuracy: 0.9643\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.2595 - val_accuracy: 0.9641\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.2570 - val_accuracy: 0.9657\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.2617 - val_accuracy: 0.9641\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.2647 - val_accuracy: 0.9641\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2894 - val_accuracy: 0.9643\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.2791 - val_accuracy: 0.9653\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.2791 - val_accuracy: 0.9652\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2997 - val_accuracy: 0.9652\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.3095 - val_accuracy: 0.9650\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.3045 - val_accuracy: 0.9655\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.3570 - val_accuracy: 0.9655\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.2977 - val_accuracy: 0.9641\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.3158 - val_accuracy: 0.9650\n",
      "Training Loss: [2.6343, 0.7528, 0.4989, 0.3739, 0.2934, 0.2467, 0.2085, 0.1821, 0.1657, 0.1608, 0.1488, 0.1461, 0.1373, 0.1275, 0.1212, 0.1193, 0.1122, 0.1098, 0.1065, 0.1022, 0.097, 0.0921, 0.0874, 0.0855, 0.0795, 0.0782, 0.0753, 0.0709, 0.0643, 0.0626, 0.0591, 0.0595, 0.0537, 0.0521, 0.0487, 0.0465, 0.0423, 0.0398, 0.0377, 0.0348, 0.0328, 0.0315, 0.0292, 0.0274, 0.0248, 0.0229, 0.02, 0.0198, 0.0176, 0.0165, 0.0143, 0.0128, 0.0111, 0.011, 0.0091, 0.0093, 0.0077, 0.0073, 0.0064, 0.0055, 0.005, 0.0051, 0.0046, 0.0035, 0.0036, 0.0039, 0.0028, 0.0022, 0.002, 0.0019, 0.0017, 0.0024, 0.0021, 0.0025, 0.0018]\n",
      "Training Accuracy: [0.6112, 0.7102, 0.7891, 0.846, 0.8845, 0.9043, 0.9258, 0.9362, 0.9439, 0.9465, 0.9496, 0.952, 0.9548, 0.9564, 0.9573, 0.9581, 0.9607, 0.9613, 0.9624, 0.9634, 0.9654, 0.9671, 0.9679, 0.9685, 0.9712, 0.9709, 0.9719, 0.9731, 0.9752, 0.976, 0.9765, 0.9765, 0.9799, 0.9794, 0.9809, 0.9816, 0.9832, 0.984, 0.9848, 0.9866, 0.987, 0.9882, 0.9886, 0.9902, 0.9897, 0.9912, 0.9927, 0.9923, 0.9936, 0.9943, 0.9948, 0.9956, 0.9961, 0.9965, 0.9973, 0.997, 0.998, 0.998, 0.9985, 0.9985, 0.9988, 0.9989, 0.9989, 0.9992, 0.9991, 0.999, 0.9994, 0.9998, 0.9998, 0.9997, 0.9998, 0.9995, 0.9996, 0.9995, 0.9997]\n",
      "Validation Loss: [0.4104, 0.3049, 0.2285, 0.1854, 0.1561, 0.1396, 0.1295, 0.1258, 0.1209, 0.1189, 0.1171, 0.1172, 0.1144, 0.114, 0.1115, 0.1104, 0.1114, 0.1104, 0.1092, 0.1104, 0.1095, 0.1092, 0.1074, 0.1099, 0.1197, 0.1113, 0.1075, 0.1164, 0.114, 0.1155, 0.1112, 0.1174, 0.1209, 0.1193, 0.1227, 0.1231, 0.1297, 0.1265, 0.1409, 0.1343, 0.1397, 0.1361, 0.1428, 0.1565, 0.1522, 0.1571, 0.1653, 0.1672, 0.1601, 0.1733, 0.1836, 0.1999, 0.1933, 0.2006, 0.2088, 0.2061, 0.2193, 0.2359, 0.2212, 0.2315, 0.2382, 0.2609, 0.2595, 0.257, 0.2617, 0.2647, 0.2894, 0.2791, 0.2791, 0.2997, 0.3095, 0.3045, 0.357, 0.2977, 0.3158]\n",
      "Validation Accuracy: [0.8128, 0.8741, 0.908, 0.9269, 0.9448, 0.9523, 0.9561, 0.9579, 0.9604, 0.9594, 0.9608, 0.9603, 0.9604, 0.9621, 0.9623, 0.9617, 0.9626, 0.963, 0.963, 0.9634, 0.9646, 0.9648, 0.9643, 0.965, 0.9641, 0.9646, 0.9646, 0.9634, 0.9641, 0.9637, 0.9637, 0.9637, 0.9646, 0.9648, 0.9644, 0.965, 0.965, 0.9643, 0.9637, 0.9643, 0.965, 0.9643, 0.9646, 0.9643, 0.9648, 0.9652, 0.9641, 0.965, 0.9643, 0.9634, 0.9635, 0.9632, 0.9637, 0.9644, 0.9641, 0.9632, 0.9635, 0.9641, 0.9637, 0.9637, 0.9639, 0.9643, 0.9641, 0.9657, 0.9641, 0.9641, 0.9643, 0.9653, 0.9652, 0.9652, 0.965, 0.9655, 0.9655, 0.9641, 0.965]\n",
      "\n",
      "0 input_6 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 28s 1ms/step - loss: 2.9644 - accuracy: 0.6161 - val_loss: 0.3487 - val_accuracy: 0.8500\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.7685 - accuracy: 0.7433 - val_loss: 0.2440 - val_accuracy: 0.9084\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.4613 - accuracy: 0.8284 - val_loss: 0.1958 - val_accuracy: 0.9325\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.3313 - accuracy: 0.8770 - val_loss: 0.1757 - val_accuracy: 0.9434\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.2652 - accuracy: 0.9076 - val_loss: 0.1627 - val_accuracy: 0.9490\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.2353 - accuracy: 0.9178 - val_loss: 0.1581 - val_accuracy: 0.9519\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.2062 - accuracy: 0.9306 - val_loss: 0.1487 - val_accuracy: 0.9532\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1899 - accuracy: 0.9366 - val_loss: 0.1434 - val_accuracy: 0.9532\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1750 - accuracy: 0.9415 - val_loss: 0.1379 - val_accuracy: 0.9534\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1606 - accuracy: 0.9469 - val_loss: 0.1361 - val_accuracy: 0.9546\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1517 - accuracy: 0.9489 - val_loss: 0.1368 - val_accuracy: 0.9566\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1426 - accuracy: 0.9512 - val_loss: 0.1315 - val_accuracy: 0.9570\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1367 - accuracy: 0.9532 - val_loss: 0.1314 - val_accuracy: 0.9590\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1327 - accuracy: 0.9556 - val_loss: 0.1273 - val_accuracy: 0.9581\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1243 - accuracy: 0.9568 - val_loss: 0.1267 - val_accuracy: 0.9608\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1191 - accuracy: 0.9581 - val_loss: 0.1242 - val_accuracy: 0.9608\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1124 - accuracy: 0.9615 - val_loss: 0.1213 - val_accuracy: 0.9601\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1096 - accuracy: 0.9612 - val_loss: 0.1227 - val_accuracy: 0.9612\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.1040 - accuracy: 0.9638 - val_loss: 0.1234 - val_accuracy: 0.9626\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0993 - accuracy: 0.9642 - val_loss: 0.1170 - val_accuracy: 0.9619\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0957 - accuracy: 0.9655 - val_loss: 0.1193 - val_accuracy: 0.9641\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0919 - accuracy: 0.9668 - val_loss: 0.1214 - val_accuracy: 0.9637\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0875 - accuracy: 0.9687 - val_loss: 0.1158 - val_accuracy: 0.9623\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0827 - accuracy: 0.9687 - val_loss: 0.1158 - val_accuracy: 0.9643\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0808 - accuracy: 0.9709 - val_loss: 0.1140 - val_accuracy: 0.9637\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0751 - accuracy: 0.9721 - val_loss: 0.1213 - val_accuracy: 0.9632\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0736 - accuracy: 0.9728 - val_loss: 0.1162 - val_accuracy: 0.9639\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0692 - accuracy: 0.9747 - val_loss: 0.1109 - val_accuracy: 0.9637\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0652 - accuracy: 0.9758 - val_loss: 0.1148 - val_accuracy: 0.9641\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0632 - accuracy: 0.9770 - val_loss: 0.1145 - val_accuracy: 0.9646\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0597 - accuracy: 0.9776 - val_loss: 0.1182 - val_accuracy: 0.9650\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0573 - accuracy: 0.9783 - val_loss: 0.1185 - val_accuracy: 0.9657\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0528 - accuracy: 0.9800 - val_loss: 0.1202 - val_accuracy: 0.9657\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0519 - accuracy: 0.9802 - val_loss: 0.1191 - val_accuracy: 0.9648\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0479 - accuracy: 0.9823 - val_loss: 0.1216 - val_accuracy: 0.9655\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0460 - accuracy: 0.9825 - val_loss: 0.1236 - val_accuracy: 0.9670\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0424 - accuracy: 0.9849 - val_loss: 0.1204 - val_accuracy: 0.9663\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0396 - accuracy: 0.9853 - val_loss: 0.1197 - val_accuracy: 0.9652\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0387 - accuracy: 0.9861 - val_loss: 0.1304 - val_accuracy: 0.9657\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0359 - accuracy: 0.9866 - val_loss: 0.1336 - val_accuracy: 0.9668\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0340 - accuracy: 0.9863 - val_loss: 0.1377 - val_accuracy: 0.9650\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0309 - accuracy: 0.9885 - val_loss: 0.1337 - val_accuracy: 0.9644\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0294 - accuracy: 0.9890 - val_loss: 0.1380 - val_accuracy: 0.9657\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0262 - accuracy: 0.9901 - val_loss: 0.1435 - val_accuracy: 0.9663\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0244 - accuracy: 0.9915 - val_loss: 0.1420 - val_accuracy: 0.9670\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0236 - accuracy: 0.9916 - val_loss: 0.1362 - val_accuracy: 0.9663\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0216 - accuracy: 0.9919 - val_loss: 0.1449 - val_accuracy: 0.9650\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0195 - accuracy: 0.9933 - val_loss: 0.1608 - val_accuracy: 0.9670\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0170 - accuracy: 0.9942 - val_loss: 0.1627 - val_accuracy: 0.9675\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.1562 - val_accuracy: 0.9663\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 0.1700 - val_accuracy: 0.9648\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.1734 - val_accuracy: 0.9664\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.1620 - val_accuracy: 0.9666\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0120 - accuracy: 0.9959 - val_loss: 0.1849 - val_accuracy: 0.9672\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.1878 - val_accuracy: 0.9673\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.1852 - val_accuracy: 0.9672\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.1864 - val_accuracy: 0.9663\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.1920 - val_accuracy: 0.9677\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.2067 - val_accuracy: 0.9628\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.2208 - val_accuracy: 0.9681\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.2224 - val_accuracy: 0.9672\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.2417 - val_accuracy: 0.9690\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.2290 - val_accuracy: 0.9675\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.2201 - val_accuracy: 0.9675\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.2484 - val_accuracy: 0.9677\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.2383 - val_accuracy: 0.9661\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.2250 - val_accuracy: 0.9664\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.2712 - val_accuracy: 0.9652\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2575 - val_accuracy: 0.9661\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.2748 - val_accuracy: 0.9673\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2671 - val_accuracy: 0.9672\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.2614 - val_accuracy: 0.9672\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2752 - val_accuracy: 0.9681\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2786 - val_accuracy: 0.9666\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 27s 1ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.2970 - val_accuracy: 0.9681\n",
      "Training Loss: [2.9644, 0.7685, 0.4613, 0.3313, 0.2652, 0.2353, 0.2062, 0.1899, 0.175, 0.1606, 0.1517, 0.1426, 0.1367, 0.1327, 0.1243, 0.1191, 0.1124, 0.1096, 0.104, 0.0993, 0.0957, 0.0919, 0.0875, 0.0827, 0.0808, 0.0751, 0.0736, 0.0692, 0.0652, 0.0632, 0.0597, 0.0573, 0.0528, 0.0519, 0.0479, 0.046, 0.0424, 0.0396, 0.0387, 0.0359, 0.034, 0.0309, 0.0294, 0.0262, 0.0244, 0.0236, 0.0216, 0.0195, 0.017, 0.0169, 0.0148, 0.0134, 0.0125, 0.012, 0.0096, 0.0096, 0.0082, 0.0072, 0.0074, 0.007, 0.0052, 0.0049, 0.0041, 0.0049, 0.0037, 0.003, 0.0038, 0.0033, 0.0025, 0.0022, 0.0017, 0.0016, 0.0016, 0.0016, 0.0014]\n",
      "Training Accuracy: [0.6161, 0.7433, 0.8284, 0.877, 0.9076, 0.9178, 0.9306, 0.9366, 0.9415, 0.9469, 0.9489, 0.9512, 0.9532, 0.9556, 0.9568, 0.9581, 0.9615, 0.9612, 0.9638, 0.9642, 0.9655, 0.9668, 0.9687, 0.9687, 0.9709, 0.9721, 0.9728, 0.9747, 0.9758, 0.977, 0.9776, 0.9783, 0.98, 0.9802, 0.9823, 0.9825, 0.9849, 0.9853, 0.9861, 0.9866, 0.9863, 0.9885, 0.989, 0.9901, 0.9915, 0.9916, 0.9919, 0.9933, 0.9942, 0.9941, 0.9953, 0.9954, 0.9961, 0.9959, 0.9972, 0.9969, 0.9975, 0.9981, 0.9978, 0.9981, 0.9988, 0.9988, 0.9988, 0.9989, 0.9991, 0.9996, 0.9991, 0.9991, 0.9996, 0.9997, 0.9998, 0.9999, 0.9997, 0.9997, 0.9999]\n",
      "Validation Loss: [0.3487, 0.244, 0.1958, 0.1757, 0.1627, 0.1581, 0.1487, 0.1434, 0.1379, 0.1361, 0.1368, 0.1315, 0.1314, 0.1273, 0.1267, 0.1242, 0.1213, 0.1227, 0.1234, 0.117, 0.1193, 0.1214, 0.1158, 0.1158, 0.114, 0.1213, 0.1162, 0.1109, 0.1148, 0.1145, 0.1182, 0.1185, 0.1202, 0.1191, 0.1216, 0.1236, 0.1204, 0.1197, 0.1304, 0.1336, 0.1377, 0.1337, 0.138, 0.1435, 0.142, 0.1362, 0.1449, 0.1608, 0.1627, 0.1562, 0.17, 0.1734, 0.162, 0.1849, 0.1878, 0.1852, 0.1864, 0.192, 0.2067, 0.2208, 0.2224, 0.2417, 0.229, 0.2201, 0.2484, 0.2383, 0.225, 0.2712, 0.2575, 0.2748, 0.2671, 0.2614, 0.2752, 0.2786, 0.297]\n",
      "Validation Accuracy: [0.85, 0.9084, 0.9325, 0.9434, 0.949, 0.9519, 0.9532, 0.9532, 0.9534, 0.9546, 0.9566, 0.957, 0.959, 0.9581, 0.9608, 0.9608, 0.9601, 0.9612, 0.9626, 0.9619, 0.9641, 0.9637, 0.9623, 0.9643, 0.9637, 0.9632, 0.9639, 0.9637, 0.9641, 0.9646, 0.965, 0.9657, 0.9657, 0.9648, 0.9655, 0.967, 0.9663, 0.9652, 0.9657, 0.9668, 0.965, 0.9644, 0.9657, 0.9663, 0.967, 0.9663, 0.965, 0.967, 0.9675, 0.9663, 0.9648, 0.9664, 0.9666, 0.9672, 0.9673, 0.9672, 0.9663, 0.9677, 0.9628, 0.9681, 0.9672, 0.969, 0.9675, 0.9675, 0.9677, 0.9661, 0.9664, 0.9652, 0.9661, 0.9673, 0.9672, 0.9672, 0.9681, 0.9666, 0.9681]\n",
      "\n",
      "0 input_7 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22048 samples, validate on 5510 samples\n",
      "Epoch 1/75\n",
      "22048/22048 [==============================] - 33s 1ms/step - loss: 2.3052 - accuracy: 0.6665 - val_loss: 0.3495 - val_accuracy: 0.8550\n",
      "Epoch 2/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.6637 - accuracy: 0.7739 - val_loss: 0.2382 - val_accuracy: 0.9065\n",
      "Epoch 3/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.4211 - accuracy: 0.8434 - val_loss: 0.1858 - val_accuracy: 0.9323\n",
      "Epoch 4/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.3137 - accuracy: 0.8857 - val_loss: 0.1570 - val_accuracy: 0.9456\n",
      "Epoch 5/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.2415 - accuracy: 0.9147 - val_loss: 0.1447 - val_accuracy: 0.9477\n",
      "Epoch 6/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.2117 - accuracy: 0.9283 - val_loss: 0.1402 - val_accuracy: 0.9528\n",
      "Epoch 7/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1970 - accuracy: 0.9351 - val_loss: 0.1349 - val_accuracy: 0.9539\n",
      "Epoch 8/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1812 - accuracy: 0.9389 - val_loss: 0.1302 - val_accuracy: 0.9550\n",
      "Epoch 9/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1660 - accuracy: 0.9468 - val_loss: 0.1274 - val_accuracy: 0.9561\n",
      "Epoch 10/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1552 - accuracy: 0.9470 - val_loss: 0.1256 - val_accuracy: 0.9568\n",
      "Epoch 11/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1477 - accuracy: 0.9514 - val_loss: 0.1250 - val_accuracy: 0.9588\n",
      "Epoch 12/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1397 - accuracy: 0.9526 - val_loss: 0.1217 - val_accuracy: 0.9595\n",
      "Epoch 13/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1367 - accuracy: 0.9555 - val_loss: 0.1170 - val_accuracy: 0.9599\n",
      "Epoch 14/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1336 - accuracy: 0.9541 - val_loss: 0.1167 - val_accuracy: 0.9595\n",
      "Epoch 15/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1259 - accuracy: 0.9580 - val_loss: 0.1140 - val_accuracy: 0.9608\n",
      "Epoch 16/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1168 - accuracy: 0.9590 - val_loss: 0.1131 - val_accuracy: 0.9615\n",
      "Epoch 17/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1148 - accuracy: 0.9594 - val_loss: 0.1100 - val_accuracy: 0.9615\n",
      "Epoch 18/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1092 - accuracy: 0.9617 - val_loss: 0.1094 - val_accuracy: 0.9621\n",
      "Epoch 19/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1063 - accuracy: 0.9622 - val_loss: 0.1085 - val_accuracy: 0.9626\n",
      "Epoch 20/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.1027 - accuracy: 0.9634 - val_loss: 0.1098 - val_accuracy: 0.9628\n",
      "Epoch 21/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0997 - accuracy: 0.9648 - val_loss: 0.1082 - val_accuracy: 0.9628\n",
      "Epoch 22/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0943 - accuracy: 0.9661 - val_loss: 0.1067 - val_accuracy: 0.9617\n",
      "Epoch 23/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0926 - accuracy: 0.9660 - val_loss: 0.1064 - val_accuracy: 0.9633\n",
      "Epoch 24/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0887 - accuracy: 0.9675 - val_loss: 0.1045 - val_accuracy: 0.9624\n",
      "Epoch 25/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0848 - accuracy: 0.9697 - val_loss: 0.1089 - val_accuracy: 0.9624\n",
      "Epoch 26/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0812 - accuracy: 0.9693 - val_loss: 0.1074 - val_accuracy: 0.9633\n",
      "Epoch 27/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0758 - accuracy: 0.9727 - val_loss: 0.1058 - val_accuracy: 0.9633\n",
      "Epoch 28/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0746 - accuracy: 0.9730 - val_loss: 0.1054 - val_accuracy: 0.9646\n",
      "Epoch 29/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0706 - accuracy: 0.9738 - val_loss: 0.1069 - val_accuracy: 0.9637\n",
      "Epoch 30/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0661 - accuracy: 0.9752 - val_loss: 0.1060 - val_accuracy: 0.9635\n",
      "Epoch 31/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0641 - accuracy: 0.9764 - val_loss: 0.1065 - val_accuracy: 0.9642\n",
      "Epoch 32/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0618 - accuracy: 0.9769 - val_loss: 0.1072 - val_accuracy: 0.9639\n",
      "Epoch 33/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0594 - accuracy: 0.9780 - val_loss: 0.1132 - val_accuracy: 0.9642\n",
      "Epoch 34/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0553 - accuracy: 0.9800 - val_loss: 0.1084 - val_accuracy: 0.9652\n",
      "Epoch 35/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0534 - accuracy: 0.9805 - val_loss: 0.1106 - val_accuracy: 0.9641\n",
      "Epoch 36/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0504 - accuracy: 0.9819 - val_loss: 0.1151 - val_accuracy: 0.9642\n",
      "Epoch 37/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0481 - accuracy: 0.9817 - val_loss: 0.1151 - val_accuracy: 0.9655\n",
      "Epoch 38/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0469 - accuracy: 0.9822 - val_loss: 0.1147 - val_accuracy: 0.9648\n",
      "Epoch 39/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0441 - accuracy: 0.9836 - val_loss: 0.1227 - val_accuracy: 0.9648\n",
      "Epoch 40/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0400 - accuracy: 0.9851 - val_loss: 0.1246 - val_accuracy: 0.9652\n",
      "Epoch 41/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0396 - accuracy: 0.9852 - val_loss: 0.1279 - val_accuracy: 0.9642\n",
      "Epoch 42/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0371 - accuracy: 0.9871 - val_loss: 0.1251 - val_accuracy: 0.9662\n",
      "Epoch 43/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0354 - accuracy: 0.9875 - val_loss: 0.1300 - val_accuracy: 0.9648\n",
      "Epoch 44/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0335 - accuracy: 0.9887 - val_loss: 0.1300 - val_accuracy: 0.9648\n",
      "Epoch 45/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0316 - accuracy: 0.9886 - val_loss: 0.1417 - val_accuracy: 0.9637\n",
      "Epoch 46/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0292 - accuracy: 0.9906 - val_loss: 0.1344 - val_accuracy: 0.9642\n",
      "Epoch 47/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0279 - accuracy: 0.9901 - val_loss: 0.1408 - val_accuracy: 0.9648\n",
      "Epoch 48/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0251 - accuracy: 0.9916 - val_loss: 0.1467 - val_accuracy: 0.9659\n",
      "Epoch 49/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.1595 - val_accuracy: 0.9642\n",
      "Epoch 50/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0217 - accuracy: 0.9922 - val_loss: 0.1484 - val_accuracy: 0.9653\n",
      "Epoch 51/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 0.1524 - val_accuracy: 0.9644\n",
      "Epoch 52/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0198 - accuracy: 0.9927 - val_loss: 0.1524 - val_accuracy: 0.9659\n",
      "Epoch 53/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0178 - accuracy: 0.9940 - val_loss: 0.1568 - val_accuracy: 0.9664\n",
      "Epoch 54/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0164 - accuracy: 0.9944 - val_loss: 0.1587 - val_accuracy: 0.9657\n",
      "Epoch 55/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0156 - accuracy: 0.9948 - val_loss: 0.1705 - val_accuracy: 0.9661\n",
      "Epoch 56/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0142 - accuracy: 0.9951 - val_loss: 0.1675 - val_accuracy: 0.9655\n",
      "Epoch 57/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.1839 - val_accuracy: 0.9661\n",
      "Epoch 58/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.1825 - val_accuracy: 0.9664\n",
      "Epoch 59/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0103 - accuracy: 0.9971 - val_loss: 0.1755 - val_accuracy: 0.9670\n",
      "Epoch 60/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.1910 - val_accuracy: 0.9650\n",
      "Epoch 61/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.1962 - val_accuracy: 0.9655\n",
      "Epoch 62/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.1956 - val_accuracy: 0.9653\n",
      "Epoch 63/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.2114 - val_accuracy: 0.9642\n",
      "Epoch 64/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.2150 - val_accuracy: 0.9657\n",
      "Epoch 65/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 0.2139 - val_accuracy: 0.9662\n",
      "Epoch 66/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.2275 - val_accuracy: 0.9648\n",
      "Epoch 67/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.2377 - val_accuracy: 0.9630\n",
      "Epoch 68/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.2410 - val_accuracy: 0.9652\n",
      "Epoch 69/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2423 - val_accuracy: 0.9648\n",
      "Epoch 70/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.2522 - val_accuracy: 0.9661\n",
      "Epoch 71/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.2526 - val_accuracy: 0.9657\n",
      "Epoch 72/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.2561 - val_accuracy: 0.9655\n",
      "Epoch 73/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2561 - val_accuracy: 0.9662\n",
      "Epoch 74/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2482 - val_accuracy: 0.9655\n",
      "Epoch 75/75\n",
      "22048/22048 [==============================] - 32s 1ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.2729 - val_accuracy: 0.9672\n",
      "Training Loss: [2.3052, 0.6637, 0.4211, 0.3137, 0.2415, 0.2117, 0.197, 0.1812, 0.166, 0.1552, 0.1477, 0.1397, 0.1367, 0.1336, 0.1259, 0.1168, 0.1148, 0.1092, 0.1063, 0.1027, 0.0997, 0.0943, 0.0926, 0.0887, 0.0848, 0.0812, 0.0758, 0.0746, 0.0706, 0.0661, 0.0641, 0.0618, 0.0594, 0.0553, 0.0534, 0.0504, 0.0481, 0.0469, 0.0441, 0.04, 0.0396, 0.0371, 0.0354, 0.0335, 0.0316, 0.0292, 0.0279, 0.0251, 0.0237, 0.0217, 0.0209, 0.0198, 0.0178, 0.0164, 0.0156, 0.0142, 0.0131, 0.0117, 0.0103, 0.0107, 0.0083, 0.0074, 0.0069, 0.0067, 0.0062, 0.0057, 0.005, 0.0047, 0.0039, 0.0037, 0.0038, 0.0036, 0.0029, 0.0027, 0.002]\n",
      "Training Accuracy: [0.6665, 0.7739, 0.8434, 0.8857, 0.9147, 0.9283, 0.9351, 0.9389, 0.9468, 0.947, 0.9514, 0.9526, 0.9555, 0.9541, 0.958, 0.959, 0.9594, 0.9617, 0.9622, 0.9634, 0.9648, 0.9661, 0.966, 0.9675, 0.9697, 0.9693, 0.9727, 0.973, 0.9738, 0.9752, 0.9764, 0.9769, 0.978, 0.98, 0.9805, 0.9819, 0.9817, 0.9822, 0.9836, 0.9851, 0.9852, 0.9871, 0.9875, 0.9887, 0.9886, 0.9906, 0.9901, 0.9916, 0.992, 0.9922, 0.9927, 0.9927, 0.994, 0.9944, 0.9948, 0.9951, 0.996, 0.9965, 0.9971, 0.9967, 0.9978, 0.9978, 0.9981, 0.9984, 0.9986, 0.9985, 0.9986, 0.9989, 0.9993, 0.9992, 0.9991, 0.999, 0.9995, 0.9993, 0.9999]\n",
      "Validation Loss: [0.3495, 0.2382, 0.1858, 0.157, 0.1447, 0.1402, 0.1349, 0.1302, 0.1274, 0.1256, 0.125, 0.1217, 0.117, 0.1167, 0.114, 0.1131, 0.11, 0.1094, 0.1085, 0.1098, 0.1082, 0.1067, 0.1064, 0.1045, 0.1089, 0.1074, 0.1058, 0.1054, 0.1069, 0.106, 0.1065, 0.1072, 0.1132, 0.1084, 0.1106, 0.1151, 0.1151, 0.1147, 0.1227, 0.1246, 0.1279, 0.1251, 0.13, 0.13, 0.1417, 0.1344, 0.1408, 0.1467, 0.1595, 0.1484, 0.1524, 0.1524, 0.1568, 0.1587, 0.1705, 0.1675, 0.1839, 0.1825, 0.1755, 0.191, 0.1962, 0.1956, 0.2114, 0.215, 0.2139, 0.2275, 0.2377, 0.241, 0.2423, 0.2522, 0.2526, 0.2561, 0.2561, 0.2482, 0.2729]\n",
      "Validation Accuracy: [0.855, 0.9065, 0.9323, 0.9456, 0.9477, 0.9528, 0.9539, 0.955, 0.9561, 0.9568, 0.9588, 0.9595, 0.9599, 0.9595, 0.9608, 0.9615, 0.9615, 0.9621, 0.9626, 0.9628, 0.9628, 0.9617, 0.9633, 0.9624, 0.9624, 0.9633, 0.9633, 0.9646, 0.9637, 0.9635, 0.9642, 0.9639, 0.9642, 0.9652, 0.9641, 0.9642, 0.9655, 0.9648, 0.9648, 0.9652, 0.9642, 0.9662, 0.9648, 0.9648, 0.9637, 0.9642, 0.9648, 0.9659, 0.9642, 0.9653, 0.9644, 0.9659, 0.9664, 0.9657, 0.9661, 0.9655, 0.9661, 0.9664, 0.967, 0.965, 0.9655, 0.9653, 0.9642, 0.9657, 0.9662, 0.9648, 0.963, 0.9652, 0.9648, 0.9661, 0.9657, 0.9655, 0.9662, 0.9655, 0.9672]\n",
      "\n",
      "0 input_8 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 33s 1ms/step - loss: 2.4703 - accuracy: 0.6248 - val_loss: 0.3372 - val_accuracy: 0.8541\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.6861 - accuracy: 0.7512 - val_loss: 0.2299 - val_accuracy: 0.9202\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.4380 - accuracy: 0.8335 - val_loss: 0.1870 - val_accuracy: 0.9409\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.3180 - accuracy: 0.8841 - val_loss: 0.1645 - val_accuracy: 0.9523\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2527 - accuracy: 0.9120 - val_loss: 0.1517 - val_accuracy: 0.9548\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2223 - accuracy: 0.9248 - val_loss: 0.1442 - val_accuracy: 0.9577\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1940 - accuracy: 0.9365 - val_loss: 0.1346 - val_accuracy: 0.9597\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1880 - accuracy: 0.9384 - val_loss: 0.1320 - val_accuracy: 0.9608\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1736 - accuracy: 0.9427 - val_loss: 0.1272 - val_accuracy: 0.9614\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1599 - accuracy: 0.9462 - val_loss: 0.1296 - val_accuracy: 0.9617\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1541 - accuracy: 0.9489 - val_loss: 0.1207 - val_accuracy: 0.9621\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1433 - accuracy: 0.9518 - val_loss: 0.1176 - val_accuracy: 0.9628\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1400 - accuracy: 0.9532 - val_loss: 0.1193 - val_accuracy: 0.9626\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1344 - accuracy: 0.9554 - val_loss: 0.1198 - val_accuracy: 0.9624\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1260 - accuracy: 0.9565 - val_loss: 0.1125 - val_accuracy: 0.9641\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1222 - accuracy: 0.9582 - val_loss: 0.1108 - val_accuracy: 0.9644\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1169 - accuracy: 0.9595 - val_loss: 0.1099 - val_accuracy: 0.9637\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1134 - accuracy: 0.9605 - val_loss: 0.1076 - val_accuracy: 0.9637\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1063 - accuracy: 0.9624 - val_loss: 0.1048 - val_accuracy: 0.9655\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1065 - accuracy: 0.9631 - val_loss: 0.1045 - val_accuracy: 0.9653\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0996 - accuracy: 0.9642 - val_loss: 0.1027 - val_accuracy: 0.9661\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0975 - accuracy: 0.9656 - val_loss: 0.1030 - val_accuracy: 0.9655\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0929 - accuracy: 0.9673 - val_loss: 0.1032 - val_accuracy: 0.9661\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0892 - accuracy: 0.9683 - val_loss: 0.0986 - val_accuracy: 0.9653\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0870 - accuracy: 0.9693 - val_loss: 0.1030 - val_accuracy: 0.9644\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0827 - accuracy: 0.9706 - val_loss: 0.1000 - val_accuracy: 0.9655\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0780 - accuracy: 0.9717 - val_loss: 0.1002 - val_accuracy: 0.9648\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0751 - accuracy: 0.9731 - val_loss: 0.1035 - val_accuracy: 0.9661\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0720 - accuracy: 0.9742 - val_loss: 0.0975 - val_accuracy: 0.9663\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0686 - accuracy: 0.9751 - val_loss: 0.1006 - val_accuracy: 0.9670\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0675 - accuracy: 0.9754 - val_loss: 0.1041 - val_accuracy: 0.9652\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0627 - accuracy: 0.9775 - val_loss: 0.1058 - val_accuracy: 0.9653\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0591 - accuracy: 0.9791 - val_loss: 0.0966 - val_accuracy: 0.9670\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0580 - accuracy: 0.9792 - val_loss: 0.1017 - val_accuracy: 0.9675\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0546 - accuracy: 0.9798 - val_loss: 0.1008 - val_accuracy: 0.9668\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0516 - accuracy: 0.9820 - val_loss: 0.1046 - val_accuracy: 0.9663\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0481 - accuracy: 0.9834 - val_loss: 0.1036 - val_accuracy: 0.9670\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0460 - accuracy: 0.9843 - val_loss: 0.1024 - val_accuracy: 0.9692\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0451 - accuracy: 0.9844 - val_loss: 0.1047 - val_accuracy: 0.9670\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0437 - accuracy: 0.9844 - val_loss: 0.1052 - val_accuracy: 0.9690\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0397 - accuracy: 0.9860 - val_loss: 0.1078 - val_accuracy: 0.9668\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0379 - accuracy: 0.9869 - val_loss: 0.1070 - val_accuracy: 0.9673\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0349 - accuracy: 0.9876 - val_loss: 0.1083 - val_accuracy: 0.9686\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0318 - accuracy: 0.9890 - val_loss: 0.1130 - val_accuracy: 0.9679\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0300 - accuracy: 0.9905 - val_loss: 0.1152 - val_accuracy: 0.9688\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0277 - accuracy: 0.9902 - val_loss: 0.1157 - val_accuracy: 0.9701\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0278 - accuracy: 0.9909 - val_loss: 0.1168 - val_accuracy: 0.9686\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0239 - accuracy: 0.9921 - val_loss: 0.1299 - val_accuracy: 0.9670\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0219 - accuracy: 0.9932 - val_loss: 0.1317 - val_accuracy: 0.9661\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.1178 - val_accuracy: 0.9690\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0184 - accuracy: 0.9949 - val_loss: 0.1235 - val_accuracy: 0.9697\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0181 - accuracy: 0.9942 - val_loss: 0.1465 - val_accuracy: 0.9646\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0161 - accuracy: 0.9955 - val_loss: 0.1347 - val_accuracy: 0.9679\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.1445 - val_accuracy: 0.9684\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.1378 - val_accuracy: 0.9692\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.1545 - val_accuracy: 0.9652\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0106 - accuracy: 0.9974 - val_loss: 0.1533 - val_accuracy: 0.9683\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0103 - accuracy: 0.9971 - val_loss: 0.1621 - val_accuracy: 0.9683\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.1581 - val_accuracy: 0.9681\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.1529 - val_accuracy: 0.9693\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.1611 - val_accuracy: 0.9702\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.1593 - val_accuracy: 0.9693\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.1679 - val_accuracy: 0.9693\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.1763 - val_accuracy: 0.9701\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.1861 - val_accuracy: 0.9690\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.1731 - val_accuracy: 0.9690\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.1962 - val_accuracy: 0.9666\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.1851 - val_accuracy: 0.9668\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.2104 - val_accuracy: 0.9692\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.2005 - val_accuracy: 0.9693\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.2040 - val_accuracy: 0.9693\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.2139 - val_accuracy: 0.9699\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.2168 - val_accuracy: 0.9692\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2231 - val_accuracy: 0.9701\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.2422 - val_accuracy: 0.9693\n",
      "Training Loss: [2.4703, 0.6861, 0.438, 0.318, 0.2527, 0.2223, 0.194, 0.188, 0.1736, 0.1599, 0.1541, 0.1433, 0.14, 0.1344, 0.126, 0.1222, 0.1169, 0.1134, 0.1063, 0.1065, 0.0996, 0.0975, 0.0929, 0.0892, 0.087, 0.0827, 0.078, 0.0751, 0.072, 0.0686, 0.0675, 0.0627, 0.0591, 0.058, 0.0546, 0.0516, 0.0481, 0.046, 0.0451, 0.0437, 0.0397, 0.0379, 0.0349, 0.0318, 0.03, 0.0277, 0.0278, 0.0239, 0.0219, 0.0208, 0.0184, 0.0181, 0.0161, 0.0143, 0.0135, 0.012, 0.0106, 0.0103, 0.0085, 0.0081, 0.0067, 0.0063, 0.0062, 0.0058, 0.0041, 0.0044, 0.0032, 0.0037, 0.0033, 0.0029, 0.0021, 0.0026, 0.0016, 0.0018, 0.0018]\n",
      "Training Accuracy: [0.6248, 0.7512, 0.8335, 0.8841, 0.912, 0.9248, 0.9365, 0.9384, 0.9427, 0.9462, 0.9489, 0.9518, 0.9532, 0.9554, 0.9565, 0.9582, 0.9595, 0.9605, 0.9624, 0.9631, 0.9642, 0.9656, 0.9673, 0.9683, 0.9693, 0.9706, 0.9717, 0.9731, 0.9742, 0.9751, 0.9754, 0.9775, 0.9791, 0.9792, 0.9798, 0.982, 0.9834, 0.9843, 0.9844, 0.9844, 0.986, 0.9869, 0.9876, 0.989, 0.9905, 0.9902, 0.9909, 0.9921, 0.9932, 0.9931, 0.9949, 0.9942, 0.9955, 0.9954, 0.9962, 0.9961, 0.9974, 0.9971, 0.9976, 0.998, 0.9985, 0.9986, 0.9985, 0.9987, 0.9991, 0.9988, 0.9995, 0.9989, 0.999, 0.9992, 0.9997, 0.9993, 0.9999, 0.9997, 0.9996]\n",
      "Validation Loss: [0.3372, 0.2299, 0.187, 0.1645, 0.1517, 0.1442, 0.1346, 0.132, 0.1272, 0.1296, 0.1207, 0.1176, 0.1193, 0.1198, 0.1125, 0.1108, 0.1099, 0.1076, 0.1048, 0.1045, 0.1027, 0.103, 0.1032, 0.0986, 0.103, 0.1, 0.1002, 0.1035, 0.0975, 0.1006, 0.1041, 0.1058, 0.0966, 0.1017, 0.1008, 0.1046, 0.1036, 0.1024, 0.1047, 0.1052, 0.1078, 0.107, 0.1083, 0.113, 0.1152, 0.1157, 0.1168, 0.1299, 0.1317, 0.1178, 0.1235, 0.1465, 0.1347, 0.1445, 0.1378, 0.1545, 0.1533, 0.1621, 0.1581, 0.1529, 0.1611, 0.1593, 0.1679, 0.1763, 0.1861, 0.1731, 0.1962, 0.1851, 0.2104, 0.2005, 0.204, 0.2139, 0.2168, 0.2231, 0.2422]\n",
      "Validation Accuracy: [0.8541, 0.9202, 0.9409, 0.9523, 0.9548, 0.9577, 0.9597, 0.9608, 0.9614, 0.9617, 0.9621, 0.9628, 0.9626, 0.9624, 0.9641, 0.9644, 0.9637, 0.9637, 0.9655, 0.9653, 0.9661, 0.9655, 0.9661, 0.9653, 0.9644, 0.9655, 0.9648, 0.9661, 0.9663, 0.967, 0.9652, 0.9653, 0.967, 0.9675, 0.9668, 0.9663, 0.967, 0.9692, 0.967, 0.969, 0.9668, 0.9673, 0.9686, 0.9679, 0.9688, 0.9701, 0.9686, 0.967, 0.9661, 0.969, 0.9697, 0.9646, 0.9679, 0.9684, 0.9692, 0.9652, 0.9683, 0.9683, 0.9681, 0.9693, 0.9702, 0.9693, 0.9693, 0.9701, 0.969, 0.969, 0.9666, 0.9668, 0.9692, 0.9693, 0.9693, 0.9699, 0.9692, 0.9701, 0.9693]\n",
      "\n",
      "0 input_9 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 33s 1ms/step - loss: 2.3918 - accuracy: 0.6243 - val_loss: 0.3338 - val_accuracy: 0.8507\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.6676 - accuracy: 0.7439 - val_loss: 0.2343 - val_accuracy: 0.9086\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.3962 - accuracy: 0.8420 - val_loss: 0.1922 - val_accuracy: 0.9343\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2957 - accuracy: 0.8891 - val_loss: 0.1708 - val_accuracy: 0.9436\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2433 - accuracy: 0.9114 - val_loss: 0.1606 - val_accuracy: 0.9485\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2108 - accuracy: 0.9264 - val_loss: 0.1524 - val_accuracy: 0.9505\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1845 - accuracy: 0.9394 - val_loss: 0.1491 - val_accuracy: 0.9523\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1710 - accuracy: 0.9444 - val_loss: 0.1439 - val_accuracy: 0.9546\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1603 - accuracy: 0.9482 - val_loss: 0.1468 - val_accuracy: 0.9550\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1501 - accuracy: 0.9511 - val_loss: 0.1386 - val_accuracy: 0.9556\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1446 - accuracy: 0.9528 - val_loss: 0.1360 - val_accuracy: 0.9546\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1327 - accuracy: 0.9559 - val_loss: 0.1364 - val_accuracy: 0.9565\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1333 - accuracy: 0.9566 - val_loss: 0.1317 - val_accuracy: 0.9554\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1258 - accuracy: 0.9593 - val_loss: 0.1392 - val_accuracy: 0.9563\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1231 - accuracy: 0.9590 - val_loss: 0.1282 - val_accuracy: 0.9572\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1129 - accuracy: 0.9617 - val_loss: 0.1290 - val_accuracy: 0.9566\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1122 - accuracy: 0.9629 - val_loss: 0.1270 - val_accuracy: 0.9581\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1059 - accuracy: 0.9633 - val_loss: 0.1249 - val_accuracy: 0.9592\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1010 - accuracy: 0.9652 - val_loss: 0.1239 - val_accuracy: 0.9595\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1000 - accuracy: 0.9664 - val_loss: 0.1227 - val_accuracy: 0.9597\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0940 - accuracy: 0.9677 - val_loss: 0.1224 - val_accuracy: 0.9601\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0902 - accuracy: 0.9681 - val_loss: 0.1219 - val_accuracy: 0.9597\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0877 - accuracy: 0.9702 - val_loss: 0.1273 - val_accuracy: 0.9604\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0844 - accuracy: 0.9709 - val_loss: 0.1190 - val_accuracy: 0.9599\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0796 - accuracy: 0.9719 - val_loss: 0.1220 - val_accuracy: 0.9601\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0766 - accuracy: 0.9737 - val_loss: 0.1231 - val_accuracy: 0.9606\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0724 - accuracy: 0.9736 - val_loss: 0.1259 - val_accuracy: 0.9601\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0707 - accuracy: 0.9756 - val_loss: 0.1268 - val_accuracy: 0.9588\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0652 - accuracy: 0.9775 - val_loss: 0.1279 - val_accuracy: 0.9592\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0619 - accuracy: 0.9774 - val_loss: 0.1257 - val_accuracy: 0.9619\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0599 - accuracy: 0.9784 - val_loss: 0.1235 - val_accuracy: 0.9612\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0565 - accuracy: 0.9800 - val_loss: 0.1337 - val_accuracy: 0.9604\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0540 - accuracy: 0.9807 - val_loss: 0.1298 - val_accuracy: 0.9608\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0501 - accuracy: 0.9821 - val_loss: 0.1337 - val_accuracy: 0.9608\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0476 - accuracy: 0.9826 - val_loss: 0.1408 - val_accuracy: 0.9612\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0460 - accuracy: 0.9833 - val_loss: 0.1323 - val_accuracy: 0.9608\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0408 - accuracy: 0.9863 - val_loss: 0.1350 - val_accuracy: 0.9610\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0383 - accuracy: 0.9865 - val_loss: 0.1472 - val_accuracy: 0.9617\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0378 - accuracy: 0.9857 - val_loss: 0.1474 - val_accuracy: 0.9615\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0333 - accuracy: 0.9887 - val_loss: 0.1481 - val_accuracy: 0.9606\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0318 - accuracy: 0.9881 - val_loss: 0.1628 - val_accuracy: 0.9612\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0281 - accuracy: 0.9901 - val_loss: 0.1625 - val_accuracy: 0.9614\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0270 - accuracy: 0.9910 - val_loss: 0.1741 - val_accuracy: 0.9606\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0256 - accuracy: 0.9912 - val_loss: 0.1647 - val_accuracy: 0.9617\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 0.1839 - val_accuracy: 0.9619\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0215 - accuracy: 0.9922 - val_loss: 0.1773 - val_accuracy: 0.9621\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0191 - accuracy: 0.9936 - val_loss: 0.1827 - val_accuracy: 0.9608\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0185 - accuracy: 0.9938 - val_loss: 0.1816 - val_accuracy: 0.9619\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.1969 - val_accuracy: 0.9619\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 0.2044 - val_accuracy: 0.9606\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0135 - accuracy: 0.9958 - val_loss: 0.2072 - val_accuracy: 0.9615\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0124 - accuracy: 0.9964 - val_loss: 0.1980 - val_accuracy: 0.9603\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0114 - accuracy: 0.9968 - val_loss: 0.2074 - val_accuracy: 0.9614\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.2239 - val_accuracy: 0.9628\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0092 - accuracy: 0.9976 - val_loss: 0.2232 - val_accuracy: 0.9619\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.2368 - val_accuracy: 0.9626\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0069 - accuracy: 0.9976 - val_loss: 0.2325 - val_accuracy: 0.9623\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.2407 - val_accuracy: 0.9621\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.2553 - val_accuracy: 0.9624\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.2647 - val_accuracy: 0.9614\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.2547 - val_accuracy: 0.9621\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.2684 - val_accuracy: 0.9621\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.2831 - val_accuracy: 0.9614\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2789 - val_accuracy: 0.9632\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.2975 - val_accuracy: 0.9632\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2966 - val_accuracy: 0.9632\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2974 - val_accuracy: 0.9621\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.3328 - val_accuracy: 0.9630\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.3253 - val_accuracy: 0.9604\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.3050 - val_accuracy: 0.9608\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.3230 - val_accuracy: 0.9632\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3237 - val_accuracy: 0.9632\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.3305 - val_accuracy: 0.9615\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3295 - val_accuracy: 0.9624\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.3307 - val_accuracy: 0.9634\n",
      "Training Loss: [2.3918, 0.6676, 0.3962, 0.2957, 0.2433, 0.2108, 0.1845, 0.171, 0.1603, 0.1501, 0.1446, 0.1327, 0.1333, 0.1258, 0.1231, 0.1129, 0.1122, 0.1059, 0.101, 0.1, 0.094, 0.0902, 0.0877, 0.0844, 0.0796, 0.0766, 0.0724, 0.0707, 0.0652, 0.0619, 0.0599, 0.0565, 0.054, 0.0501, 0.0476, 0.046, 0.0408, 0.0383, 0.0378, 0.0333, 0.0318, 0.0281, 0.027, 0.0256, 0.0239, 0.0215, 0.0191, 0.0185, 0.0166, 0.0148, 0.0135, 0.0124, 0.0114, 0.0101, 0.0092, 0.0084, 0.0069, 0.0069, 0.0053, 0.0054, 0.0041, 0.0043, 0.0036, 0.0033, 0.0032, 0.0023, 0.0021, 0.0019, 0.0042, 0.0025, 0.0017, 0.0012, 0.0017, 0.0014, 0.0019]\n",
      "Training Accuracy: [0.6243, 0.7439, 0.842, 0.8891, 0.9114, 0.9264, 0.9394, 0.9444, 0.9482, 0.9511, 0.9528, 0.9559, 0.9566, 0.9593, 0.959, 0.9617, 0.9629, 0.9633, 0.9652, 0.9664, 0.9677, 0.9681, 0.9702, 0.9709, 0.9719, 0.9737, 0.9736, 0.9756, 0.9775, 0.9774, 0.9784, 0.98, 0.9807, 0.9821, 0.9826, 0.9833, 0.9863, 0.9865, 0.9857, 0.9887, 0.9881, 0.9901, 0.991, 0.9912, 0.9918, 0.9922, 0.9936, 0.9938, 0.9946, 0.9952, 0.9958, 0.9964, 0.9968, 0.9971, 0.9976, 0.9977, 0.9976, 0.9982, 0.999, 0.9985, 0.9992, 0.9988, 0.9991, 0.9994, 0.9992, 0.9996, 0.9998, 0.9997, 0.9989, 0.9995, 0.9997, 0.9999, 0.9995, 0.9998, 0.9996]\n",
      "Validation Loss: [0.3338, 0.2343, 0.1922, 0.1708, 0.1606, 0.1524, 0.1491, 0.1439, 0.1468, 0.1386, 0.136, 0.1364, 0.1317, 0.1392, 0.1282, 0.129, 0.127, 0.1249, 0.1239, 0.1227, 0.1224, 0.1219, 0.1273, 0.119, 0.122, 0.1231, 0.1259, 0.1268, 0.1279, 0.1257, 0.1235, 0.1337, 0.1298, 0.1337, 0.1408, 0.1323, 0.135, 0.1472, 0.1474, 0.1481, 0.1628, 0.1625, 0.1741, 0.1647, 0.1839, 0.1773, 0.1827, 0.1816, 0.1969, 0.2044, 0.2072, 0.198, 0.2074, 0.2239, 0.2232, 0.2368, 0.2325, 0.2407, 0.2553, 0.2647, 0.2547, 0.2684, 0.2831, 0.2789, 0.2975, 0.2966, 0.2974, 0.3328, 0.3253, 0.305, 0.323, 0.3237, 0.3305, 0.3295, 0.3307]\n",
      "Validation Accuracy: [0.8507, 0.9086, 0.9343, 0.9436, 0.9485, 0.9505, 0.9523, 0.9546, 0.955, 0.9556, 0.9546, 0.9565, 0.9554, 0.9563, 0.9572, 0.9566, 0.9581, 0.9592, 0.9595, 0.9597, 0.9601, 0.9597, 0.9604, 0.9599, 0.9601, 0.9606, 0.9601, 0.9588, 0.9592, 0.9619, 0.9612, 0.9604, 0.9608, 0.9608, 0.9612, 0.9608, 0.961, 0.9617, 0.9615, 0.9606, 0.9612, 0.9614, 0.9606, 0.9617, 0.9619, 0.9621, 0.9608, 0.9619, 0.9619, 0.9606, 0.9615, 0.9603, 0.9614, 0.9628, 0.9619, 0.9626, 0.9623, 0.9621, 0.9624, 0.9614, 0.9621, 0.9621, 0.9614, 0.9632, 0.9632, 0.9632, 0.9621, 0.963, 0.9604, 0.9608, 0.9632, 0.9632, 0.9615, 0.9624, 0.9634]\n",
      "\n",
      "0 input_10 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 33s 1ms/step - loss: 2.6177 - accuracy: 0.6053 - val_loss: 0.3954 - val_accuracy: 0.8055\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.7119 - accuracy: 0.7177 - val_loss: 0.2856 - val_accuracy: 0.8766\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.4818 - accuracy: 0.7980 - val_loss: 0.2220 - val_accuracy: 0.9100\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.3540 - accuracy: 0.8545 - val_loss: 0.1773 - val_accuracy: 0.9378\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2741 - accuracy: 0.8925 - val_loss: 0.1508 - val_accuracy: 0.9490\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2232 - accuracy: 0.9181 - val_loss: 0.1376 - val_accuracy: 0.9537\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2018 - accuracy: 0.9295 - val_loss: 0.1301 - val_accuracy: 0.9566\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1781 - accuracy: 0.9400 - val_loss: 0.1256 - val_accuracy: 0.9579\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1662 - accuracy: 0.9435 - val_loss: 0.1255 - val_accuracy: 0.9590\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1558 - accuracy: 0.9486 - val_loss: 0.1201 - val_accuracy: 0.9614\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1464 - accuracy: 0.9517 - val_loss: 0.1194 - val_accuracy: 0.9615\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1389 - accuracy: 0.9544 - val_loss: 0.1177 - val_accuracy: 0.9628\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1328 - accuracy: 0.9551 - val_loss: 0.1148 - val_accuracy: 0.9630\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1285 - accuracy: 0.9568 - val_loss: 0.1138 - val_accuracy: 0.9632\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1192 - accuracy: 0.9594 - val_loss: 0.1136 - val_accuracy: 0.9632\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1171 - accuracy: 0.9583 - val_loss: 0.1109 - val_accuracy: 0.9635\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1129 - accuracy: 0.9606 - val_loss: 0.1101 - val_accuracy: 0.9632\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1087 - accuracy: 0.9624 - val_loss: 0.1079 - val_accuracy: 0.9637\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1032 - accuracy: 0.9641 - val_loss: 0.1092 - val_accuracy: 0.9639\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1023 - accuracy: 0.9647 - val_loss: 0.1074 - val_accuracy: 0.9635\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0974 - accuracy: 0.9657 - val_loss: 0.1080 - val_accuracy: 0.9639\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0943 - accuracy: 0.9674 - val_loss: 0.1059 - val_accuracy: 0.9628\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0885 - accuracy: 0.9694 - val_loss: 0.1055 - val_accuracy: 0.9641\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0865 - accuracy: 0.9699 - val_loss: 0.1046 - val_accuracy: 0.9648\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0817 - accuracy: 0.9721 - val_loss: 0.1068 - val_accuracy: 0.9657\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0762 - accuracy: 0.9737 - val_loss: 0.1071 - val_accuracy: 0.9659\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0757 - accuracy: 0.9740 - val_loss: 0.1075 - val_accuracy: 0.9646\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0723 - accuracy: 0.9750 - val_loss: 0.1046 - val_accuracy: 0.9655\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0677 - accuracy: 0.9754 - val_loss: 0.1082 - val_accuracy: 0.9663\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0648 - accuracy: 0.9779 - val_loss: 0.1056 - val_accuracy: 0.9666\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0646 - accuracy: 0.9771 - val_loss: 0.1061 - val_accuracy: 0.9648\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0601 - accuracy: 0.9785 - val_loss: 0.1072 - val_accuracy: 0.9670\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0572 - accuracy: 0.9796 - val_loss: 0.1079 - val_accuracy: 0.9664\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0541 - accuracy: 0.9809 - val_loss: 0.1069 - val_accuracy: 0.9655\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0512 - accuracy: 0.9814 - val_loss: 0.1115 - val_accuracy: 0.9664\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0500 - accuracy: 0.9818 - val_loss: 0.1138 - val_accuracy: 0.9661\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0458 - accuracy: 0.9838 - val_loss: 0.1163 - val_accuracy: 0.9673\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0430 - accuracy: 0.9844 - val_loss: 0.1171 - val_accuracy: 0.9672\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0394 - accuracy: 0.9863 - val_loss: 0.1171 - val_accuracy: 0.9663\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0369 - accuracy: 0.9867 - val_loss: 0.1211 - val_accuracy: 0.9657\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0352 - accuracy: 0.9882 - val_loss: 0.1207 - val_accuracy: 0.9666\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.1201 - val_accuracy: 0.9675\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0302 - accuracy: 0.9898 - val_loss: 0.1243 - val_accuracy: 0.9661\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 0.1247 - val_accuracy: 0.9661\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0259 - accuracy: 0.9912 - val_loss: 0.1280 - val_accuracy: 0.9677\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0244 - accuracy: 0.9918 - val_loss: 0.1296 - val_accuracy: 0.9666\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0225 - accuracy: 0.9928 - val_loss: 0.1447 - val_accuracy: 0.9643\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.1406 - val_accuracy: 0.9679\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0191 - accuracy: 0.9938 - val_loss: 0.1475 - val_accuracy: 0.9673\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0171 - accuracy: 0.9944 - val_loss: 0.1561 - val_accuracy: 0.9637\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0154 - accuracy: 0.9953 - val_loss: 0.1504 - val_accuracy: 0.9686\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.1586 - val_accuracy: 0.9681\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.1551 - val_accuracy: 0.9677\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0123 - accuracy: 0.9965 - val_loss: 0.1723 - val_accuracy: 0.9666\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.1629 - val_accuracy: 0.9679\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 0.1643 - val_accuracy: 0.9670\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.1736 - val_accuracy: 0.9670\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.1771 - val_accuracy: 0.9683\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.1809 - val_accuracy: 0.9677\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.1832 - val_accuracy: 0.9684\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.1961 - val_accuracy: 0.9672\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.2086 - val_accuracy: 0.9661\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.2013 - val_accuracy: 0.9670\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.2062 - val_accuracy: 0.9683\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.2159 - val_accuracy: 0.9673\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.2190 - val_accuracy: 0.9692\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2209 - val_accuracy: 0.9692\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.2262 - val_accuracy: 0.9664\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.2259 - val_accuracy: 0.9679\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.2309 - val_accuracy: 0.9690\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2368 - val_accuracy: 0.9670\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.2483 - val_accuracy: 0.9677\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2409 - val_accuracy: 0.9624\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2483 - val_accuracy: 0.9683\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2504 - val_accuracy: 0.9692\n",
      "Training Loss: [2.6177, 0.7119, 0.4818, 0.354, 0.2741, 0.2232, 0.2018, 0.1781, 0.1662, 0.1558, 0.1464, 0.1389, 0.1328, 0.1285, 0.1192, 0.1171, 0.1129, 0.1087, 0.1032, 0.1023, 0.0974, 0.0943, 0.0885, 0.0865, 0.0817, 0.0762, 0.0757, 0.0723, 0.0677, 0.0648, 0.0646, 0.0601, 0.0572, 0.0541, 0.0512, 0.05, 0.0458, 0.043, 0.0394, 0.0369, 0.0352, 0.0333, 0.0302, 0.0285, 0.0259, 0.0244, 0.0225, 0.0208, 0.0191, 0.0171, 0.0154, 0.0143, 0.0135, 0.0123, 0.0112, 0.01, 0.0085, 0.0084, 0.007, 0.0061, 0.0058, 0.0055, 0.0049, 0.004, 0.0036, 0.0034, 0.0028, 0.0033, 0.0035, 0.0022, 0.0018, 0.002, 0.0021, 0.0022, 0.0025]\n",
      "Training Accuracy: [0.6053, 0.7177, 0.798, 0.8545, 0.8925, 0.9181, 0.9295, 0.94, 0.9435, 0.9486, 0.9517, 0.9544, 0.9551, 0.9568, 0.9594, 0.9583, 0.9606, 0.9624, 0.9641, 0.9647, 0.9657, 0.9674, 0.9694, 0.9699, 0.9721, 0.9737, 0.974, 0.975, 0.9754, 0.9779, 0.9771, 0.9785, 0.9796, 0.9809, 0.9814, 0.9818, 0.9838, 0.9844, 0.9863, 0.9867, 0.9882, 0.9889, 0.9898, 0.9908, 0.9912, 0.9918, 0.9928, 0.9932, 0.9938, 0.9944, 0.9953, 0.9956, 0.9957, 0.9965, 0.9968, 0.9971, 0.9976, 0.9973, 0.9982, 0.9983, 0.9985, 0.9984, 0.9989, 0.9992, 0.9992, 0.9992, 0.9995, 0.9992, 0.9991, 0.9998, 0.9997, 0.9997, 0.9995, 0.9996, 0.9994]\n",
      "Validation Loss: [0.3954, 0.2856, 0.222, 0.1773, 0.1508, 0.1376, 0.1301, 0.1256, 0.1255, 0.1201, 0.1194, 0.1177, 0.1148, 0.1138, 0.1136, 0.1109, 0.1101, 0.1079, 0.1092, 0.1074, 0.108, 0.1059, 0.1055, 0.1046, 0.1068, 0.1071, 0.1075, 0.1046, 0.1082, 0.1056, 0.1061, 0.1072, 0.1079, 0.1069, 0.1115, 0.1138, 0.1163, 0.1171, 0.1171, 0.1211, 0.1207, 0.1201, 0.1243, 0.1247, 0.128, 0.1296, 0.1447, 0.1406, 0.1475, 0.1561, 0.1504, 0.1586, 0.1551, 0.1723, 0.1629, 0.1643, 0.1736, 0.1771, 0.1809, 0.1832, 0.1961, 0.2086, 0.2013, 0.2062, 0.2159, 0.219, 0.2209, 0.2262, 0.2259, 0.2309, 0.2368, 0.2483, 0.2409, 0.2483, 0.2504]\n",
      "Validation Accuracy: [0.8055, 0.8766, 0.91, 0.9378, 0.949, 0.9537, 0.9566, 0.9579, 0.959, 0.9614, 0.9615, 0.9628, 0.963, 0.9632, 0.9632, 0.9635, 0.9632, 0.9637, 0.9639, 0.9635, 0.9639, 0.9628, 0.9641, 0.9648, 0.9657, 0.9659, 0.9646, 0.9655, 0.9663, 0.9666, 0.9648, 0.967, 0.9664, 0.9655, 0.9664, 0.9661, 0.9673, 0.9672, 0.9663, 0.9657, 0.9666, 0.9675, 0.9661, 0.9661, 0.9677, 0.9666, 0.9643, 0.9679, 0.9673, 0.9637, 0.9686, 0.9681, 0.9677, 0.9666, 0.9679, 0.967, 0.967, 0.9683, 0.9677, 0.9684, 0.9672, 0.9661, 0.967, 0.9683, 0.9673, 0.9692, 0.9692, 0.9664, 0.9679, 0.969, 0.967, 0.9677, 0.9624, 0.9683, 0.9692]\n",
      "\n",
      "0 input_11 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 2.4659 - accuracy: 0.6252 - val_loss: 0.3624 - val_accuracy: 0.8433\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.7009 - accuracy: 0.7429 - val_loss: 0.2360 - val_accuracy: 0.9129\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.4231 - accuracy: 0.8324 - val_loss: 0.1819 - val_accuracy: 0.9367\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.3100 - accuracy: 0.8890 - val_loss: 0.1616 - val_accuracy: 0.9479\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2377 - accuracy: 0.9141 - val_loss: 0.1540 - val_accuracy: 0.9514\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.2139 - accuracy: 0.9277 - val_loss: 0.1475 - val_accuracy: 0.9516\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1955 - accuracy: 0.9342 - val_loss: 0.1440 - val_accuracy: 0.9528\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1749 - accuracy: 0.9418 - val_loss: 0.1395 - val_accuracy: 0.9548\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1609 - accuracy: 0.9469 - val_loss: 0.1372 - val_accuracy: 0.9546\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1575 - accuracy: 0.9464 - val_loss: 0.1340 - val_accuracy: 0.9554\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1462 - accuracy: 0.9519 - val_loss: 0.1306 - val_accuracy: 0.9550\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1385 - accuracy: 0.9529 - val_loss: 0.1296 - val_accuracy: 0.9556\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1345 - accuracy: 0.9553 - val_loss: 0.1285 - val_accuracy: 0.9563\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1282 - accuracy: 0.9565 - val_loss: 0.1234 - val_accuracy: 0.9577\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1217 - accuracy: 0.9597 - val_loss: 0.1224 - val_accuracy: 0.9575\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1189 - accuracy: 0.9589 - val_loss: 0.1197 - val_accuracy: 0.9601\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1139 - accuracy: 0.9603 - val_loss: 0.1199 - val_accuracy: 0.9597\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1101 - accuracy: 0.9614 - val_loss: 0.1188 - val_accuracy: 0.9599\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.1047 - accuracy: 0.9630 - val_loss: 0.1167 - val_accuracy: 0.9608\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0995 - accuracy: 0.9641 - val_loss: 0.1159 - val_accuracy: 0.9612\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0961 - accuracy: 0.9658 - val_loss: 0.1156 - val_accuracy: 0.9610\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0942 - accuracy: 0.9663 - val_loss: 0.1170 - val_accuracy: 0.9617\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0901 - accuracy: 0.9668 - val_loss: 0.1133 - val_accuracy: 0.9626\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0863 - accuracy: 0.9694 - val_loss: 0.1160 - val_accuracy: 0.9632\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0836 - accuracy: 0.9703 - val_loss: 0.1176 - val_accuracy: 0.9644\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0792 - accuracy: 0.9711 - val_loss: 0.1143 - val_accuracy: 0.9634\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0773 - accuracy: 0.9723 - val_loss: 0.1205 - val_accuracy: 0.9635\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0718 - accuracy: 0.9736 - val_loss: 0.1141 - val_accuracy: 0.9648\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0661 - accuracy: 0.9759 - val_loss: 0.1173 - val_accuracy: 0.9648\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0649 - accuracy: 0.9760 - val_loss: 0.1122 - val_accuracy: 0.9655\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0624 - accuracy: 0.9765 - val_loss: 0.1180 - val_accuracy: 0.9655\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0598 - accuracy: 0.9774 - val_loss: 0.1118 - val_accuracy: 0.9663\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0567 - accuracy: 0.9795 - val_loss: 0.1232 - val_accuracy: 0.9659\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0537 - accuracy: 0.9808 - val_loss: 0.1188 - val_accuracy: 0.9668\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0491 - accuracy: 0.9818 - val_loss: 0.1200 - val_accuracy: 0.9668\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0489 - accuracy: 0.9825 - val_loss: 0.1207 - val_accuracy: 0.9668\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0462 - accuracy: 0.9834 - val_loss: 0.1164 - val_accuracy: 0.9681\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0426 - accuracy: 0.9844 - val_loss: 0.1249 - val_accuracy: 0.9657\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0403 - accuracy: 0.9857 - val_loss: 0.1231 - val_accuracy: 0.9688\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0392 - accuracy: 0.9861 - val_loss: 0.1248 - val_accuracy: 0.9681\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0358 - accuracy: 0.9878 - val_loss: 0.1332 - val_accuracy: 0.9661\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0340 - accuracy: 0.9878 - val_loss: 0.1286 - val_accuracy: 0.9679\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0317 - accuracy: 0.9894 - val_loss: 0.1321 - val_accuracy: 0.9668\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0297 - accuracy: 0.9897 - val_loss: 0.1399 - val_accuracy: 0.9681\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0263 - accuracy: 0.9910 - val_loss: 0.1342 - val_accuracy: 0.9677\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0255 - accuracy: 0.9918 - val_loss: 0.1508 - val_accuracy: 0.9666\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0234 - accuracy: 0.9918 - val_loss: 0.1490 - val_accuracy: 0.9681\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 0.1451 - val_accuracy: 0.9673\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0206 - accuracy: 0.9929 - val_loss: 0.1532 - val_accuracy: 0.9666\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1548 - val_accuracy: 0.9641\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.1553 - val_accuracy: 0.9688\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.1610 - val_accuracy: 0.9672\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0147 - accuracy: 0.9959 - val_loss: 0.1625 - val_accuracy: 0.9666\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0124 - accuracy: 0.9961 - val_loss: 0.1782 - val_accuracy: 0.9650\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.1706 - val_accuracy: 0.9675\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 0.1794 - val_accuracy: 0.9672\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.1864 - val_accuracy: 0.9673\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.1902 - val_accuracy: 0.9672\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.1950 - val_accuracy: 0.9666\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.1944 - val_accuracy: 0.9679\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.2108 - val_accuracy: 0.9686\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.2089 - val_accuracy: 0.9659\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.2027 - val_accuracy: 0.9670\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.2127 - val_accuracy: 0.9684\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.2100 - val_accuracy: 0.9679\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.2207 - val_accuracy: 0.9639\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.2254 - val_accuracy: 0.9679\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.2169 - val_accuracy: 0.9668\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.2320 - val_accuracy: 0.9666\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.2328 - val_accuracy: 0.9686\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.2330 - val_accuracy: 0.9672\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2486 - val_accuracy: 0.9668\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.2609 - val_accuracy: 0.9659\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2538 - val_accuracy: 0.9675\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 32s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2465 - val_accuracy: 0.9670\n",
      "Training Loss: [2.4659, 0.7009, 0.4231, 0.31, 0.2377, 0.2139, 0.1955, 0.1749, 0.1609, 0.1575, 0.1462, 0.1385, 0.1345, 0.1282, 0.1217, 0.1189, 0.1139, 0.1101, 0.1047, 0.0995, 0.0961, 0.0942, 0.0901, 0.0863, 0.0836, 0.0792, 0.0773, 0.0718, 0.0661, 0.0649, 0.0624, 0.0598, 0.0567, 0.0537, 0.0491, 0.0489, 0.0462, 0.0426, 0.0403, 0.0392, 0.0358, 0.034, 0.0317, 0.0297, 0.0263, 0.0255, 0.0234, 0.0218, 0.0206, 0.0185, 0.0176, 0.0158, 0.0147, 0.0124, 0.0125, 0.0125, 0.0094, 0.0094, 0.0081, 0.007, 0.0064, 0.0051, 0.0059, 0.005, 0.0043, 0.0046, 0.004, 0.0047, 0.0032, 0.0027, 0.0033, 0.0028, 0.0021, 0.0017, 0.0018]\n",
      "Training Accuracy: [0.6252, 0.7429, 0.8324, 0.889, 0.9141, 0.9277, 0.9342, 0.9418, 0.9469, 0.9464, 0.9519, 0.9529, 0.9553, 0.9565, 0.9597, 0.9589, 0.9603, 0.9614, 0.963, 0.9641, 0.9658, 0.9663, 0.9668, 0.9694, 0.9703, 0.9711, 0.9723, 0.9736, 0.9759, 0.976, 0.9765, 0.9774, 0.9795, 0.9808, 0.9818, 0.9825, 0.9834, 0.9844, 0.9857, 0.9861, 0.9878, 0.9878, 0.9894, 0.9897, 0.991, 0.9918, 0.9918, 0.9932, 0.9929, 0.9939, 0.9942, 0.9951, 0.9959, 0.9961, 0.9961, 0.9963, 0.9975, 0.9973, 0.9976, 0.9985, 0.9983, 0.999, 0.9982, 0.9987, 0.9991, 0.9988, 0.9993, 0.999, 0.9994, 0.9994, 0.9992, 0.9994, 0.9994, 0.9998, 0.9997]\n",
      "Validation Loss: [0.3624, 0.236, 0.1819, 0.1616, 0.154, 0.1475, 0.144, 0.1395, 0.1372, 0.134, 0.1306, 0.1296, 0.1285, 0.1234, 0.1224, 0.1197, 0.1199, 0.1188, 0.1167, 0.1159, 0.1156, 0.117, 0.1133, 0.116, 0.1176, 0.1143, 0.1205, 0.1141, 0.1173, 0.1122, 0.118, 0.1118, 0.1232, 0.1188, 0.12, 0.1207, 0.1164, 0.1249, 0.1231, 0.1248, 0.1332, 0.1286, 0.1321, 0.1399, 0.1342, 0.1508, 0.149, 0.1451, 0.1532, 0.1548, 0.1553, 0.161, 0.1625, 0.1782, 0.1706, 0.1794, 0.1864, 0.1902, 0.195, 0.1944, 0.2108, 0.2089, 0.2027, 0.2127, 0.21, 0.2207, 0.2254, 0.2169, 0.232, 0.2328, 0.233, 0.2486, 0.2609, 0.2538, 0.2465]\n",
      "Validation Accuracy: [0.8433, 0.9129, 0.9367, 0.9479, 0.9514, 0.9516, 0.9528, 0.9548, 0.9546, 0.9554, 0.955, 0.9556, 0.9563, 0.9577, 0.9575, 0.9601, 0.9597, 0.9599, 0.9608, 0.9612, 0.961, 0.9617, 0.9626, 0.9632, 0.9644, 0.9634, 0.9635, 0.9648, 0.9648, 0.9655, 0.9655, 0.9663, 0.9659, 0.9668, 0.9668, 0.9668, 0.9681, 0.9657, 0.9688, 0.9681, 0.9661, 0.9679, 0.9668, 0.9681, 0.9677, 0.9666, 0.9681, 0.9673, 0.9666, 0.9641, 0.9688, 0.9672, 0.9666, 0.965, 0.9675, 0.9672, 0.9673, 0.9672, 0.9666, 0.9679, 0.9686, 0.9659, 0.967, 0.9684, 0.9679, 0.9639, 0.9679, 0.9668, 0.9666, 0.9686, 0.9672, 0.9668, 0.9659, 0.9675, 0.967]\n",
      "\n",
      "0 input_12 False\n",
      "1 block1_conv1 True\n",
      "2 block1_conv2 True\n",
      "3 block1_pool True\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22048 samples, validate on 5510 samples\n",
      "Epoch 1/75\n",
      "22048/22048 [==============================] - 40s 2ms/step - loss: 2.6083 - accuracy: 0.6352 - val_loss: 0.3020 - val_accuracy: 0.8742\n",
      "Epoch 2/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.6969 - accuracy: 0.7591 - val_loss: 0.2046 - val_accuracy: 0.9254\n",
      "Epoch 3/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.4259 - accuracy: 0.8420 - val_loss: 0.1680 - val_accuracy: 0.9403\n",
      "Epoch 4/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.3048 - accuracy: 0.8893 - val_loss: 0.1565 - val_accuracy: 0.9494\n",
      "Epoch 5/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.2558 - accuracy: 0.9121 - val_loss: 0.1504 - val_accuracy: 0.9541\n",
      "Epoch 6/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.2209 - accuracy: 0.9277 - val_loss: 0.1459 - val_accuracy: 0.9550\n",
      "Epoch 7/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.2005 - accuracy: 0.9365 - val_loss: 0.1409 - val_accuracy: 0.9546\n",
      "Epoch 8/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1861 - accuracy: 0.9397 - val_loss: 0.1353 - val_accuracy: 0.9563\n",
      "Epoch 9/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1728 - accuracy: 0.9444 - val_loss: 0.1341 - val_accuracy: 0.9583\n",
      "Epoch 10/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1577 - accuracy: 0.9489 - val_loss: 0.1298 - val_accuracy: 0.9579\n",
      "Epoch 11/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1530 - accuracy: 0.9493 - val_loss: 0.1293 - val_accuracy: 0.9584\n",
      "Epoch 12/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1465 - accuracy: 0.9521 - val_loss: 0.1238 - val_accuracy: 0.9586\n",
      "Epoch 13/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1444 - accuracy: 0.9521 - val_loss: 0.1202 - val_accuracy: 0.9588\n",
      "Epoch 14/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1383 - accuracy: 0.9542 - val_loss: 0.1196 - val_accuracy: 0.9588\n",
      "Epoch 15/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1240 - accuracy: 0.9574 - val_loss: 0.1179 - val_accuracy: 0.9593\n",
      "Epoch 16/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1265 - accuracy: 0.9575 - val_loss: 0.1174 - val_accuracy: 0.9597\n",
      "Epoch 17/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1196 - accuracy: 0.9592 - val_loss: 0.1162 - val_accuracy: 0.9593\n",
      "Epoch 18/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1121 - accuracy: 0.9618 - val_loss: 0.1137 - val_accuracy: 0.9588\n",
      "Epoch 19/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1098 - accuracy: 0.9616 - val_loss: 0.1112 - val_accuracy: 0.9606\n",
      "Epoch 20/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1056 - accuracy: 0.9630 - val_loss: 0.1085 - val_accuracy: 0.9617\n",
      "Epoch 21/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.1036 - accuracy: 0.9636 - val_loss: 0.1096 - val_accuracy: 0.9624\n",
      "Epoch 22/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0996 - accuracy: 0.9634 - val_loss: 0.1090 - val_accuracy: 0.9615\n",
      "Epoch 23/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0972 - accuracy: 0.9655 - val_loss: 0.1093 - val_accuracy: 0.9621\n",
      "Epoch 24/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0933 - accuracy: 0.9666 - val_loss: 0.1075 - val_accuracy: 0.9632\n",
      "Epoch 25/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0882 - accuracy: 0.9686 - val_loss: 0.1043 - val_accuracy: 0.9632\n",
      "Epoch 26/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0857 - accuracy: 0.9697 - val_loss: 0.1041 - val_accuracy: 0.9630\n",
      "Epoch 27/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0816 - accuracy: 0.9707 - val_loss: 0.1056 - val_accuracy: 0.9637\n",
      "Epoch 28/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0762 - accuracy: 0.9720 - val_loss: 0.1050 - val_accuracy: 0.9624\n",
      "Epoch 29/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0749 - accuracy: 0.9720 - val_loss: 0.1101 - val_accuracy: 0.9637\n",
      "Epoch 30/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0720 - accuracy: 0.9730 - val_loss: 0.1088 - val_accuracy: 0.9650\n",
      "Epoch 31/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0681 - accuracy: 0.9752 - val_loss: 0.1100 - val_accuracy: 0.9646\n",
      "Epoch 32/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0666 - accuracy: 0.9750 - val_loss: 0.1091 - val_accuracy: 0.9652\n",
      "Epoch 33/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0618 - accuracy: 0.9766 - val_loss: 0.1087 - val_accuracy: 0.9642\n",
      "Epoch 34/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0602 - accuracy: 0.9780 - val_loss: 0.1185 - val_accuracy: 0.9650\n",
      "Epoch 35/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0592 - accuracy: 0.9779 - val_loss: 0.1106 - val_accuracy: 0.9653\n",
      "Epoch 36/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0552 - accuracy: 0.9791 - val_loss: 0.1132 - val_accuracy: 0.9652\n",
      "Epoch 37/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0527 - accuracy: 0.9804 - val_loss: 0.1200 - val_accuracy: 0.9648\n",
      "Epoch 38/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0500 - accuracy: 0.9819 - val_loss: 0.1122 - val_accuracy: 0.9648\n",
      "Epoch 39/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0464 - accuracy: 0.9834 - val_loss: 0.1246 - val_accuracy: 0.9653\n",
      "Epoch 40/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0461 - accuracy: 0.9823 - val_loss: 0.1176 - val_accuracy: 0.9661\n",
      "Epoch 41/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0436 - accuracy: 0.9839 - val_loss: 0.1241 - val_accuracy: 0.9639\n",
      "Epoch 42/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0409 - accuracy: 0.9844 - val_loss: 0.1213 - val_accuracy: 0.9655\n",
      "Epoch 43/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0385 - accuracy: 0.9858 - val_loss: 0.1240 - val_accuracy: 0.9668\n",
      "Epoch 44/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0365 - accuracy: 0.9862 - val_loss: 0.1292 - val_accuracy: 0.9655\n",
      "Epoch 45/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0345 - accuracy: 0.9867 - val_loss: 0.1311 - val_accuracy: 0.9650\n",
      "Epoch 46/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0330 - accuracy: 0.9881 - val_loss: 0.1409 - val_accuracy: 0.9661\n",
      "Epoch 47/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0305 - accuracy: 0.9893 - val_loss: 0.1418 - val_accuracy: 0.9652\n",
      "Epoch 48/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0280 - accuracy: 0.9898 - val_loss: 0.1501 - val_accuracy: 0.9653\n",
      "Epoch 49/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0276 - accuracy: 0.9898 - val_loss: 0.1442 - val_accuracy: 0.9655\n",
      "Epoch 50/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0248 - accuracy: 0.9910 - val_loss: 0.1438 - val_accuracy: 0.9657\n",
      "Epoch 51/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0238 - accuracy: 0.9907 - val_loss: 0.1519 - val_accuracy: 0.9637\n",
      "Epoch 52/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 0.1478 - val_accuracy: 0.9646\n",
      "Epoch 53/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0198 - accuracy: 0.9931 - val_loss: 0.1528 - val_accuracy: 0.9653\n",
      "Epoch 54/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.1617 - val_accuracy: 0.9646\n",
      "Epoch 55/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0175 - accuracy: 0.9938 - val_loss: 0.1826 - val_accuracy: 0.9655\n",
      "Epoch 56/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0156 - accuracy: 0.9951 - val_loss: 0.1707 - val_accuracy: 0.9653\n",
      "Epoch 57/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0158 - accuracy: 0.9943 - val_loss: 0.1700 - val_accuracy: 0.9659\n",
      "Epoch 58/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0137 - accuracy: 0.9952 - val_loss: 0.1798 - val_accuracy: 0.9661\n",
      "Epoch 59/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0120 - accuracy: 0.9965 - val_loss: 0.1874 - val_accuracy: 0.9668\n",
      "Epoch 60/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0116 - accuracy: 0.9961 - val_loss: 0.1879 - val_accuracy: 0.9659\n",
      "Epoch 61/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.2038 - val_accuracy: 0.9650\n",
      "Epoch 62/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.1944 - val_accuracy: 0.9652\n",
      "Epoch 63/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.2073 - val_accuracy: 0.9664\n",
      "Epoch 64/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.2246 - val_accuracy: 0.9652\n",
      "Epoch 65/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.2040 - val_accuracy: 0.9653\n",
      "Epoch 66/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.2209 - val_accuracy: 0.9650\n",
      "Epoch 67/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.2261 - val_accuracy: 0.9662\n",
      "Epoch 68/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.2208 - val_accuracy: 0.9657\n",
      "Epoch 69/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.2460 - val_accuracy: 0.9648\n",
      "Epoch 70/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.2383 - val_accuracy: 0.9646\n",
      "Epoch 71/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.2510 - val_accuracy: 0.9639\n",
      "Epoch 72/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.2560 - val_accuracy: 0.9655\n",
      "Epoch 73/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.2649 - val_accuracy: 0.9652\n",
      "Epoch 74/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.2668 - val_accuracy: 0.9646\n",
      "Epoch 75/75\n",
      "22048/22048 [==============================] - 39s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.2663 - val_accuracy: 0.9621\n",
      "Training Loss: [2.6083, 0.6969, 0.4259, 0.3048, 0.2558, 0.2209, 0.2005, 0.1861, 0.1728, 0.1577, 0.153, 0.1465, 0.1444, 0.1383, 0.124, 0.1265, 0.1196, 0.1121, 0.1098, 0.1056, 0.1036, 0.0996, 0.0972, 0.0933, 0.0882, 0.0857, 0.0816, 0.0762, 0.0749, 0.072, 0.0681, 0.0666, 0.0618, 0.0602, 0.0592, 0.0552, 0.0527, 0.05, 0.0464, 0.0461, 0.0436, 0.0409, 0.0385, 0.0365, 0.0345, 0.033, 0.0305, 0.028, 0.0276, 0.0248, 0.0238, 0.0226, 0.0198, 0.0188, 0.0175, 0.0156, 0.0158, 0.0137, 0.012, 0.0116, 0.0109, 0.0097, 0.0087, 0.0077, 0.0073, 0.007, 0.0059, 0.0046, 0.006, 0.0045, 0.0045, 0.0037, 0.0036, 0.0032, 0.004]\n",
      "Training Accuracy: [0.6352, 0.7591, 0.842, 0.8893, 0.9121, 0.9277, 0.9365, 0.9397, 0.9444, 0.9489, 0.9493, 0.9521, 0.9521, 0.9542, 0.9574, 0.9575, 0.9592, 0.9618, 0.9616, 0.963, 0.9636, 0.9634, 0.9655, 0.9666, 0.9686, 0.9697, 0.9707, 0.972, 0.972, 0.973, 0.9752, 0.975, 0.9766, 0.978, 0.9779, 0.9791, 0.9804, 0.9819, 0.9834, 0.9823, 0.9839, 0.9844, 0.9858, 0.9862, 0.9867, 0.9881, 0.9893, 0.9898, 0.9898, 0.991, 0.9907, 0.9927, 0.9931, 0.9937, 0.9938, 0.9951, 0.9943, 0.9952, 0.9965, 0.9961, 0.9965, 0.9971, 0.9976, 0.9977, 0.9976, 0.9979, 0.9982, 0.999, 0.9985, 0.9989, 0.9988, 0.9992, 0.9992, 0.9995, 0.9991]\n",
      "Validation Loss: [0.302, 0.2046, 0.168, 0.1565, 0.1504, 0.1459, 0.1409, 0.1353, 0.1341, 0.1298, 0.1293, 0.1238, 0.1202, 0.1196, 0.1179, 0.1174, 0.1162, 0.1137, 0.1112, 0.1085, 0.1096, 0.109, 0.1093, 0.1075, 0.1043, 0.1041, 0.1056, 0.105, 0.1101, 0.1088, 0.11, 0.1091, 0.1087, 0.1185, 0.1106, 0.1132, 0.12, 0.1122, 0.1246, 0.1176, 0.1241, 0.1213, 0.124, 0.1292, 0.1311, 0.1409, 0.1418, 0.1501, 0.1442, 0.1438, 0.1519, 0.1478, 0.1528, 0.1617, 0.1826, 0.1707, 0.17, 0.1798, 0.1874, 0.1879, 0.2038, 0.1944, 0.2073, 0.2246, 0.204, 0.2209, 0.2261, 0.2208, 0.246, 0.2383, 0.251, 0.256, 0.2649, 0.2668, 0.2663]\n",
      "Validation Accuracy: [0.8742, 0.9254, 0.9403, 0.9494, 0.9541, 0.955, 0.9546, 0.9563, 0.9583, 0.9579, 0.9584, 0.9586, 0.9588, 0.9588, 0.9593, 0.9597, 0.9593, 0.9588, 0.9606, 0.9617, 0.9624, 0.9615, 0.9621, 0.9632, 0.9632, 0.963, 0.9637, 0.9624, 0.9637, 0.965, 0.9646, 0.9652, 0.9642, 0.965, 0.9653, 0.9652, 0.9648, 0.9648, 0.9653, 0.9661, 0.9639, 0.9655, 0.9668, 0.9655, 0.965, 0.9661, 0.9652, 0.9653, 0.9655, 0.9657, 0.9637, 0.9646, 0.9653, 0.9646, 0.9655, 0.9653, 0.9659, 0.9661, 0.9668, 0.9659, 0.965, 0.9652, 0.9664, 0.9652, 0.9653, 0.965, 0.9662, 0.9657, 0.9648, 0.9646, 0.9639, 0.9655, 0.9652, 0.9646, 0.9621]\n",
      "\n",
      "0 input_13 False\n",
      "1 block1_conv1 True\n",
      "2 block1_conv2 True\n",
      "3 block1_pool True\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 2.4132 - accuracy: 0.6329 - val_loss: 0.3320 - val_accuracy: 0.8521\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.6442 - accuracy: 0.7644 - val_loss: 0.1930 - val_accuracy: 0.9334\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.3716 - accuracy: 0.8618 - val_loss: 0.1576 - val_accuracy: 0.9497\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.2785 - accuracy: 0.9023 - val_loss: 0.1461 - val_accuracy: 0.9559\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.2292 - accuracy: 0.9217 - val_loss: 0.1402 - val_accuracy: 0.9579\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.2009 - accuracy: 0.9332 - val_loss: 0.1353 - val_accuracy: 0.9603\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1856 - accuracy: 0.9379 - val_loss: 0.1302 - val_accuracy: 0.9606\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1742 - accuracy: 0.9423 - val_loss: 0.1295 - val_accuracy: 0.9614\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1602 - accuracy: 0.9472 - val_loss: 0.1248 - val_accuracy: 0.9624\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1502 - accuracy: 0.9506 - val_loss: 0.1225 - val_accuracy: 0.9626\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1418 - accuracy: 0.9532 - val_loss: 0.1183 - val_accuracy: 0.9634\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1424 - accuracy: 0.9531 - val_loss: 0.1186 - val_accuracy: 0.9630\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1373 - accuracy: 0.9536 - val_loss: 0.1161 - val_accuracy: 0.9646\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1275 - accuracy: 0.9555 - val_loss: 0.1141 - val_accuracy: 0.9644\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1243 - accuracy: 0.9577 - val_loss: 0.1127 - val_accuracy: 0.9648\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1221 - accuracy: 0.9585 - val_loss: 0.1103 - val_accuracy: 0.9650\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1153 - accuracy: 0.9604 - val_loss: 0.1098 - val_accuracy: 0.9659\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1121 - accuracy: 0.9614 - val_loss: 0.1069 - val_accuracy: 0.9664\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1085 - accuracy: 0.9621 - val_loss: 0.1049 - val_accuracy: 0.9661\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1015 - accuracy: 0.9641 - val_loss: 0.1038 - val_accuracy: 0.9659\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0982 - accuracy: 0.9652 - val_loss: 0.1031 - val_accuracy: 0.9663\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0939 - accuracy: 0.9674 - val_loss: 0.1066 - val_accuracy: 0.9672\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0919 - accuracy: 0.9675 - val_loss: 0.1018 - val_accuracy: 0.9675\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0883 - accuracy: 0.9682 - val_loss: 0.1056 - val_accuracy: 0.9681\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0845 - accuracy: 0.9686 - val_loss: 0.1027 - val_accuracy: 0.9677\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0813 - accuracy: 0.9717 - val_loss: 0.1002 - val_accuracy: 0.9688\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0787 - accuracy: 0.9714 - val_loss: 0.1015 - val_accuracy: 0.9686\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0737 - accuracy: 0.9729 - val_loss: 0.1054 - val_accuracy: 0.9675\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0737 - accuracy: 0.9729 - val_loss: 0.0995 - val_accuracy: 0.9683\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0696 - accuracy: 0.9746 - val_loss: 0.1025 - val_accuracy: 0.9677\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0665 - accuracy: 0.9761 - val_loss: 0.1025 - val_accuracy: 0.9672\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0657 - accuracy: 0.9763 - val_loss: 0.1026 - val_accuracy: 0.9673\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0594 - accuracy: 0.9793 - val_loss: 0.1035 - val_accuracy: 0.9681\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0560 - accuracy: 0.9803 - val_loss: 0.1116 - val_accuracy: 0.9675\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0557 - accuracy: 0.9802 - val_loss: 0.1055 - val_accuracy: 0.9683\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0544 - accuracy: 0.9806 - val_loss: 0.1077 - val_accuracy: 0.9688\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0500 - accuracy: 0.9830 - val_loss: 0.1103 - val_accuracy: 0.9672\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0470 - accuracy: 0.9845 - val_loss: 0.1093 - val_accuracy: 0.9688\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0455 - accuracy: 0.9846 - val_loss: 0.1043 - val_accuracy: 0.9677\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 0.1198 - val_accuracy: 0.9683\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0396 - accuracy: 0.9872 - val_loss: 0.1121 - val_accuracy: 0.9686\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0389 - accuracy: 0.9859 - val_loss: 0.1108 - val_accuracy: 0.9673\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0346 - accuracy: 0.9879 - val_loss: 0.1204 - val_accuracy: 0.9675\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0346 - accuracy: 0.9881 - val_loss: 0.1194 - val_accuracy: 0.9684s - loss: 0.0\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0325 - accuracy: 0.9885 - val_loss: 0.1238 - val_accuracy: 0.9686\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0299 - accuracy: 0.9896 - val_loss: 0.1251 - val_accuracy: 0.9693\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0287 - accuracy: 0.9911 - val_loss: 0.1197 - val_accuracy: 0.9681\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0271 - accuracy: 0.9907 - val_loss: 0.1244 - val_accuracy: 0.9695\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0242 - accuracy: 0.9927 - val_loss: 0.1321 - val_accuracy: 0.9659\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.1356 - val_accuracy: 0.9679\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.1407 - val_accuracy: 0.9679\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0198 - accuracy: 0.9933 - val_loss: 0.1304 - val_accuracy: 0.9684\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0188 - accuracy: 0.9936 - val_loss: 0.1451 - val_accuracy: 0.9677\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0162 - accuracy: 0.9952 - val_loss: 0.1400 - val_accuracy: 0.9693\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.1475 - val_accuracy: 0.9673\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0143 - accuracy: 0.9958 - val_loss: 0.1495 - val_accuracy: 0.9675\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 0.1524 - val_accuracy: 0.9677\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.1552 - val_accuracy: 0.9684\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.1626 - val_accuracy: 0.9668\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.1689 - val_accuracy: 0.9668\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.1729 - val_accuracy: 0.9677\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.1689 - val_accuracy: 0.9683\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1866 - val_accuracy: 0.9684\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.1886 - val_accuracy: 0.9686\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.1809 - val_accuracy: 0.9679\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.1817 - val_accuracy: 0.9681\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.2012 - val_accuracy: 0.9693\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.2068 - val_accuracy: 0.9664\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2138 - val_accuracy: 0.9670\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.2101 - val_accuracy: 0.9677\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.2166 - val_accuracy: 0.9675\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2070 - val_accuracy: 0.9672\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.2441 - val_accuracy: 0.9686\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.2168 - val_accuracy: 0.9673\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.2304 - val_accuracy: 0.9675\n",
      "Training Loss: [2.4132, 0.6442, 0.3716, 0.2785, 0.2292, 0.2009, 0.1856, 0.1742, 0.1602, 0.1502, 0.1418, 0.1424, 0.1373, 0.1275, 0.1243, 0.1221, 0.1153, 0.1121, 0.1085, 0.1015, 0.0982, 0.0939, 0.0919, 0.0883, 0.0845, 0.0813, 0.0787, 0.0737, 0.0737, 0.0696, 0.0665, 0.0657, 0.0594, 0.056, 0.0557, 0.0544, 0.05, 0.047, 0.0455, 0.0433, 0.0396, 0.0389, 0.0346, 0.0346, 0.0325, 0.0299, 0.0287, 0.0271, 0.0242, 0.0237, 0.0196, 0.0198, 0.0188, 0.0162, 0.0155, 0.0143, 0.013, 0.0122, 0.0103, 0.0093, 0.0092, 0.0081, 0.0077, 0.0065, 0.0061, 0.0049, 0.0049, 0.0042, 0.0039, 0.0038, 0.003, 0.0029, 0.0022, 0.0026, 0.0018]\n",
      "Training Accuracy: [0.6329, 0.7644, 0.8618, 0.9023, 0.9217, 0.9332, 0.9379, 0.9423, 0.9472, 0.9506, 0.9532, 0.9531, 0.9536, 0.9555, 0.9577, 0.9585, 0.9604, 0.9614, 0.9621, 0.9641, 0.9652, 0.9674, 0.9675, 0.9682, 0.9686, 0.9717, 0.9714, 0.9729, 0.9729, 0.9746, 0.9761, 0.9763, 0.9793, 0.9803, 0.9802, 0.9806, 0.983, 0.9845, 0.9846, 0.9852, 0.9872, 0.9859, 0.9879, 0.9881, 0.9885, 0.9896, 0.9911, 0.9907, 0.9927, 0.9925, 0.9937, 0.9933, 0.9936, 0.9952, 0.9952, 0.9958, 0.9959, 0.9962, 0.9969, 0.9972, 0.9972, 0.998, 0.9977, 0.9982, 0.9983, 0.9987, 0.9988, 0.9989, 0.9993, 0.9989, 0.9993, 0.9995, 0.9997, 0.9993, 0.9998]\n",
      "Validation Loss: [0.332, 0.193, 0.1576, 0.1461, 0.1402, 0.1353, 0.1302, 0.1295, 0.1248, 0.1225, 0.1183, 0.1186, 0.1161, 0.1141, 0.1127, 0.1103, 0.1098, 0.1069, 0.1049, 0.1038, 0.1031, 0.1066, 0.1018, 0.1056, 0.1027, 0.1002, 0.1015, 0.1054, 0.0995, 0.1025, 0.1025, 0.1026, 0.1035, 0.1116, 0.1055, 0.1077, 0.1103, 0.1093, 0.1043, 0.1198, 0.1121, 0.1108, 0.1204, 0.1194, 0.1238, 0.1251, 0.1197, 0.1244, 0.1321, 0.1356, 0.1407, 0.1304, 0.1451, 0.14, 0.1475, 0.1495, 0.1524, 0.1552, 0.1626, 0.1689, 0.1729, 0.1689, 0.1866, 0.1886, 0.1809, 0.1817, 0.2012, 0.2068, 0.2138, 0.2101, 0.2166, 0.207, 0.2441, 0.2168, 0.2304]\n",
      "Validation Accuracy: [0.8521, 0.9334, 0.9497, 0.9559, 0.9579, 0.9603, 0.9606, 0.9614, 0.9624, 0.9626, 0.9634, 0.963, 0.9646, 0.9644, 0.9648, 0.965, 0.9659, 0.9664, 0.9661, 0.9659, 0.9663, 0.9672, 0.9675, 0.9681, 0.9677, 0.9688, 0.9686, 0.9675, 0.9683, 0.9677, 0.9672, 0.9673, 0.9681, 0.9675, 0.9683, 0.9688, 0.9672, 0.9688, 0.9677, 0.9683, 0.9686, 0.9673, 0.9675, 0.9684, 0.9686, 0.9693, 0.9681, 0.9695, 0.9659, 0.9679, 0.9679, 0.9684, 0.9677, 0.9693, 0.9673, 0.9675, 0.9677, 0.9684, 0.9668, 0.9668, 0.9677, 0.9683, 0.9684, 0.9686, 0.9679, 0.9681, 0.9693, 0.9664, 0.967, 0.9677, 0.9675, 0.9672, 0.9686, 0.9673, 0.9675]\n",
      "\n",
      "0 input_14 False\n",
      "1 block1_conv1 True\n",
      "2 block1_conv2 True\n",
      "3 block1_pool True\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 40s 2ms/step - loss: 2.5319 - accuracy: 0.6453 - val_loss: 0.3377 - val_accuracy: 0.8619\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.6403 - accuracy: 0.7757 - val_loss: 0.2051 - val_accuracy: 0.9218\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.3785 - accuracy: 0.8541 - val_loss: 0.1664 - val_accuracy: 0.9421\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.2798 - accuracy: 0.8976 - val_loss: 0.1522 - val_accuracy: 0.9470\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.2292 - accuracy: 0.9210 - val_loss: 0.1467 - val_accuracy: 0.9490\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1982 - accuracy: 0.9332 - val_loss: 0.1440 - val_accuracy: 0.9514\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1817 - accuracy: 0.9426 - val_loss: 0.1414 - val_accuracy: 0.9514\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1713 - accuracy: 0.9453 - val_loss: 0.1396 - val_accuracy: 0.9523\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1613 - accuracy: 0.9477 - val_loss: 0.1356 - val_accuracy: 0.9530\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1512 - accuracy: 0.9525 - val_loss: 0.1373 - val_accuracy: 0.9525\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1420 - accuracy: 0.9546 - val_loss: 0.1354 - val_accuracy: 0.9534\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1376 - accuracy: 0.9554 - val_loss: 0.1326 - val_accuracy: 0.9534\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1346 - accuracy: 0.9555 - val_loss: 0.1308 - val_accuracy: 0.9543\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1254 - accuracy: 0.9576 - val_loss: 0.1285 - val_accuracy: 0.9541\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1197 - accuracy: 0.9599 - val_loss: 0.1284 - val_accuracy: 0.9548\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1144 - accuracy: 0.9601 - val_loss: 0.1281 - val_accuracy: 0.9550\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1132 - accuracy: 0.9614 - val_loss: 0.1259 - val_accuracy: 0.9545\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1072 - accuracy: 0.9636 - val_loss: 0.1252 - val_accuracy: 0.9554\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.1051 - accuracy: 0.9639 - val_loss: 0.1214 - val_accuracy: 0.9559\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0981 - accuracy: 0.9660 - val_loss: 0.1252 - val_accuracy: 0.9572\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0950 - accuracy: 0.9666 - val_loss: 0.1269 - val_accuracy: 0.9563\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0927 - accuracy: 0.9679 - val_loss: 0.1216 - val_accuracy: 0.9575\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0880 - accuracy: 0.9697 - val_loss: 0.1255 - val_accuracy: 0.9585\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0849 - accuracy: 0.9700 - val_loss: 0.1236 - val_accuracy: 0.9586\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0807 - accuracy: 0.9712 - val_loss: 0.1241 - val_accuracy: 0.9581\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0791 - accuracy: 0.9726 - val_loss: 0.1203 - val_accuracy: 0.9592\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0728 - accuracy: 0.9747 - val_loss: 0.1230 - val_accuracy: 0.9586\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0714 - accuracy: 0.9758 - val_loss: 0.1221 - val_accuracy: 0.9595\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0689 - accuracy: 0.9754 - val_loss: 0.1256 - val_accuracy: 0.9588\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0656 - accuracy: 0.9760 - val_loss: 0.1211 - val_accuracy: 0.9595\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0622 - accuracy: 0.9783 - val_loss: 0.1269 - val_accuracy: 0.9585\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0589 - accuracy: 0.9797 - val_loss: 0.1279 - val_accuracy: 0.9601\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0555 - accuracy: 0.9801 - val_loss: 0.1269 - val_accuracy: 0.9603\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0546 - accuracy: 0.9810 - val_loss: 0.1271 - val_accuracy: 0.9601\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0529 - accuracy: 0.9812 - val_loss: 0.1262 - val_accuracy: 0.9604\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0484 - accuracy: 0.9834 - val_loss: 0.1389 - val_accuracy: 0.9604\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0441 - accuracy: 0.9846 - val_loss: 0.1337 - val_accuracy: 0.9608\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0442 - accuracy: 0.9847 - val_loss: 0.1457 - val_accuracy: 0.9617\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0418 - accuracy: 0.9857 - val_loss: 0.1383 - val_accuracy: 0.9621\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0396 - accuracy: 0.9852 - val_loss: 0.1456 - val_accuracy: 0.9623\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0365 - accuracy: 0.9871 - val_loss: 0.1471 - val_accuracy: 0.9615\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0357 - accuracy: 0.9881 - val_loss: 0.1515 - val_accuracy: 0.9610\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0316 - accuracy: 0.9892 - val_loss: 0.1550 - val_accuracy: 0.9617\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0305 - accuracy: 0.9888 - val_loss: 0.1548 - val_accuracy: 0.9617\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0297 - accuracy: 0.9894 - val_loss: 0.1605 - val_accuracy: 0.9610\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0274 - accuracy: 0.9898 - val_loss: 0.1570 - val_accuracy: 0.9624\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0243 - accuracy: 0.9915 - val_loss: 0.1616 - val_accuracy: 0.9624\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.1763 - val_accuracy: 0.9617\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.1655 - val_accuracy: 0.9614\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0206 - accuracy: 0.9927 - val_loss: 0.1818 - val_accuracy: 0.9619\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0190 - accuracy: 0.9938 - val_loss: 0.1865 - val_accuracy: 0.9614\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0180 - accuracy: 0.9936 - val_loss: 0.1806 - val_accuracy: 0.9621\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.1910 - val_accuracy: 0.9621\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0142 - accuracy: 0.9957 - val_loss: 0.2054 - val_accuracy: 0.9615\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.1959 - val_accuracy: 0.9630\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0121 - accuracy: 0.9960 - val_loss: 0.2004 - val_accuracy: 0.9617\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.2119 - val_accuracy: 0.9626\n",
      "Epoch 58/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0104 - accuracy: 0.9974 - val_loss: 0.2164 - val_accuracy: 0.9621\n",
      "Epoch 59/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.2133 - val_accuracy: 0.9612\n",
      "Epoch 60/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.2322 - val_accuracy: 0.9615\n",
      "Epoch 61/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.2379 - val_accuracy: 0.9606\n",
      "Epoch 62/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.2488 - val_accuracy: 0.9624\n",
      "Epoch 63/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.2445 - val_accuracy: 0.9621\n",
      "Epoch 64/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.2433 - val_accuracy: 0.9624\n",
      "Epoch 65/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.2533 - val_accuracy: 0.9619\n",
      "Epoch 66/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2601 - val_accuracy: 0.9628\n",
      "Epoch 67/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.2690 - val_accuracy: 0.9621\n",
      "Epoch 68/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.2741 - val_accuracy: 0.9635\n",
      "Epoch 69/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.2602 - val_accuracy: 0.9612\n",
      "Epoch 70/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.2861 - val_accuracy: 0.9610\n",
      "Epoch 71/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.2862 - val_accuracy: 0.9612\n",
      "Epoch 72/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.3049 - val_accuracy: 0.9634\n",
      "Epoch 73/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.2996 - val_accuracy: 0.9621\n",
      "Epoch 74/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.3042 - val_accuracy: 0.9617\n",
      "Epoch 75/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.3280 - val_accuracy: 0.9621\n",
      "Training Loss: [2.5319, 0.6403, 0.3785, 0.2798, 0.2292, 0.1982, 0.1817, 0.1713, 0.1613, 0.1512, 0.142, 0.1376, 0.1346, 0.1254, 0.1197, 0.1144, 0.1132, 0.1072, 0.1051, 0.0981, 0.095, 0.0927, 0.088, 0.0849, 0.0807, 0.0791, 0.0728, 0.0714, 0.0689, 0.0656, 0.0622, 0.0589, 0.0555, 0.0546, 0.0529, 0.0484, 0.0441, 0.0442, 0.0418, 0.0396, 0.0365, 0.0357, 0.0316, 0.0305, 0.0297, 0.0274, 0.0243, 0.023, 0.0227, 0.0206, 0.019, 0.018, 0.0154, 0.0142, 0.0129, 0.0121, 0.0111, 0.0104, 0.0092, 0.0081, 0.0077, 0.0068, 0.0055, 0.0058, 0.0054, 0.0039, 0.0038, 0.005, 0.0033, 0.0031, 0.0028, 0.0032, 0.0024, 0.0023, 0.002]\n",
      "Training Accuracy: [0.6453, 0.7757, 0.8541, 0.8976, 0.921, 0.9332, 0.9426, 0.9453, 0.9477, 0.9525, 0.9546, 0.9554, 0.9555, 0.9576, 0.9599, 0.9601, 0.9614, 0.9636, 0.9639, 0.966, 0.9666, 0.9679, 0.9697, 0.97, 0.9712, 0.9726, 0.9747, 0.9758, 0.9754, 0.976, 0.9783, 0.9797, 0.9801, 0.981, 0.9812, 0.9834, 0.9846, 0.9847, 0.9857, 0.9852, 0.9871, 0.9881, 0.9892, 0.9888, 0.9894, 0.9898, 0.9915, 0.9927, 0.9922, 0.9927, 0.9938, 0.9936, 0.9949, 0.9957, 0.9958, 0.996, 0.997, 0.9974, 0.9973, 0.9977, 0.9978, 0.998, 0.9986, 0.9985, 0.9985, 0.9993, 0.9991, 0.9986, 0.9992, 0.9993, 0.9993, 0.9992, 0.9996, 0.9996, 0.9997]\n",
      "Validation Loss: [0.3377, 0.2051, 0.1664, 0.1522, 0.1467, 0.144, 0.1414, 0.1396, 0.1356, 0.1373, 0.1354, 0.1326, 0.1308, 0.1285, 0.1284, 0.1281, 0.1259, 0.1252, 0.1214, 0.1252, 0.1269, 0.1216, 0.1255, 0.1236, 0.1241, 0.1203, 0.123, 0.1221, 0.1256, 0.1211, 0.1269, 0.1279, 0.1269, 0.1271, 0.1262, 0.1389, 0.1337, 0.1457, 0.1383, 0.1456, 0.1471, 0.1515, 0.155, 0.1548, 0.1605, 0.157, 0.1616, 0.1763, 0.1655, 0.1818, 0.1865, 0.1806, 0.191, 0.2054, 0.1959, 0.2004, 0.2119, 0.2164, 0.2133, 0.2322, 0.2379, 0.2488, 0.2445, 0.2433, 0.2533, 0.2601, 0.269, 0.2741, 0.2602, 0.2861, 0.2862, 0.3049, 0.2996, 0.3042, 0.328]\n",
      "Validation Accuracy: [0.8619, 0.9218, 0.9421, 0.947, 0.949, 0.9514, 0.9514, 0.9523, 0.953, 0.9525, 0.9534, 0.9534, 0.9543, 0.9541, 0.9548, 0.955, 0.9545, 0.9554, 0.9559, 0.9572, 0.9563, 0.9575, 0.9585, 0.9586, 0.9581, 0.9592, 0.9586, 0.9595, 0.9588, 0.9595, 0.9585, 0.9601, 0.9603, 0.9601, 0.9604, 0.9604, 0.9608, 0.9617, 0.9621, 0.9623, 0.9615, 0.961, 0.9617, 0.9617, 0.961, 0.9624, 0.9624, 0.9617, 0.9614, 0.9619, 0.9614, 0.9621, 0.9621, 0.9615, 0.963, 0.9617, 0.9626, 0.9621, 0.9612, 0.9615, 0.9606, 0.9624, 0.9621, 0.9624, 0.9619, 0.9628, 0.9621, 0.9635, 0.9612, 0.961, 0.9612, 0.9634, 0.9621, 0.9617, 0.9621]\n",
      "\n",
      "0 input_15 False\n",
      "1 block1_conv1 True\n",
      "2 block1_conv2 True\n",
      "3 block1_pool True\n",
      "4 block2_conv1 True\n",
      "5 block2_conv2 True\n",
      "6 block2_pool True\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "We are now training cross-validation set # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22046 samples, validate on 5512 samples\n",
      "Epoch 1/75\n",
      "22046/22046 [==============================] - 39s 2ms/step - loss: 2.6565 - accuracy: 0.5969 - val_loss: 0.4135 - val_accuracy: 0.8066\n",
      "Epoch 2/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.7165 - accuracy: 0.7102 - val_loss: 0.2718 - val_accuracy: 0.8953\n",
      "Epoch 3/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.4422 - accuracy: 0.8175 - val_loss: 0.1800 - val_accuracy: 0.9396\n",
      "Epoch 4/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.3162 - accuracy: 0.8784 - val_loss: 0.1489 - val_accuracy: 0.9526\n",
      "Epoch 5/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.2503 - accuracy: 0.9101 - val_loss: 0.1381 - val_accuracy: 0.9554\n",
      "Epoch 6/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.2132 - accuracy: 0.9266 - val_loss: 0.1354 - val_accuracy: 0.9575\n",
      "Epoch 7/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1932 - accuracy: 0.9358 - val_loss: 0.1294 - val_accuracy: 0.9581\n",
      "Epoch 8/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1737 - accuracy: 0.9424 - val_loss: 0.1280 - val_accuracy: 0.9595\n",
      "Epoch 9/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1644 - accuracy: 0.9452 - val_loss: 0.1261 - val_accuracy: 0.9604\n",
      "Epoch 10/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1538 - accuracy: 0.9503 - val_loss: 0.1248 - val_accuracy: 0.9603\n",
      "Epoch 11/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1455 - accuracy: 0.9525 - val_loss: 0.1229 - val_accuracy: 0.9608\n",
      "Epoch 12/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1399 - accuracy: 0.9552 - val_loss: 0.1208 - val_accuracy: 0.9617\n",
      "Epoch 13/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1354 - accuracy: 0.9544 - val_loss: 0.1180 - val_accuracy: 0.9610\n",
      "Epoch 14/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1290 - accuracy: 0.9569 - val_loss: 0.1152 - val_accuracy: 0.9612\n",
      "Epoch 15/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1244 - accuracy: 0.9594 - val_loss: 0.1142 - val_accuracy: 0.9614\n",
      "Epoch 16/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1184 - accuracy: 0.9596 - val_loss: 0.1141 - val_accuracy: 0.9619\n",
      "Epoch 17/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1148 - accuracy: 0.9608 - val_loss: 0.1122 - val_accuracy: 0.9626\n",
      "Epoch 18/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1107 - accuracy: 0.9619 - val_loss: 0.1112 - val_accuracy: 0.9624\n",
      "Epoch 19/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1051 - accuracy: 0.9625 - val_loss: 0.1127 - val_accuracy: 0.9626\n",
      "Epoch 20/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.1021 - accuracy: 0.9642 - val_loss: 0.1111 - val_accuracy: 0.9628\n",
      "Epoch 21/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0982 - accuracy: 0.9659 - val_loss: 0.1069 - val_accuracy: 0.9639\n",
      "Epoch 22/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0954 - accuracy: 0.9670 - val_loss: 0.1072 - val_accuracy: 0.9643\n",
      "Epoch 23/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0913 - accuracy: 0.9680 - val_loss: 0.1081 - val_accuracy: 0.9648\n",
      "Epoch 24/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0860 - accuracy: 0.9702 - val_loss: 0.1077 - val_accuracy: 0.9635\n",
      "Epoch 25/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0834 - accuracy: 0.9702 - val_loss: 0.1102 - val_accuracy: 0.9650\n",
      "Epoch 26/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0791 - accuracy: 0.9721 - val_loss: 0.1070 - val_accuracy: 0.9644\n",
      "Epoch 27/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0748 - accuracy: 0.9729 - val_loss: 0.1090 - val_accuracy: 0.9632\n",
      "Epoch 28/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0726 - accuracy: 0.9744 - val_loss: 0.1090 - val_accuracy: 0.9659\n",
      "Epoch 29/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0692 - accuracy: 0.9753 - val_loss: 0.1099 - val_accuracy: 0.9666\n",
      "Epoch 30/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0667 - accuracy: 0.9762 - val_loss: 0.1109 - val_accuracy: 0.9644\n",
      "Epoch 31/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0637 - accuracy: 0.9777 - val_loss: 0.1103 - val_accuracy: 0.9653\n",
      "Epoch 32/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0597 - accuracy: 0.9785 - val_loss: 0.1106 - val_accuracy: 0.9657\n",
      "Epoch 33/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0584 - accuracy: 0.9788 - val_loss: 0.1129 - val_accuracy: 0.9666\n",
      "Epoch 34/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0536 - accuracy: 0.9809 - val_loss: 0.1167 - val_accuracy: 0.9663\n",
      "Epoch 35/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0507 - accuracy: 0.9812 - val_loss: 0.1197 - val_accuracy: 0.9650\n",
      "Epoch 36/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0481 - accuracy: 0.9833 - val_loss: 0.1201 - val_accuracy: 0.9664\n",
      "Epoch 37/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0466 - accuracy: 0.9829 - val_loss: 0.1172 - val_accuracy: 0.9666\n",
      "Epoch 38/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0436 - accuracy: 0.9845 - val_loss: 0.1202 - val_accuracy: 0.9652\n",
      "Epoch 39/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0401 - accuracy: 0.9858 - val_loss: 0.1240 - val_accuracy: 0.9663\n",
      "Epoch 40/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0383 - accuracy: 0.9867 - val_loss: 0.1288 - val_accuracy: 0.9666\n",
      "Epoch 41/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0372 - accuracy: 0.9868 - val_loss: 0.1258 - val_accuracy: 0.9657\n",
      "Epoch 42/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0338 - accuracy: 0.9876 - val_loss: 0.1235 - val_accuracy: 0.9663\n",
      "Epoch 43/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0325 - accuracy: 0.9890 - val_loss: 0.1312 - val_accuracy: 0.9644\n",
      "Epoch 44/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0288 - accuracy: 0.9906 - val_loss: 0.1324 - val_accuracy: 0.9646\n",
      "Epoch 45/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0268 - accuracy: 0.9912 - val_loss: 0.1433 - val_accuracy: 0.9663\n",
      "Epoch 46/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0253 - accuracy: 0.9916 - val_loss: 0.1503 - val_accuracy: 0.9650\n",
      "Epoch 47/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.1447 - val_accuracy: 0.9659\n",
      "Epoch 48/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0226 - accuracy: 0.9920 - val_loss: 0.1466 - val_accuracy: 0.9659\n",
      "Epoch 49/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0196 - accuracy: 0.9940 - val_loss: 0.1539 - val_accuracy: 0.9652\n",
      "Epoch 50/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0183 - accuracy: 0.9943 - val_loss: 0.1633 - val_accuracy: 0.9666\n",
      "Epoch 51/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0179 - accuracy: 0.9942 - val_loss: 0.1624 - val_accuracy: 0.9664\n",
      "Epoch 52/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0149 - accuracy: 0.9952 - val_loss: 0.1669 - val_accuracy: 0.9663\n",
      "Epoch 53/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0127 - accuracy: 0.9966 - val_loss: 0.1739 - val_accuracy: 0.9652\n",
      "Epoch 54/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.1740 - val_accuracy: 0.9663\n",
      "Epoch 55/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.1880 - val_accuracy: 0.9650\n",
      "Epoch 56/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.1902 - val_accuracy: 0.9655\n",
      "Epoch 57/75\n",
      "22046/22046 [==============================] - 38s 2ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.1895 - val_accuracy: 0.9652\n",
      "Epoch 58/75\n",
      "15616/22046 [====================>.........] - ETA: 10s - loss: 0.0079 - accuracy: 0.9974- ETA: 17s - lo"
     ]
    }
   ],
   "source": [
    "# Import relevant packages for neural network training\n",
    "import sys\n",
    "import csv\n",
    "if 'tensorflow' in sys.modules == False:\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Import relevant neural network architecture packages \n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# Specify layers to freeze\n",
    "FreezeLayers = [19,15,11,7,4,0]\n",
    "\n",
    "for j in FreezeLayers:\n",
    "    \n",
    "    # Create empty lists to store results\n",
    "    TrainLoss = []\n",
    "    TrainAcc = []\n",
    "    TestLoss = []\n",
    "    TestAcc = []\n",
    "    All_FPR = []\n",
    "    All_TPR = []\n",
    "    All_thresholds = []\n",
    "    All_AUC = []\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        # Create the appropriate training and testing sets\n",
    "        if i == 0:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index4,:]), axis=0)\n",
    "            TestImages = Dataset[Index5,:]\n",
    "            TestLabels = Labels[Index5,:]\n",
    "        elif i == 1:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index4,:]\n",
    "            TestLabels = Labels[Index4,:]\n",
    "        elif i == 2:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index3,:]\n",
    "            TestLabels = Labels[Index3,:]\n",
    "        elif i == 3:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index2,:]\n",
    "            TestLabels = Labels[Index2,:]\n",
    "        else:\n",
    "            TrainImages = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index2,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index1,:]\n",
    "            TestLabels = Labels[Index1,:]\n",
    "\n",
    "        base_model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (128,128,3))\n",
    "\n",
    "        for layer in base_model.layers[:j]:\n",
    "            layer.trainable=False\n",
    "        for k,layer in enumerate(base_model.layers):\n",
    "            print(k,layer.name,layer.trainable)\n",
    "\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(1024, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(1024, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model(input = base_model.input, output = predictions)\n",
    "        adam = optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss = \"categorical_crossentropy\", optimizer = adam, metrics=[\"accuracy\"])\n",
    "\n",
    "        # Train model and evaluate performance\n",
    "        print('We are now training cross-validation set #',i+1)\n",
    "        Results = model.fit(TrainImages, TrainLabels, epochs=75, batch_size=64, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
    "\n",
    "        # Display and store performance results\n",
    "        Results.history['loss'] = [round(k, 4) for k in Results.history['loss']]\n",
    "        Results.history['accuracy'] = [round(k, 4) for k in Results.history['accuracy']]\n",
    "        Results.history['val_loss'] = [round(k, 4) for k in Results.history['val_loss']]\n",
    "        Results.history['val_accuracy'] = [round(k, 4) for k in Results.history['val_accuracy']]\n",
    "\n",
    "        print('Training Loss:',Results.history['loss'])\n",
    "        print('Training Accuracy:',Results.history['accuracy'])\n",
    "        print('Validation Loss:',Results.history['val_loss'])\n",
    "        print('Validation Accuracy:',Results.history['val_accuracy'])\n",
    "\n",
    "        TrainLoss.append(Results.history['loss'])\n",
    "        TrainAcc.append(Results.history['accuracy'])\n",
    "        TestLoss.append(Results.history['val_loss'])\n",
    "        TestAcc.append(Results.history['val_accuracy'])\n",
    "        print('')\n",
    "\n",
    "\n",
    "        # Predict values for test set\n",
    "        Probabilities = model.predict(TestImages)\n",
    "\n",
    "        # Calculate data for ROC curve\n",
    "        FPR, TPR, thresholds = roc_curve(TestLabels[:,1], Probabilities[:,1])\n",
    "        All_FPR.append(FPR)\n",
    "        All_TPR.append(TPR)\n",
    "        All_thresholds.append(thresholds)\n",
    "\n",
    "        # Save and export as CSV files\n",
    "        with open(str(j)+\"Freeze_TrainLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TrainLoss)\n",
    "        with open(str(j)+\"Freeze_TrainAcc.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TrainAcc)\n",
    "        with open(str(j)+\"Freeze__TestLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TestLoss)\n",
    "        with open(str(j)+\"Freeze__TestAcc.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TestAcc)\n",
    "        with open(str(j)+\"Freeze__FPR.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(All_FPR)\n",
    "        with open(str(j)+\"Freeze__TPR.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(All_TPR)\n",
    "        with open(str(j)+\"Freeze__Thresholds.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(All_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine RAM Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine RAM Usage\n",
    "import sys\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
