{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NIH Dataset (ZIP Format)\n",
    "Here we import the NIH dataset (zip file format) from a website housed by the NIH National Library of Medicine (NLM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Download NIH dataset zip file\n",
    "!wget -nc ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
    "\n",
    "# Extract images if not already extracted\n",
    "ROOT_DIR = os.path.join(\"/\", \"content\")\n",
    "if not os.path.isdir(\"cell_images\"):\n",
    "    print(\"Extracting images...\")\n",
    "    with ZipFile(os.path.join(\"cell_images.zip\"), \"r\") as zipObj:\n",
    "        zipObj.extractall()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Images, Resize, and Store in NumPy Arrays\n",
    "We load the 10000 images (5000 each class) from the zip file into two different folders. There are 13779 images in each class, with a \"Thumbs.db\" file located in each folder, which we remove. We only use 5000 images in each class and resize each individual image into 128x128 pixels, while maintaining the 3 RGB channels, and store them into the NumPy arrays ```Parasitized``` and ```Uninfected```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "!pip install opencv-python\n",
    "!apt update && apt install -y libsm6 libxext6 libxrender1\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Create new folders to save rescaled images\n",
    "if not os.path.isdir(\"RescaledSet\"):\n",
    "    os.mkdir(\"RescaledSet\")\n",
    "if not os.path.isdir(\"RescaledSet/Parasitized\"):\n",
    "    os.mkdir(\"RescaledSet/Parasitized\")\n",
    "if not os.path.isdir(\"RescaledSet/Uninfected\"):\n",
    "    os.mkdir(\"RescaledSet/Uninfected\")\n",
    "\n",
    "# Generate list of parasitized file names\n",
    "ParasitizedFiles = os.listdir(\"cell_images/Parasitized/\")\n",
    "UninfectedFiles = os.listdir(\"cell_images/Uninfected/\")\n",
    "\n",
    "# Remove Thumb.db files\n",
    "while 'Thumbs.db' in ParasitizedFiles: ParasitizedFiles.remove('Thumbs.db')   \n",
    "while 'Thumbs.db' in UninfectedFiles: UninfectedFiles.remove('Thumbs.db')  \n",
    "\n",
    "# Pre-allocate memory space for images\n",
    "Parasitized = np.empty([5000,128,128,3])\n",
    "Uninfected = np.empty([5000,128,128,3])\n",
    "\n",
    "# Resize and load parasitized images\n",
    "for i in range(5000):\n",
    "    TempImage = cv2.imread('cell_images/Parasitized/'+ParasitizedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Parasitized[i,:,:,:] = ResizedImage\n",
    "\n",
    "# Resize and load uninfected images\n",
    "for i in range(5000):\n",
    "    TempImage = cv2.imread('cell_images/Uninfected/'+UninfectedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Uninfected[i,:,:,:] = ResizedImage\n",
    "    \n",
    "print('Uninfected Dataset size is:',np.shape(Uninfected))\n",
    "print('Parasitized Dataset size is:',np.shape(Parasitized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cross-Validation Indices for Training and Testing Sets\n",
    "Here we randomly generate five cross-validation group indices to access the images in the dataset.## Generate Cross-Validation Indices for Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset labels\n",
    "ParasitizedLabels = np.repeat([[0,1]], 5000, axis=0)\n",
    "UninfectedLabels = np.repeat([[1,0]], 5000, axis=0)\n",
    "Labels = np.concatenate((ParasitizedLabels,UninfectedLabels), axis=0)\n",
    "\n",
    "# Generate image dataset\n",
    "Dataset = np.concatenate((Parasitized, Uninfected), axis=0)\n",
    "\n",
    "# Generate 5-fold cross-validation groups\n",
    "CVIndices = np.random.permutation(Dataset.shape[0])\n",
    "Index1, Index2, Index3, Index4, Index5 = CVIndices[:2000], CVIndices[2000:4000], CVIndices[4000:6000], CVIndices[6000:8000], CVIndices[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create List of Classification Layer Hyperparameters\n",
    "Here we just use the list variable ```Activation``` to specify the dense layer activation function we wish to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant neural network architecture packages \n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# List of hyperparameters\n",
    "BatchSize = [4,8,16,32,64,128,256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Save Results as CSV Files\n",
    "Now we test different model variants based on varying amounts of dense nodes in each of the two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import relevant packages for neural network training\n",
    "import sys\n",
    "import csv\n",
    "if 'tensorflow' in sys.modules == False:\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "for k in BatchSize:\n",
    "    # Create empty lists to store results\n",
    "    TrainLoss = []\n",
    "    TrainAcc = []\n",
    "    TestLoss = []\n",
    "    TestAcc = []\n",
    "    All_FPR = []\n",
    "    All_TPR = []\n",
    "    All_thresholds = []\n",
    "    All_AUC = []\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        # Create the appropriate training and testing sets\n",
    "        if i == 0:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index4,:]), axis=0)\n",
    "            TestImages = Dataset[Index5,:]\n",
    "            TestLabels = Labels[Index5,:]\n",
    "        elif i == 1:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index4,:]\n",
    "            TestLabels = Labels[Index4,:]\n",
    "        elif i == 2:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index3,:]\n",
    "            TestLabels = Labels[Index3,:]\n",
    "        elif i == 3:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index2,:]\n",
    "            TestLabels = Labels[Index2,:]\n",
    "        else:\n",
    "            TrainImages = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index2,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index1,:]\n",
    "            TestLabels = Labels[Index1,:]\n",
    "\n",
    "        base_model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (128,128,3))\n",
    "\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model(input = base_model.input, output = predictions)\n",
    "        adam = optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss = \"categorical_crossentropy\", optimizer = adam, metrics=[\"accuracy\"])\n",
    "\n",
    "        # Train model and evaluate performance\n",
    "        print('We are now training cross-validation set #',i+1)\n",
    "        Results = model.fit(TrainImages, TrainLabels, epochs=40, batch_size=k, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
    "\n",
    "        # Display and store performance results\n",
    "        Results.history['loss'] = [round(l, 4) for l in Results.history['loss']]\n",
    "        Results.history['accuracy'] = [round(l, 4) for l in Results.history['accuracy']]\n",
    "        Results.history['val_loss'] = [round(l, 4) for l in Results.history['val_loss']]\n",
    "        Results.history['val_accuracy'] = [round(l, 4) for l in Results.history['val_accuracy']]\n",
    "\n",
    "        print('Training Loss:',Results.history['loss'])\n",
    "        print('Training Accuracy:',Results.history['accuracy'])\n",
    "        print('Validation Loss:',Results.history['val_loss'])\n",
    "        print('Validation Accuracy:',Results.history['val_accuracy'])\n",
    "\n",
    "        TrainLoss.append(Results.history['loss'])\n",
    "        TrainAcc.append(Results.history['accuracy'])\n",
    "        TestLoss.append(Results.history['val_loss'])\n",
    "        TestAcc.append(Results.history['val_accuracy'])\n",
    "        print('')\n",
    "\n",
    "\n",
    "        # Predict values for test set\n",
    "        Probabilities = model.predict(TestImages)\n",
    "\n",
    "        # Calculate data for ROC curve\n",
    "        FPR, TPR, thresholds = roc_curve(TestLabels[:,1], Probabilities[:,1])\n",
    "        All_FPR.append(FPR)\n",
    "        All_TPR.append(TPR)\n",
    "        All_thresholds.append(thresholds)\n",
    "\n",
    "    # Save and export as CSV files\n",
    "    with open(str(k)+\"_TrainLoss.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(TrainLoss)\n",
    "    with open(str(k)+\"_TrainAcc.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(TrainAcc)\n",
    "    with open(str(k)+\"_TestLoss.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(TestLoss)\n",
    "    with open(str(k)+\"_TestAcc.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(TestAcc)\n",
    "    with open(str(k)+\"_FPR.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(All_FPR)\n",
    "    with open(str(k)+\"_TPR.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(All_TPR)\n",
    "    with open(str(k)+\"_Thresholds.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(All_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine RAM Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine RAM Usage\n",
    "import sys\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
