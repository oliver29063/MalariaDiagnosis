{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NIH Dataset (ZIP Format)\n",
    "Here we import the NIH dataset (zip file format) from a website housed by the NIH National Library of Medicine (NLM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Download NIH dataset zip file\n",
    "!wget -nc ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
    "\n",
    "# Extract images if not already extracted\n",
    "ROOT_DIR = os.path.join(\"/\", \"content\")\n",
    "if not os.path.isdir(\"cell_images\"):\n",
    "    print(\"Extracting images...\")\n",
    "    with ZipFile(os.path.join(\"cell_images.zip\"), \"r\") as zipObj:\n",
    "        zipObj.extractall()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Images, Resize, and Store in NumPy Arrays\n",
    "We load the images from the zip file into two different folders. There are 13779 images in each class, with a \"Thumbs.db\" file located in each folder, which we remove. We resize each individual image into 128x128 pixels, while maintaining the 3 RGB channels, and store them into the NumPy arrays ```Parasitized``` and ```Uninfected```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "!pip install opencv-python\n",
    "!apt update && apt install -y libsm6 libxext6 libxrender1\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Create new folders to save rescaled images\n",
    "if not os.path.isdir(\"RescaledSet\"):\n",
    "    os.mkdir(\"RescaledSet\")\n",
    "if not os.path.isdir(\"RescaledSet/Parasitized\"):\n",
    "    os.mkdir(\"RescaledSet/Parasitized\")\n",
    "if not os.path.isdir(\"RescaledSet/Uninfected\"):\n",
    "    os.mkdir(\"RescaledSet/Uninfected\")\n",
    "\n",
    "# Generate list of parasitized file names\n",
    "ParasitizedFiles = os.listdir(\"cell_images/Parasitized/\")\n",
    "UninfectedFiles = os.listdir(\"cell_images/Uninfected/\")\n",
    "\n",
    "# Remove Thumb.db files\n",
    "while 'Thumbs.db' in ParasitizedFiles: ParasitizedFiles.remove('Thumbs.db')   \n",
    "while 'Thumbs.db' in UninfectedFiles: UninfectedFiles.remove('Thumbs.db')  \n",
    "\n",
    "# Pre-allocate memory space for images\n",
    "Parasitized = np.empty([13779,128,128,3])\n",
    "Uninfected = np.empty([13779,128,128,3])\n",
    "\n",
    "# Resize and load parasitized images\n",
    "for i in range(13779):\n",
    "    TempImage = cv2.imread('cell_images/Parasitized/'+ParasitizedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Parasitized[i,:,:,:] = ResizedImage\n",
    "\n",
    "# Resize and load uninfected images\n",
    "for i in range(13779):\n",
    "    TempImage = cv2.imread('cell_images/Uninfected/'+UninfectedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Uninfected[i,:,:,:] = ResizedImage\n",
    "    \n",
    "print('Uninfected Dataset size is:',np.shape(Uninfected))\n",
    "print('Parasitized Dataset size is:',np.shape(Parasitized))\n",
    "\n",
    "\n",
    "# Generate image dataset\n",
    "Dataset = np.concatenate((Parasitized, Uninfected), axis=0)\n",
    "del Parasitized\n",
    "del Uninfected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cross-Validation Indices for Training and Testing Sets\n",
    "Here we randomly generate five cross-validation group indices to access the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5-fold cross-validation groups\n",
    "CVIndices = np.random.permutation(Dataset.shape[0])\n",
    "Index1, Index2, Index3, Index4, Index5 = CVIndices[:5512], CVIndices[5512:11024], CVIndices[11024:16536], CVIndices[16536:22048], CVIndices[22048:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify High and Low Resolution Feature Dimensions\n",
    "Here we specify the values for the high resolution feature dimension and low resolution feature dimension, as a list of values in the variables ```High``` and ```Low``` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "High = [48,56]\n",
    "Low = [12,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Save Results as CSV Files (4 Mapping Blocks)\n",
    "Here we test all FSRCNN variants with 4 mapping layers, and store them in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import relevant packages for neural network training\n",
    "import sys\n",
    "import csv\n",
    "if 'tensorflow' in sys.modules == False:\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "import keras\n",
    "## Create FSRCNN architecture\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, Conv2DTranspose, merge \n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import image\n",
    "\n",
    "!pip install scikit-image\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "\n",
    "for d in High:\n",
    "    for s in Low:\n",
    "\n",
    "        # Create empty lists to store results\n",
    "        TrainLoss = []\n",
    "        TestLoss = []\n",
    "        \n",
    "        for p in range(5):\n",
    "\n",
    "            # Create the appropriate training and testing sets\n",
    "            if i == 0:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
    "                TestOut = Dataset[Index5,:]\n",
    "            elif i == 1:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index4,:]\n",
    "            elif i == 2:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index3,:]\n",
    "            elif i == 3:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index2,:]\n",
    "            else:\n",
    "                TrainOut = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index1,:]\n",
    "\n",
    "\n",
    "            # Generate train and test sets\n",
    "            TrainIn = np.zeros([np.shape(TrainOut)[0],32,32,3])\n",
    "            TestIn = np.zeros([np.shape(TestOut)[0],32,32,3])\n",
    "            for i in range(np.shape(TrainOut)[0]):\n",
    "                TrainIn[i,:,:,:] = downscale_local_mean(TrainOut[i,:,:,:], (4,4,1))\n",
    "            for i in range(np.shape(TestOut)[0]):\n",
    "                TestIn[i,:,:,:] = downscale_local_mean(TestOut[i,:,:,:], (4,4,1))       \n",
    "\n",
    "\n",
    "            #Feature Extraction\n",
    "            model = Sequential()\n",
    "            input_img = Input(shape=(32,32,3))\n",
    "            model = Conv2D(filters = d, kernel_size = (5, 5), padding='same', kernel_initializer='he_normal')(input_img)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Shrink\n",
    "            model = Conv2D(filters = 16, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Mapping\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Exapansion\n",
    "            model = Conv2D(filters = d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Deconvolution\n",
    "            model = Conv2DTranspose(filters = 3, kernel_size = (9, 9), strides=(4, 4), padding='same')(model)\n",
    "            output_img = model\n",
    "\n",
    "            model = Model(input_img, output_img) #Create the model object\n",
    "            adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False) #Training optimizer\n",
    "            model.compile(loss = \"mean_squared_error\", optimizer = adam, metrics=[\"mean_squared_error\"]) #How we measure error\n",
    "\n",
    "            #model.summary()\n",
    "\n",
    "            # Train model and evaluate performance\n",
    "            print('We are now training cross-validation set #',p+1)\n",
    "            Results = model.fit(y=TrainOut, x=TrainIn, validation_data = (TestIn,TestOut), epochs=100, batch_size = 32, validation_freq=1)\n",
    "\n",
    "\n",
    "            # Display and store performance results\n",
    "            Results.history['loss'] = [round(l, 4) for l in Results.history['loss']]\n",
    "            Results.history['val_loss'] = [round(l, 4) for l in Results.history['val_loss']]\n",
    "\n",
    "            print('Training Loss:',Results.history['loss'])\n",
    "            print('Validation Loss:',Results.history['val_loss'])\n",
    "\n",
    "            TrainLoss.append(Results.history['loss'])\n",
    "            TestLoss.append(Results.history['val_loss'])\n",
    "            print('')\n",
    "\n",
    "\n",
    "        # Save and export as CSV files\n",
    "        with open(str(d)+\"_\"+str(s)+\"_4Maps_TrainLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TrainLoss)\n",
    "        with open(str(d)+\"_\"+str(s)+\"_4Maps_TestLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TestLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Save Results as CSV Files (3 Mapping Blocks)\n",
    "Here we test all FSRCNN variants with 3 mapping layers, and store them in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages for neural network training\n",
    "import sys\n",
    "import csv\n",
    "if 'tensorflow' in sys.modules == False:\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "import keras\n",
    "## Create FSRCNN architecture\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, Conv2DTranspose, merge \n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import image\n",
    "\n",
    "!pip install scikit-image\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "\n",
    "for d in High:\n",
    "    for s in Low:\n",
    "\n",
    "        # Create empty lists to store results\n",
    "        TrainLoss = []\n",
    "        TestLoss = []\n",
    "        \n",
    "        for p in range(5):\n",
    "\n",
    "            # Create the appropriate training and testing sets\n",
    "            if i == 0:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
    "                TestOut = Dataset[Index5,:]\n",
    "            elif i == 1:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index4,:]\n",
    "            elif i == 2:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index3,:]\n",
    "            elif i == 3:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index2,:]\n",
    "            else:\n",
    "                TrainOut = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index1,:]\n",
    "\n",
    "\n",
    "            # Generate train and test sets\n",
    "            TrainIn = np.zeros([np.shape(TrainOut)[0],32,32,3])\n",
    "            TestIn = np.zeros([np.shape(TestOut)[0],32,32,3])\n",
    "            for i in range(np.shape(TrainOut)[0]):\n",
    "                TrainIn[i,:,:,:] = downscale_local_mean(TrainOut[i,:,:,:], (4,4,1))\n",
    "            for i in range(np.shape(TestOut)[0]):\n",
    "                TestIn[i,:,:,:] = downscale_local_mean(TestOut[i,:,:,:], (4,4,1))       \n",
    "\n",
    "\n",
    "            #Feature Extraction\n",
    "            model = Sequential()\n",
    "            input_img = Input(shape=(32,32,3))\n",
    "            model = Conv2D(filters = d, kernel_size = (5, 5), padding='same', kernel_initializer='he_normal')(input_img)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Shrink\n",
    "            model = Conv2D(filters = 16, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Mapping\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Exapansion\n",
    "            model = Conv2D(filters = d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Deconvolution\n",
    "            model = Conv2DTranspose(filters = 3, kernel_size = (9, 9), strides=(4, 4), padding='same')(model)\n",
    "            output_img = model\n",
    "\n",
    "            model = Model(input_img, output_img) #Create the model object\n",
    "            adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False) #Training optimizer\n",
    "            model.compile(loss = \"mean_squared_error\", optimizer = adam, metrics=[\"mean_squared_error\"]) #How we measure error\n",
    "\n",
    "            #model.summary()\n",
    "\n",
    "            # Train model and evaluate performance\n",
    "            print('We are now training cross-validation set #',p+1)\n",
    "            Results = model.fit(y=TrainOut, x=TrainIn, validation_data = (TestIn,TestOut), epochs=100, batch_size = 32, validation_freq=1)\n",
    "\n",
    "\n",
    "            # Display and store performance results\n",
    "            Results.history['loss'] = [round(l, 4) for l in Results.history['loss']]\n",
    "            Results.history['val_loss'] = [round(l, 4) for l in Results.history['val_loss']]\n",
    "\n",
    "            print('Training Loss:',Results.history['loss'])\n",
    "            print('Validation Loss:',Results.history['val_loss'])\n",
    "\n",
    "            TrainLoss.append(Results.history['loss'])\n",
    "            TestLoss.append(Results.history['val_loss'])\n",
    "            print('')\n",
    "\n",
    "\n",
    "        # Save and export as CSV files\n",
    "        with open(str(d)+\"_\"+str(s)+\"_3Maps_TrainLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TrainLoss)\n",
    "        with open(str(d)+\"_\"+str(s)+\"_3Maps_TestLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TestLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Save Results as CSV Files (2 Mapping Blocks)\n",
    "Here we test all FSRCNN variants with 2 mapping layers, and store them in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages for neural network training\n",
    "import sys\n",
    "import csv\n",
    "if 'tensorflow' in sys.modules == False:\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "import keras\n",
    "## Create FSRCNN architecture\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, Conv2DTranspose, merge \n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import image\n",
    "\n",
    "!pip install scikit-image\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "\n",
    "for d in High:\n",
    "    for s in Low:\n",
    "\n",
    "        # Create empty lists to store results\n",
    "        TrainLoss = []\n",
    "        TestLoss = []\n",
    "        \n",
    "        for p in range(5):\n",
    "\n",
    "            # Create the appropriate training and testing sets\n",
    "            if i == 0:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
    "                TestOut = Dataset[Index5,:]\n",
    "            elif i == 1:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index4,:]\n",
    "            elif i == 2:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index3,:]\n",
    "            elif i == 3:\n",
    "                TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index2,:]\n",
    "            else:\n",
    "                TrainOut = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "                TestOut = Dataset[Index1,:]\n",
    "\n",
    "\n",
    "            # Generate train and test sets\n",
    "            TrainIn = np.zeros([np.shape(TrainOut)[0],32,32,3])\n",
    "            TestIn = np.zeros([np.shape(TestOut)[0],32,32,3])\n",
    "            for i in range(np.shape(TrainOut)[0]):\n",
    "                TrainIn[i,:,:,:] = downscale_local_mean(TrainOut[i,:,:,:], (4,4,1))\n",
    "            for i in range(np.shape(TestOut)[0]):\n",
    "                TestIn[i,:,:,:] = downscale_local_mean(TestOut[i,:,:,:], (4,4,1))       \n",
    "\n",
    "\n",
    "            #Feature Extraction\n",
    "            model = Sequential()\n",
    "            input_img = Input(shape=(32,32,3))\n",
    "            model = Conv2D(filters = d, kernel_size = (5, 5), padding='same', kernel_initializer='he_normal')(input_img)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Shrink\n",
    "            model = Conv2D(filters = 16, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Mapping\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "            model = Conv2D(filters = s, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Exapansion\n",
    "            model = Conv2D(filters = d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
    "            model = PReLU()(model)\n",
    "\n",
    "            #Deconvolution\n",
    "            model = Conv2DTranspose(filters = 3, kernel_size = (9, 9), strides=(4, 4), padding='same')(model)\n",
    "            output_img = model\n",
    "\n",
    "            model = Model(input_img, output_img) #Create the model object\n",
    "            adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False) #Training optimizer\n",
    "            model.compile(loss = \"mean_squared_error\", optimizer = adam, metrics=[\"mean_squared_error\"]) #How we measure error\n",
    "\n",
    "            #model.summary()\n",
    "\n",
    "            # Train model and evaluate performance\n",
    "            print('We are now training cross-validation set #',p+1)\n",
    "            Results = model.fit(y=TrainOut, x=TrainIn, validation_data = (TestIn,TestOut), epochs=100, batch_size = 32, validation_freq=1)\n",
    "\n",
    "\n",
    "            # Display and store performance results\n",
    "            Results.history['loss'] = [round(l, 4) for l in Results.history['loss']]\n",
    "            Results.history['val_loss'] = [round(l, 4) for l in Results.history['val_loss']]\n",
    "\n",
    "            print('Training Loss:',Results.history['loss'])\n",
    "            print('Validation Loss:',Results.history['val_loss'])\n",
    "\n",
    "            TrainLoss.append(Results.history['loss'])\n",
    "            TestLoss.append(Results.history['val_loss'])\n",
    "            print('')\n",
    "\n",
    "\n",
    "        # Save and export as CSV files\n",
    "        with open(str(d)+\"_\"+str(s)+\"_2Maps_TrainLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TrainLoss)\n",
    "        with open(str(d)+\"_\"+str(s)+\"_2Maps_TestLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TestLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine RAM Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine RAM Usage\n",
    "import sys\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
