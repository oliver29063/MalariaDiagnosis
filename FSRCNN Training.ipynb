{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FSRCNN Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBUjoX1q6kfm",
        "colab_type": "text"
      },
      "source": [
        "# INFORMATION TO USER\n",
        "The notebook is used to generate the results at the following GitHub folders via the **Google Cloud Computing Platform**:\n",
        "\n",
        "- https://github.com/oliver29063/MalariaDiagnosis/tree/master/ImageUpscaling/FSRCNN\n",
        "\n",
        "These results are also referred to in **Table 4** of the manuscript in the **Results** subsection titled **Image Resolution Upscaling**. In short, this notebook is used to determine the ideal high dimension feature space, low dimension feature space, and number of mapping layers.\n",
        "\n",
        "Please note that this notebook requires about 24 GB of RAM to run properly, which exceeds Google Colab capacity and most personal laptops. Consequently, please adjust the number of images used to reduce RAM is necessary when testing the functionality of this notebook. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U-9hf2B7-02",
        "colab_type": "text"
      },
      "source": [
        "# Package Versions\n",
        "Standard Libraries for Python 3.6.9\n",
        "- os\n",
        "- shutil\n",
        "- zipfile\n",
        "- sys\n",
        "- csv\n",
        "\n",
        "Imported Libraries\n",
        "- numpy: 1.18.5\n",
        "- tensorflow: 2.2.0\n",
        "- keras: 2.3.1\n",
        "- sklearn: 0.22.2.post1\n",
        "- cv2: 4.1.2\n",
        "- PIL: 7.0.0\n",
        "- skimage: 0.16.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px0nG1HU4dKu",
        "colab_type": "text"
      },
      "source": [
        "### Download NIH Malaria Dataset\n",
        "For more information about the dataset, visit https://lhncbc.nlm.nih.gov/publication/pub9932"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IwWg3TPzlVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant packages\n",
        "import numpy as np\n",
        "import os\n",
        "from shutil import copyfile\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Download NIH dataset zip file\n",
        "!wget -nc ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
        "\n",
        "# Extract images if not already extracted\n",
        "ROOT_DIR = os.path.join(\"/\", \"content\")\n",
        "if not os.path.isdir(\"cell_images\"):\n",
        "    print(\"Extracting images...\")\n",
        "    with ZipFile(os.path.join(\"cell_images.zip\"), \"r\") as zipObj:\n",
        "        zipObj.extractall()\n",
        "    print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SQ_fb1B4pnp",
        "colab_type": "text"
      },
      "source": [
        "### Specify Number of Images to Use\n",
        "In the manuscript we used 10000 images instead of the full set of approximately 22000 images to reduce computational burden. To check if the code runs properly on your device, we first recommend setting ```numImg``` equal to some small amount, such as 1000 to prevent RAM overload. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1o_Yy3K2ae_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numImg = 1000\n",
        "numClass = numImg//2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3OEdQR65EPc",
        "colab_type": "text"
      },
      "source": [
        "### Basic Image Preprocessing\n",
        "Here we just load our image data into two NumPy arrays ```Parasitized``` and ```Uninfected```, which each contain our set of 128x128 RGB images in a 4D NumPy array for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QtH1pvmzpi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install and import relevant packages\n",
        "import numpy as np\n",
        "import os\n",
        "!pip install opencv-python\n",
        "!apt update && apt install -y libsm6 libxext6 libxrender1\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Create new folders to save rescaled images\n",
        "if not os.path.isdir(\"RescaledSet\"):\n",
        "    os.mkdir(\"RescaledSet\")\n",
        "if not os.path.isdir(\"RescaledSet/Parasitized\"):\n",
        "    os.mkdir(\"RescaledSet/Parasitized\")\n",
        "if not os.path.isdir(\"RescaledSet/Uninfected\"):\n",
        "    os.mkdir(\"RescaledSet/Uninfected\")\n",
        "\n",
        "# Generate list of parasitized file names\n",
        "ParasitizedFiles = os.listdir(\"cell_images/Parasitized/\")\n",
        "UninfectedFiles = os.listdir(\"cell_images/Uninfected/\")\n",
        "\n",
        "# Remove Thumb.db files\n",
        "while 'Thumbs.db' in ParasitizedFiles: ParasitizedFiles.remove('Thumbs.db')   \n",
        "while 'Thumbs.db' in UninfectedFiles: UninfectedFiles.remove('Thumbs.db')  \n",
        "\n",
        "# Pre-allocate memory space for images\n",
        "Parasitized = np.empty([numClass,128,128,3])\n",
        "Uninfected = np.empty([numClass,128,128,3])\n",
        "\n",
        "# Resize and load parasitized images\n",
        "for i in range(numClass):\n",
        "    TempImage = cv2.imread('cell_images/Parasitized/'+ParasitizedFiles[i])\n",
        "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
        "    Parasitized[i,:,:,:] = ResizedImage\n",
        "\n",
        "# Resize and load uninfected images\n",
        "for i in range(numClass):\n",
        "    TempImage = cv2.imread('cell_images/Uninfected/'+UninfectedFiles[i])\n",
        "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
        "    Uninfected[i,:,:,:] = ResizedImage\n",
        "    \n",
        "print('Uninfected Dataset size is:',np.shape(Uninfected))\n",
        "print('Parasitized Dataset size is:',np.shape(Parasitized))\n",
        "\n",
        "# Generate image dataset\n",
        "Dataset = np.concatenate((Parasitized, Uninfected), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDfiSqjFSUBP",
        "colab_type": "text"
      },
      "source": [
        "### Create Downscaled Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAAwRmr6SUHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate train and test sets\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "In = np.zeros([np.shape(Dataset)[0],32,32,3])\n",
        "for i in range(np.shape(Dataset)[0]):\n",
        "  In[i,:,:,:] = downscale_local_mean(Dataset[i,:,:,:], (4,4,1))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "535gq1zgTWm2",
        "colab_type": "text"
      },
      "source": [
        "### Create Cross-Validation Groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La1ww_pKS-EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate 5-fold cross-validation groups\n",
        "Spaces = np.linspace(0,numImg,6).astype('int')\n",
        "CVIndices = np.random.permutation(Dataset.shape[0])\n",
        "Index1, Index2, Index3, Index4, Index5 = CVIndices[:Spaces[1]], CVIndices[Spaces[1]:Spaces[2]], CVIndices[Spaces[2]:Spaces[3]], CVIndices[Spaces[3]:Spaces[4]], CVIndices[Spaces[4]:]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu3E8yMRTrft",
        "colab_type": "text"
      },
      "source": [
        "###List High and Low Dimension Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUOlRV4LTrlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "high_dimension = [48,56]\n",
        "low_dimension = [12,16]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEk1OVOfV1jR",
        "colab_type": "text"
      },
      "source": [
        "### Import Relevant Packages for Neural Network Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9SkMp8cV1rQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "58b76136-0f37-42b8-8063-1058e6208e0a"
      },
      "source": [
        "# Import relevant packages for neural network training\n",
        "import sys\n",
        "import csv\n",
        "if 'tensorflow' in sys.modules == False:\n",
        "    %tensorflow_version 2.x\n",
        "    import tensorflow as tf\n",
        "import keras\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "\n",
        "!pip install scikit-learn\n",
        "import sklearn\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtNfsWCk5jro",
        "colab_type": "text"
      },
      "source": [
        "### Create FSRCNN Architecture w/ 4 Mapping Layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAX-ZEtb2AWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create FSRCNN architecture\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, Conv2DTranspose, merge \n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.preprocessing import image\n",
        "\n",
        "for h_d in high_dimension:\n",
        "  for l_d in low_dimension:\n",
        "\n",
        "    # Create empty lists to store results\n",
        "    TrainLoss = []\n",
        "    TestLoss = []\n",
        "\n",
        "    for i in range(5):\n",
        "\n",
        "      # Create the appropriate training and testing sets\n",
        "      if i == 0:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index3,:], In[Index4,:]), axis=0)\n",
        "          TestOut = Dataset[Index5,:]\n",
        "          TestIn = In[Index5,:]\n",
        "      elif i == 1:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index3,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index4,:]\n",
        "          TestIn = In[Index4,:]\n",
        "      elif i == 2:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index3,:]\n",
        "          TestIn = In[Index3,:]\n",
        "      elif i == 3:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index3,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index2,:]\n",
        "          TestIn = In[Index2,:]\n",
        "      else:\n",
        "          TrainOut = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index2,:], In[Index3,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index1,:]\n",
        "          TestIn = In[Index1,:]\n",
        "    \n",
        "      #Feature Extraction\n",
        "      model = Sequential()\n",
        "      input_img = Input(shape=(32,32,3))\n",
        "      model = Conv2D(filters = h_d, kernel_size = (5, 5), padding='same', kernel_initializer='he_normal')(input_img)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Shrink\n",
        "      model = Conv2D(filters = l_d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Mapping\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Exapansion\n",
        "      model = Conv2D(filters = h_d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Deconvolution\n",
        "      model = Conv2DTranspose(filters = 3, kernel_size = (9, 9), strides=(4, 4), padding='same')(model)\n",
        "      output_img = model\n",
        "\n",
        "      model = Model(input_img, output_img) #Create the model object\n",
        "      adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False) #Training optimizer\n",
        "      model.compile(loss = \"mean_squared_error\", optimizer = adam, metrics=[\"mean_squared_error\"]) #How we measure error\n",
        "\n",
        "      print(\"High Dimension \" + str(h_d))\n",
        "      print(\"Low Dimension \"+ str(l_d))\n",
        "\n",
        "      # Train model and evaluate performance\n",
        "      print('We are now training cross-validation set #',i+1)\n",
        "      Results = model.fit(y=TrainOut, x=TrainIn, validation_data = (TestIn,TestOut), epochs=5, batch_size = 32, validation_freq=1)\n",
        "\n",
        "    # Display and store performance results\n",
        "      Results.history['loss'] = [round(k, 4) for k in Results.history['loss']]\n",
        "      Results.history['val_loss'] = [round(k, 4) for k in Results.history['val_loss']]\n",
        "      \n",
        "      print('Training MSE:',Results.history['loss'])\n",
        "      print('Validation MSE:',Results.history['val_loss'])\n",
        "      \n",
        "      TrainLoss.append(Results.history['loss'])\n",
        "      TestLoss.append(Results.history['val_loss'])\n",
        "      print('')\n",
        "      \n",
        "      # Save and export as CSV files\n",
        "      with open(str(h_d)+\"_\"+str(l_d)+\"_\"+\"4Maps_TrainLoss.csv\", \"w\") as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(TrainLoss)\n",
        "      with open(str(h_d)+\"_\"+str(l_d)+\"_\"+\"4Maps_TestLoss.csv\", \"w\") as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(TestLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7dKGtckWHme",
        "colab_type": "text"
      },
      "source": [
        "### Create FSRCNN Architecture w/ 3 Mapping Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ7dq7Q7WFIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create FSRCNN architecture\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, Conv2DTranspose, merge \n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.preprocessing import image\n",
        "\n",
        "for h_d in high_dimension:\n",
        "  for l_d in low_dimension:\n",
        "\n",
        "    # Create empty lists to store results\n",
        "    TrainLoss = []\n",
        "    TestLoss = []\n",
        "\n",
        "    for i in range(5):\n",
        "\n",
        "      # Create the appropriate training and testing sets\n",
        "      if i == 0:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index3,:], In[Index4,:]), axis=0)\n",
        "          TestOut = Dataset[Index5,:]\n",
        "          TestIn = In[Index5,:]\n",
        "      elif i == 1:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index3,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index4,:]\n",
        "          TestIn = In[Index4,:]\n",
        "      elif i == 2:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index3,:]\n",
        "          TestIn = In[Index3,:]\n",
        "      elif i == 3:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index3,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index2,:]\n",
        "          TestIn = In[Index2,:]\n",
        "      else:\n",
        "          TrainOut = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index2,:], In[Index3,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index1,:]\n",
        "          TestIn = In[Index1,:]\n",
        "    \n",
        "      #Feature Extraction\n",
        "      model = Sequential()\n",
        "      input_img = Input(shape=(32,32,3))\n",
        "      model = Conv2D(filters = h_d, kernel_size = (5, 5), padding='same', kernel_initializer='he_normal')(input_img)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Shrink\n",
        "      model = Conv2D(filters = l_d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Mapping\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Exapansion\n",
        "      model = Conv2D(filters = h_d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Deconvolution\n",
        "      model = Conv2DTranspose(filters = 3, kernel_size = (9, 9), strides=(4, 4), padding='same')(model)\n",
        "      output_img = model\n",
        "\n",
        "      model = Model(input_img, output_img) #Create the model object\n",
        "      adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False) #Training optimizer\n",
        "      model.compile(loss = \"mean_squared_error\", optimizer = adam, metrics=[\"mean_squared_error\"]) #How we measure error\n",
        "\n",
        "      print(\"High Dimension \" + str(h_d))\n",
        "      print(\"Low Dimension \"+ str(l_d))\n",
        "\n",
        "      # Train model and evaluate performance\n",
        "      print('We are now training cross-validation set #',i+1)\n",
        "      Results = model.fit(y=TrainOut, x=TrainIn, validation_data = (TestIn,TestOut), epochs=5, batch_size = 32, validation_freq=1)\n",
        "\n",
        "    # Display and store performance results\n",
        "      Results.history['loss'] = [round(k, 4) for k in Results.history['loss']]\n",
        "      Results.history['val_loss'] = [round(k, 4) for k in Results.history['val_loss']]\n",
        "      \n",
        "      print('Training MSE:',Results.history['loss'])\n",
        "      print('Validation MSE:',Results.history['val_loss'])\n",
        "      \n",
        "      TrainLoss.append(Results.history['loss'])\n",
        "      TestLoss.append(Results.history['val_loss'])\n",
        "      print('')\n",
        "      \n",
        "      # Save and export as CSV files\n",
        "      with open(str(h_d)+\"_\"+str(l_d)+\"_\"+\"3Maps_TrainLoss.csv\", \"w\") as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(TrainLoss)\n",
        "      with open(str(h_d)+\"_\"+str(l_d)+\"_\"+\"3Maps_TestLoss.csv\", \"w\") as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(TestLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9zZWVSgWIUr",
        "colab_type": "text"
      },
      "source": [
        "### Create FSRCNN Architecture w/ 2 Mapping Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6dOMqyXWFQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create FSRCNN architecture\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, Conv2DTranspose, merge \n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.preprocessing import image\n",
        "\n",
        "for h_d in high_dimension:\n",
        "  for l_d in low_dimension:\n",
        "\n",
        "    # Create empty lists to store results\n",
        "    TrainLoss = []\n",
        "    TestLoss = []\n",
        "\n",
        "    for i in range(5):\n",
        "\n",
        "      # Create the appropriate training and testing sets\n",
        "      if i == 0:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index3,:], In[Index4,:]), axis=0)\n",
        "          TestOut = Dataset[Index5,:]\n",
        "          TestIn = In[Index5,:]\n",
        "      elif i == 1:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index3,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index4,:]\n",
        "          TestIn = In[Index4,:]\n",
        "      elif i == 2:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index2,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index3,:]\n",
        "          TestIn = In[Index3,:]\n",
        "      elif i == 3:\n",
        "          TrainOut = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index1,:], In[Index3,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index2,:]\n",
        "          TestIn = In[Index2,:]\n",
        "      else:\n",
        "          TrainOut = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "          TrainIn = np.concatenate((In[Index2,:], In[Index3,:], In[Index4,:], In[Index5,:]), axis=0)\n",
        "          TestOut = Dataset[Index1,:]\n",
        "          TestIn = In[Index1,:]\n",
        "    \n",
        "      #Feature Extraction\n",
        "      model = Sequential()\n",
        "      input_img = Input(shape=(32,32,3))\n",
        "      model = Conv2D(filters = h_d, kernel_size = (5, 5), padding='same', kernel_initializer='he_normal')(input_img)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Shrink\n",
        "      model = Conv2D(filters = l_d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Mapping\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "      model = Conv2D(filters = 12, kernel_size = (3, 3), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Exapansion\n",
        "      model = Conv2D(filters = h_d, kernel_size = (1, 1), padding='same', kernel_initializer='he_normal')(model)\n",
        "      model = PReLU()(model)\n",
        "\n",
        "      #Deconvolution\n",
        "      model = Conv2DTranspose(filters = 3, kernel_size = (9, 9), strides=(4, 4), padding='same')(model)\n",
        "      output_img = model\n",
        "\n",
        "      model = Model(input_img, output_img) #Create the model object\n",
        "      adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False) #Training optimizer\n",
        "      model.compile(loss = \"mean_squared_error\", optimizer = adam, metrics=[\"mean_squared_error\"]) #How we measure error\n",
        "\n",
        "      print(\"High Dimension \" + str(h_d))\n",
        "      print(\"Low Dimension \"+ str(l_d))\n",
        "\n",
        "      # Train model and evaluate performance\n",
        "      print('We are now training cross-validation set #',i+1)\n",
        "      Results = model.fit(y=TrainOut, x=TrainIn, validation_data = (TestIn,TestOut), epochs=5, batch_size = 32, validation_freq=1)\n",
        "\n",
        "    # Display and store performance results\n",
        "      Results.history['loss'] = [round(k, 4) for k in Results.history['loss']]\n",
        "      Results.history['val_loss'] = [round(k, 4) for k in Results.history['val_loss']]\n",
        "      \n",
        "      print('Training MSE:',Results.history['loss'])\n",
        "      print('Validation MSE:',Results.history['val_loss'])\n",
        "      \n",
        "      TrainLoss.append(Results.history['loss'])\n",
        "      TestLoss.append(Results.history['val_loss'])\n",
        "      print('')\n",
        "      \n",
        "      # Save and export as CSV files\n",
        "      with open(str(h_d)+\"_\"+str(l_d)+\"_\"+\"2Maps_TrainLoss.csv\", \"w\") as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(TrainLoss)\n",
        "      with open(str(h_d)+\"_\"+str(l_d)+\"_\"+\"2Maps_TestLoss.csv\", \"w\") as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(TestLoss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}