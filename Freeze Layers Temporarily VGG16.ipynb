{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NIH Dataset (ZIP Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘cell_images.zip’ already there; not retrieving.\r\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Download NIH dataset zip file\n",
    "!wget -nc ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
    "\n",
    "# Extract images if not already extracted\n",
    "ROOT_DIR = os.path.join(\"/\", \"content\")\n",
    "if not os.path.isdir(\"cell_images\"):\n",
    "    print(\"Extracting images...\")\n",
    "    with ZipFile(os.path.join(\"cell_images.zip\"), \"r\") as zipObj:\n",
    "        zipObj.extractall()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Images, Resize, and Store in NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.2.0.32)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.1)\n",
      "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Reading package lists... Done\u001b[0m                 \u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "28 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsm6 is already the newest version (2:1.2.2-1).\n",
      "libxext6 is already the newest version (2:1.3.3-1).\n",
      "libxrender1 is already the newest version (1:0.9.10-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
      "Uninfected Dataset size is: (13779, 128, 128, 3)\n",
      "Parasitized Dataset size is: (13779, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# Install and import relevant packages\n",
    "import numpy as np\n",
    "import os\n",
    "!pip install opencv-python\n",
    "!apt update && apt install -y libsm6 libxext6 libxrender1\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Create new folders to save rescaled images\n",
    "if not os.path.isdir(\"RescaledSet\"):\n",
    "    os.mkdir(\"RescaledSet\")\n",
    "if not os.path.isdir(\"RescaledSet/Parasitized\"):\n",
    "    os.mkdir(\"RescaledSet/Parasitized\")\n",
    "if not os.path.isdir(\"RescaledSet/Uninfected\"):\n",
    "    os.mkdir(\"RescaledSet/Uninfected\")\n",
    "\n",
    "# Generate list of parasitized file names\n",
    "ParasitizedFiles = os.listdir(\"cell_images/Parasitized/\")\n",
    "UninfectedFiles = os.listdir(\"cell_images/Uninfected/\")\n",
    "\n",
    "# Remove Thumb.db files\n",
    "while 'Thumbs.db' in ParasitizedFiles: ParasitizedFiles.remove('Thumbs.db')   \n",
    "while 'Thumbs.db' in UninfectedFiles: UninfectedFiles.remove('Thumbs.db')  \n",
    "\n",
    "# Pre-allocate memory space for images\n",
    "Parasitized = np.empty([13779,128,128,3])\n",
    "Uninfected = np.empty([13779,128,128,3])\n",
    "\n",
    "# Resize and load parasitized images\n",
    "for i in range(13779):\n",
    "    TempImage = cv2.imread('cell_images/Parasitized/'+ParasitizedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Parasitized[i,:,:,:] = ResizedImage\n",
    "\n",
    "# Resize and load uninfected images\n",
    "for i in range(13779):\n",
    "    TempImage = cv2.imread('cell_images/Uninfected/'+UninfectedFiles[i])\n",
    "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
    "    Uninfected[i,:,:,:] = ResizedImage\n",
    "    \n",
    "print('Uninfected Dataset size is:',np.shape(Uninfected))\n",
    "print('Parasitized Dataset size is:',np.shape(Parasitized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cross-Validation Indices for Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset labels\n",
    "ParasitizedLabels = np.repeat([[0,1]], 13779, axis=0)\n",
    "UninfectedLabels = np.repeat([[1,0]], 13779, axis=0)\n",
    "Labels = np.concatenate((ParasitizedLabels,UninfectedLabels), axis=0)\n",
    "\n",
    "# Generate image dataset\n",
    "Dataset = np.concatenate((Parasitized, Uninfected), axis=0)\n",
    "\n",
    "# Generate 5-fold cross-validation groups\n",
    "CVIndices = np.random.permutation(Dataset.shape[0])\n",
    "Index1, Index2, Index3, Index4, Index5 = CVIndices[:5512], CVIndices[5512:11024], CVIndices[11024:16536], CVIndices[16536:22048], CVIndices[22048:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Save Results as CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
      "0 input_2 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 False\n",
      "8 block3_conv2 False\n",
      "9 block3_conv3 False\n",
      "10 block3_pool False\n",
      "11 block4_conv1 False\n",
      "12 block4_conv2 False\n",
      "13 block4_conv3 False\n",
      "14 block4_pool False\n",
      "15 block5_conv1 False\n",
      "16 block5_conv2 False\n",
      "17 block5_conv3 False\n",
      "18 block5_pool False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now training cross-validation set # 1\n",
      "Train on 22048 samples, validate on 5510 samples\n",
      "Epoch 1/50\n",
      "22048/22048 [==============================] - 22s 996us/step - loss: 8.2497 - accuracy: 0.5527 - val_loss: 1.0471 - val_accuracy: 0.8078\n",
      "Epoch 2/50\n",
      "22048/22048 [==============================] - 19s 841us/step - loss: 5.3941 - accuracy: 0.6636 - val_loss: 0.8972 - val_accuracy: 0.8639\n",
      "Epoch 3/50\n",
      "22048/22048 [==============================] - 19s 843us/step - loss: 4.0949 - accuracy: 0.7296 - val_loss: 0.8889 - val_accuracy: 0.8815\n",
      "Epoch 4/50\n",
      "22048/22048 [==============================] - 19s 855us/step - loss: 3.3936 - accuracy: 0.7627 - val_loss: 0.8765 - val_accuracy: 0.8933\n",
      "Epoch 5/50\n",
      "22048/22048 [==============================] - 19s 840us/step - loss: 2.9603 - accuracy: 0.7937 - val_loss: 0.8492 - val_accuracy: 0.9005\n",
      "Epoch 6/50\n",
      "22048/22048 [==============================] - 19s 843us/step - loss: 2.6547 - accuracy: 0.8075 - val_loss: 0.8379 - val_accuracy: 0.9033\n",
      "Epoch 7/50\n",
      "22048/22048 [==============================] - 19s 855us/step - loss: 2.3624 - accuracy: 0.8218 - val_loss: 0.8112 - val_accuracy: 0.9089\n",
      "Epoch 8/50\n",
      "22048/22048 [==============================] - 18s 834us/step - loss: 2.1616 - accuracy: 0.8334 - val_loss: 0.8113 - val_accuracy: 0.9118\n",
      "Epoch 9/50\n",
      "22048/22048 [==============================] - 19s 858us/step - loss: 2.0172 - accuracy: 0.8378 - val_loss: 0.7620 - val_accuracy: 0.9158\n",
      "Epoch 10/50\n",
      "22048/22048 [==============================] - 19s 845us/step - loss: 1.9017 - accuracy: 0.8480 - val_loss: 0.7366 - val_accuracy: 0.9181\n",
      "Epoch 11/50\n",
      "22048/22048 [==============================] - 19s 843us/step - loss: 1.7444 - accuracy: 0.8534 - val_loss: 0.7280 - val_accuracy: 0.9212\n",
      "Epoch 12/50\n",
      "22048/22048 [==============================] - 19s 857us/step - loss: 1.6647 - accuracy: 0.8601 - val_loss: 0.6964 - val_accuracy: 0.9230\n",
      "Epoch 13/50\n",
      "22048/22048 [==============================] - 19s 848us/step - loss: 1.5990 - accuracy: 0.8625 - val_loss: 0.6909 - val_accuracy: 0.9243\n",
      "Epoch 14/50\n",
      "17472/22048 [======================>.......] - ETA: 3s - loss: 1.5345 - accuracy: 0.8669"
     ]
    }
   ],
   "source": [
    "# Import relevant neural network architecture packages \n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# Import relevant packages for neural network training\n",
    "import sys\n",
    "import csv\n",
    "if 'tensorflow' in sys.modules == False:\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Specify layers to freeze\n",
    "FreezeLayers = [19,15,11,7,4,0]\n",
    "\n",
    "# Create empty lists to store results\n",
    "TrainLoss = []\n",
    "TrainAcc = []\n",
    "TestLoss = []\n",
    "TestAcc = []\n",
    "All_FPR = []\n",
    "All_TPR = []\n",
    "All_thresholds = []\n",
    "All_AUC = []\n",
    "\n",
    "for j in FreezeLayers:\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        # Create the appropriate training and testing sets\n",
    "        if i == 0:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index4,:]), axis=0)\n",
    "            TestImages = Dataset[Index5,:]\n",
    "            TestLabels = Labels[Index5,:]\n",
    "        elif i == 1:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index4,:]\n",
    "            TestLabels = Labels[Index4,:]\n",
    "        elif i == 2:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index3,:]\n",
    "            TestLabels = Labels[Index3,:]\n",
    "        elif i == 3:\n",
    "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index2,:]\n",
    "            TestLabels = Labels[Index2,:]\n",
    "        else:\n",
    "            TrainImages = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
    "            TrainLabels = np.concatenate((Labels[Index2,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
    "            TestImages = Dataset[Index1,:]\n",
    "            TestLabels = Labels[Index1,:]\n",
    "\n",
    "        base_model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (128,128,3))\n",
    "\n",
    "        for layer in base_model.layers[:j]:\n",
    "            layer.trainable=False\n",
    "        for k,layer in enumerate(base_model.layers):\n",
    "            print(k,layer.name,layer.trainable)\n",
    "\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(1024, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(1024, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model(input = base_model.input, output = predictions)\n",
    "        adam = optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss = \"categorical_crossentropy\", optimizer = adam, metrics=[\"accuracy\"])\n",
    "\n",
    "        # Train model and evaluate performance\n",
    "        print('We are now training cross-validation set #',i+1)\n",
    "        ResultsPre = model.fit(TrainImages, TrainLabels, epochs=50, batch_size=64, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
    "\n",
    "        # Display and store performance results\n",
    "        ResultsPre.history['loss'] = [round(k, 4) for k in ResultsPre.history['loss']]\n",
    "        ResultsPre.history['accuracy'] = [round(k, 4) for k in ResultsPre.history['accuracy']]\n",
    "        ResultsPre.history['val_loss'] = [round(k, 4) for k in ResultsPre.history['val_loss']]\n",
    "        ResultsPre.history['val_accuracy'] = [round(k, 4) for k in ResultsPre.history['val_accuracy']]\n",
    "        \n",
    "        for layer in model.layers[:j]:\n",
    "            layer.trainable=True\n",
    "        for i,layer in enumerate(model.layers):\n",
    "            print(i,layer.name,layer.trainable)\n",
    "    \n",
    "        adam = optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss = \"categorical_crossentropy\", optimizer = adam, metrics=[\"accuracy\"])\n",
    "\n",
    "        # Train model and evaluate performance\n",
    "        print('We are now training cross-validation set #',i+1)\n",
    "        ResultsPost = model.fit(TrainImages, TrainLabels, epochs=50, batch_size=64, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
    "\n",
    "        # Display and store performance results\n",
    "        ResultsPost.history['loss'] = [round(k, 4) for k in ResultsPost.history['loss']]\n",
    "        ResultsPost.history['accuracy'] = [round(k, 4) for k in ResultsPost.history['accuracy']]\n",
    "        ResultsPost.history['val_loss'] = [round(k, 4) for k in ResultsPost.history['val_loss']]\n",
    "        ResultsPost.history['val_accuracy'] = [round(k, 4) for k in ResultsPost.history['val_accuracy']]\n",
    "        \n",
    "        TrainLoss.append(ResultsPre.history['loss']+ResultsPre.history['loss'])\n",
    "        TrainAcc.append(ResultsPre.history['accuracy']+ResultsPre.history['accuracy'])\n",
    "        TestLoss.append(ResultsPre.history['val_loss']+ResultsPre.history['val_loss'])\n",
    "        TestAcc.append(ResultsPre.history['val_accuracy']+ResultsPre.history['val_accuracy'])\n",
    "        print('')\n",
    "        \n",
    "        print('Training Loss:',ResultsPre.history['loss']+ResultsPre.history['loss'])\n",
    "        print('Training Accuracy:',ResultsPre.history['accuracy']+ResultsPre.history['accuracy'])\n",
    "        print('Validation Loss:',ResultsPre.history['val_loss']+ResultsPre.history['val_loss'])\n",
    "        print('Validation Accuracy:',ResultsPre.history['val_accuracy']+ResultsPre.history['val_accuracy'])\n",
    "\n",
    "        # Predict values for test set\n",
    "        Probabilities = model.predict(TestImages)\n",
    "\n",
    "        # Calculate data for ROC curve\n",
    "        FPR, TPR, thresholds = roc_curve(TestLabels[:,1], Probabilities[:,1])\n",
    "        All_FPR.append(FPR)\n",
    "        All_TPR.append(TPR)\n",
    "        All_thresholds.append(thresholds)\n",
    "\n",
    "        # Save and export as CSV files\n",
    "        with open(str(j)+\"Unfreeze_TrainLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TrainLoss)\n",
    "        with open(str(j)+\"Unfreeze_TrainAcc.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TrainAcc)\n",
    "        with open(str(j)+\"Unfreeze__TestLoss.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TestLoss)\n",
    "        with open(str(j)+\"Unfreeze__TestAcc.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(TestAcc)\n",
    "        with open(str(j)+\"Unfreeze__FPR.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(All_FPR)\n",
    "        with open(str(j)+\"Unfreeze__TPR.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(All_TPR)\n",
    "        with open(str(j)+\"Unfreeze__Thresholds.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(All_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine RAM Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine RAM Usage\n",
    "import sys\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
