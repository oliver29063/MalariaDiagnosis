{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Architecture Training for Optimizers.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBUjoX1q6kfm",
        "colab_type": "text"
      },
      "source": [
        "# INFORMATION TO USER\n",
        "The notebook is used to generate the results at the following GitHub folders via the **Google Cloud Computing Platform**:\n",
        "\n",
        "- https://github.com/oliver29063/MalariaDiagnosis/tree/master/BatchSize\n",
        "- https://github.com/oliver29063/MalariaDiagnosis/tree/master/Optimizers\n",
        "\n",
        "These results are also referred to in **Figure 4** of the manuscript in the **Results** subsection titled **Fine-Tuning Training Hyperparameters**. In short, this notebook is used to determine the ideal optimizer type, learning rate, and batch size.\n",
        "\n",
        "Please note that this notebook requires about 24 GB of RAM to run properly, which exceeds Google Colab capacity and most personal laptops. Consequently, please adjust the number of images used to reduce RAM is necessary when testing the functionality of this notebook. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U-9hf2B7-02",
        "colab_type": "text"
      },
      "source": [
        "# Package Versions\n",
        "Standard Libraries for Python 3.6.9\n",
        "- os\n",
        "- shutil\n",
        "- zipfile\n",
        "- sys\n",
        "- csv\n",
        "\n",
        "Imported Libraries\n",
        "- numpy: 1.18.5\n",
        "- tensorflow: 2.2.0\n",
        "- keras: 2.3.1\n",
        "- sklearn: 0.22.2.post1\n",
        "- cv2: 4.1.2\n",
        "- PIL: 7.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px0nG1HU4dKu",
        "colab_type": "text"
      },
      "source": [
        "### Download NIH Malaria Dataset\n",
        "For more information about the dataset, visit https://lhncbc.nlm.nih.gov/publication/pub9932"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IwWg3TPzlVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant packages\n",
        "import numpy as np\n",
        "import os\n",
        "from shutil import copyfile\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Download NIH dataset zip file\n",
        "!wget -nc ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip\n",
        "\n",
        "# Extract images if not already extracted\n",
        "ROOT_DIR = os.path.join(\"/\", \"content\")\n",
        "if not os.path.isdir(\"cell_images\"):\n",
        "    print(\"Extracting images...\")\n",
        "    with ZipFile(os.path.join(\"cell_images.zip\"), \"r\") as zipObj:\n",
        "        zipObj.extractall()\n",
        "    print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SQ_fb1B4pnp",
        "colab_type": "text"
      },
      "source": [
        "### Specify Number of Images to Use\n",
        "In the manuscript we used 10000 images instead of the full set of approximately 22000 images to reduce computational burden. To check if the code runs properly on your device, we first recommend setting ```numImg``` equal to some small amount, such as 1000 to prevent RAM overload. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1o_Yy3K2ae_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numImg = 1000\n",
        "numClass = numImg//2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3OEdQR65EPc",
        "colab_type": "text"
      },
      "source": [
        "### Basic Image Preprocessing\n",
        "Here we just load our image data into two NumPy arrays ```Parasitized``` and ```Uninfected```, which each contain our set of 128x128 RGB images in a 4D NumPy array for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QtH1pvmzpi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install and import relevant packages\n",
        "import numpy as np\n",
        "import os\n",
        "!pip install opencv-python\n",
        "!apt update && apt install -y libsm6 libxext6 libxrender1\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Create new folders to save rescaled images\n",
        "if not os.path.isdir(\"RescaledSet\"):\n",
        "    os.mkdir(\"RescaledSet\")\n",
        "if not os.path.isdir(\"RescaledSet/Parasitized\"):\n",
        "    os.mkdir(\"RescaledSet/Parasitized\")\n",
        "if not os.path.isdir(\"RescaledSet/Uninfected\"):\n",
        "    os.mkdir(\"RescaledSet/Uninfected\")\n",
        "\n",
        "# Generate list of parasitized file names\n",
        "ParasitizedFiles = os.listdir(\"cell_images/Parasitized/\")\n",
        "UninfectedFiles = os.listdir(\"cell_images/Uninfected/\")\n",
        "\n",
        "# Remove Thumb.db files\n",
        "while 'Thumbs.db' in ParasitizedFiles: ParasitizedFiles.remove('Thumbs.db')   \n",
        "while 'Thumbs.db' in UninfectedFiles: UninfectedFiles.remove('Thumbs.db')  \n",
        "\n",
        "# Pre-allocate memory space for images\n",
        "Parasitized = np.empty([numClass,128,128,3])\n",
        "Uninfected = np.empty([numClass,128,128,3])\n",
        "\n",
        "# Resize and load parasitized images\n",
        "for i in range(numClass):\n",
        "    TempImage = cv2.imread('cell_images/Parasitized/'+ParasitizedFiles[i])\n",
        "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
        "    Parasitized[i,:,:,:] = ResizedImage\n",
        "\n",
        "# Resize and load uninfected images\n",
        "for i in range(numClass):\n",
        "    TempImage = cv2.imread('cell_images/Uninfected/'+UninfectedFiles[i])\n",
        "    ResizedImage = cv2.resize(TempImage, dsize=(128,128))\n",
        "    Uninfected[i,:,:,:] = ResizedImage\n",
        "    \n",
        "print('Uninfected Dataset size is:',np.shape(Uninfected))\n",
        "print('Parasitized Dataset size is:',np.shape(Parasitized))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk7kmBIl5Qay",
        "colab_type": "text"
      },
      "source": [
        "### Create Model Inputs and Cross-Validation Groups\n",
        "Now we create our ```Labels``` as our ground truth and merge our two image classes into a single NumPy array ```Dataset```. We then randomize the order of the images and create our five cross-validation set indices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR7JDUPO14lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate dataset labels\n",
        "ParasitizedLabels = np.repeat([[0,1]], numClass, axis=0)\n",
        "UninfectedLabels = np.repeat([[1,0]], numClass, axis=0)\n",
        "Labels = np.concatenate((ParasitizedLabels,UninfectedLabels), axis=0)\n",
        "\n",
        "# Generate image dataset\n",
        "Dataset = np.concatenate((Parasitized, Uninfected), axis=0)\n",
        "\n",
        "# Generate 5-fold cross-validation groups\n",
        "Spaces = np.linspace(0,numImg,6).astype('int')\n",
        "CVIndices = np.random.permutation(Dataset.shape[0])\n",
        "Index1, Index2, Index3, Index4, Index5 = CVIndices[:Spaces[1]], CVIndices[Spaces[1]:Spaces[2]], CVIndices[Spaces[2]:Spaces[3]], CVIndices[Spaces[3]:Spaces[4]], CVIndices[Spaces[4]:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtNfsWCk5jro",
        "colab_type": "text"
      },
      "source": [
        "### Load VGG16 Model\n",
        "Here we load our pretrained VGG16 model from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAX-ZEtb2AWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "664080ac-a8fc-4175-c2ec-187a11674e8d"
      },
      "source": [
        "# Import VGG16 pre-trained neural network\n",
        "from keras.applications.vgg16 import VGG16"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbUAQvoX8D6c",
        "colab_type": "text"
      },
      "source": [
        "### Import Relevant Packages for Neural Network Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_9yvNfU8FtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant packages for neural network training\n",
        "import sys\n",
        "import csv\n",
        "if 'tensorflow' in sys.modules == False:\n",
        "    %tensorflow_version 2.x\n",
        "    import tensorflow as tf\n",
        "import keras\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "\n",
        "!pip install scikit-learn\n",
        "import sklearn\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO0G31AO5px1",
        "colab_type": "text"
      },
      "source": [
        "### List Learning Rates\n",
        "Here we just create a list of each learning rate we wish to test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7QYeUG5p95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of hyperparameters\n",
        "LR = [1e-6, 1e-5, 1e-4]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcm-CHCxJpSz",
        "colab_type": "text"
      },
      "source": [
        "### Test Different Learning Rates for Each Optimizer\n",
        "Note that you must manually change the file names for each optimizer. The code chunk below only runs a series of learning rates for ONE specific optimizer. To test a different optimizer, the code must be manually commented out/in to pick the right optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SX0-tNhIQGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in LR:\n",
        "    # Create empty lists to store results\n",
        "    TrainLoss = []\n",
        "    TrainAcc = []\n",
        "    TestLoss = []\n",
        "    TestAcc = []\n",
        "    All_FPR = []\n",
        "    All_TPR = []\n",
        "    All_thresholds = []\n",
        "    All_AUC = []\n",
        "\n",
        "    for i in range(5):\n",
        "\n",
        "        # Create the appropriate training and testing sets\n",
        "        if i == 0:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index4,:]), axis=0)\n",
        "            TestImages = Dataset[Index5,:]\n",
        "            TestLabels = Labels[Index5,:]\n",
        "        elif i == 1:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index4,:]\n",
        "            TestLabels = Labels[Index4,:]\n",
        "        elif i == 2:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index3,:]\n",
        "            TestLabels = Labels[Index3,:]\n",
        "        elif i == 3:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index2,:]\n",
        "            TestLabels = Labels[Index2,:]\n",
        "        else:\n",
        "            TrainImages = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index2,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index1,:]\n",
        "            TestLabels = Labels[Index1,:]\n",
        "\n",
        "        # Pick optimizer type\n",
        "        #opt = optimizers.SGD(lr=k, momentum=0.0, nesterov=True) # SGD w/ Nesterov\n",
        "        #opt = optimizers.Nadam(lr=k, beta_1=0.9, beta_2=0.999)  # Nadam\n",
        "        #opt = optimizers.RMSprop(lr=k,rho=0.9,momentum=0.0)     # RMSProp\n",
        "        #opt = optimizers.Adamax(lr=k, beta_1=0.9, beta_2=0.999) # AdaMax\n",
        "        opt = optimizers.Adam(lr=k, beta_1=0.9, beta_2=0.999)    # Adam\n",
        "\n",
        "        base_model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (128,128,3))\n",
        "\n",
        "        x = base_model.output\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(1024, activation='relu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(1024, activation='relu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        predictions = Dense(2, activation=\"softmax\")(x)\n",
        "        model = Model(input = base_model.input, output = predictions)\n",
        "        model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=[\"accuracy\"])\n",
        "\n",
        "        # Train model and evaluate performance\n",
        "        print('We are now training cross-validation set #',i+1)\n",
        "        Results = model.fit(TrainImages, TrainLabels, epochs=40, batch_size=64, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
        "\n",
        "        # Display and store performance results\n",
        "        Results.history['loss'] = [round(l, 4) for l in Results.history['loss']]\n",
        "        Results.history['accuracy'] = [round(l, 4) for l in Results.history['accuracy']]\n",
        "        Results.history['val_loss'] = [round(l, 4) for l in Results.history['val_loss']]\n",
        "        Results.history['val_accuracy'] = [round(l, 4) for l in Results.history['val_accuracy']]\n",
        "\n",
        "        print('Training Loss:',Results.history['loss'])\n",
        "        print('Training Accuracy:',Results.history['accuracy'])\n",
        "        print('Validation Loss:',Results.history['val_loss'])\n",
        "        print('Validation Accuracy:',Results.history['val_accuracy'])\n",
        "\n",
        "        TrainLoss.append(Results.history['loss'])\n",
        "        TrainAcc.append(Results.history['accuracy'])\n",
        "        TestLoss.append(Results.history['val_loss'])\n",
        "        TestAcc.append(Results.history['val_accuracy'])\n",
        "        print('')\n",
        "\n",
        "\n",
        "        # Predict values for test set\n",
        "        Probabilities = model.predict(TestImages)\n",
        "\n",
        "        # Calculate data for ROC curve\n",
        "        FPR, TPR, thresholds = roc_curve(TestLabels[:,1], Probabilities[:,1])\n",
        "        All_FPR.append(FPR)\n",
        "        All_TPR.append(TPR)\n",
        "        All_thresholds.append(thresholds)\n",
        "\n",
        "    # Save and export as CSV files (must manually change file names for each optimizer type)\n",
        "    with open(\"Adam_\"+str(k)+\"_TrainLoss.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(TrainLoss)\n",
        "    with open(\"Adam_\"+str(k)+\"_TrainAcc.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(TrainAcc)\n",
        "    with open(\"Adam_\"+str(k)+\"_TestLoss.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(TestLoss)\n",
        "    with open(\"Adam_\"+str(k)+\"_TestAcc.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(TestAcc)\n",
        "    with open(\"Adam_\"+str(k)+\"_FPR.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(All_FPR)\n",
        "    with open(\"Adam_\"+str(k)+\"_TPR.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(All_TPR)\n",
        "    with open(\"Adam_\"+str(k)+\"_Thresholds.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(All_thresholds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnqnkXhIKZv2",
        "colab_type": "text"
      },
      "source": [
        "### List Different Batch Sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1jhb6LsKMhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Batch = [4,8,16,32,64,128,256]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5yKBTiyKTyI",
        "colab_type": "text"
      },
      "source": [
        "### Test Different Batch Sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XIBoK_0KH6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in Batch:\n",
        "\n",
        "    # Create empty lists to store results\n",
        "    TrainLoss = []\n",
        "    TrainAcc = []\n",
        "    TestLoss = []\n",
        "    TestAcc = []\n",
        "    All_FPR = []\n",
        "    All_TPR = []\n",
        "    All_thresholds = []\n",
        "    All_AUC = []\n",
        "\n",
        "    for i in range(5):\n",
        "\n",
        "        # Create the appropriate training and testing sets\n",
        "        if i == 0:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index4,:]), axis=0)\n",
        "            TestImages = Dataset[Index5,:]\n",
        "            TestLabels = Labels[Index5,:]\n",
        "        elif i == 1:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index3,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index3,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index4,:]\n",
        "            TestLabels = Labels[Index4,:]\n",
        "        elif i == 2:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index2,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index2,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index3,:]\n",
        "            TestLabels = Labels[Index3,:]\n",
        "        elif i == 3:\n",
        "            TrainImages = np.concatenate((Dataset[Index1,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index1,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index2,:]\n",
        "            TestLabels = Labels[Index2,:]\n",
        "        else:\n",
        "            TrainImages = np.concatenate((Dataset[Index2,:],Dataset[Index3,:],Dataset[Index4,:],Dataset[Index5,:]), axis=0)\n",
        "            TrainLabels = np.concatenate((Labels[Index2,:], Labels[Index3,:], Labels[Index4,:], Labels[Index5,:]), axis=0)\n",
        "            TestImages = Dataset[Index1,:]\n",
        "            TestLabels = Labels[Index1,:]\n",
        "\n",
        "        base_model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (128,128,3))\n",
        "\n",
        "        x = base_model.output\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(1024, activation=\"relu\")(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(1024, activation=\"relu\")(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        predictions = Dense(2, activation=\"softmax\")(x)\n",
        "        model = Model(input = base_model.input, output = predictions)\n",
        "        sgd = optimizers.SGD(lr=1e-5, momentum=0.0, nesterov=True)\n",
        "        model.compile(loss = \"categorical_crossentropy\", optimizer = sgd, metrics=[\"accuracy\"])\n",
        "\n",
        "        # Train model and evaluate performance (epoch set equal to 10 for speed, true experiment had epoch set to 75)\n",
        "        print('We are now training cross-validation set #',i+1)\n",
        "        Results = model.fit(TrainImages, TrainLabels, epochs=10, batch_size=k, validation_data=(TestImages,TestLabels), validation_freq=1)\n",
        "\n",
        "        # Display and store performance results\n",
        "        Results.history['loss'] = [round(l, 4) for l in Results.history['loss']]\n",
        "        Results.history['accuracy'] = [round(l, 4) for l in Results.history['accuracy']]\n",
        "        Results.history['val_loss'] = [round(l, 4) for l in Results.history['val_loss']]\n",
        "        Results.history['val_accuracy'] = [round(l, 4) for l in Results.history['val_accuracy']]\n",
        "\n",
        "        print('Training Loss:',Results.history['loss'])\n",
        "        print('Training Accuracy:',Results.history['accuracy'])\n",
        "        print('Validation Loss:',Results.history['val_loss'])\n",
        "        print('Validation Accuracy:',Results.history['val_accuracy'])\n",
        "\n",
        "        TrainLoss.append(Results.history['loss'])\n",
        "        TrainAcc.append(Results.history['accuracy'])\n",
        "        TestLoss.append(Results.history['val_loss'])\n",
        "        TestAcc.append(Results.history['val_accuracy'])\n",
        "        print('')\n",
        "\n",
        "\n",
        "        # Predict values for test set\n",
        "        Probabilities = model.predict(TestImages)\n",
        "\n",
        "        # Calculate data for ROC curve\n",
        "        FPR, TPR, thresholds = roc_curve(TestLabels[:,1], Probabilities[:,1])\n",
        "        All_FPR.append(FPR)\n",
        "        All_TPR.append(TPR)\n",
        "        All_thresholds.append(thresholds)\n",
        "\n",
        "    # Save and export as CSV files\n",
        "    with open(str(k)+\"_TrainLoss.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(TrainLoss)\n",
        "    with open(str(k)+\"_TrainAcc.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(TrainAcc)\n",
        "    with open(str(k)+\"_TestLoss.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(TestLoss)\n",
        "    with open(str(k)+\"_TestAcc.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(TestAcc)\n",
        "    with open(str(k)+\"_FPR.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(All_FPR)\n",
        "    with open(str(k)+\"_TPR.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(All_TPR)\n",
        "    with open(str(k)+\"_Thresholds.csv\", \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(All_thresholds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}